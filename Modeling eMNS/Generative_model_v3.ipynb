{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ETH data to CNN generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    use_gpu = True\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    use_gpu = False\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "torch.Size([1400, 6, 21, 21, 21])\n",
      "current shape torch.Size([1400, 12])\n",
      "Bfield shape torch.Size([1400, 3, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from ReadData import ReadCurrentAndField_CNN, add_gaussian_noise\n",
    "import glob\n",
    "import os \n",
    "\n",
    "# TODO zhoujing edit this Data loading \n",
    "# print(os.getcwd())\n",
    "foldername=\"./Data/\"\n",
    "filepattern = \"MagneticField[0-9]*.txt\"\n",
    "train_file_num= 1200\n",
    "noise = 0.0\n",
    "#data = ReadFolder(foldername,filepattern)\n",
    "current,data = ReadCurrentAndField_CNN (foldername,filepattern,train_file_num)\n",
    "\n",
    "fileList = glob.glob(foldername+filepattern)\n",
    "position = data[:,0:3,2:18,2:18,2:18]\n",
    "Bfield = data[:,3:,2:18,2:18,2:18]\n",
    "\n",
    "# print(fileList)\n",
    "print(data.shape)\n",
    "print('current shape', current.shape)\n",
    "print('Bfield shape', Bfield.shape)\n",
    "current = add_gaussian_noise(current,noise=noise)\n",
    "Bfield = add_gaussian_noise(Bfield,noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net,Generative_net_test ,ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 8\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,3) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net_test(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)\n",
    "\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from Training_loop import grad_loss_Jacobain\n",
    "x = torch.randn(2,8)\n",
    "y = Bfield[0:2]\n",
    "preds = Generative_network(x)\n",
    "print(preds.shape)\n",
    "loss =   F.l1_loss(preds,y)+grad_loss_Jacobain(preds,y)\n",
    "        # optimizer.zero_grad() #zero out all of gradient\n",
    "loss.backward()\n",
    "\n",
    "make_dot(loss, params=dict(Generative_network.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from functools import partial\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "# split the dataset to train, validation, test\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "# normailzation\n",
    "extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "tune_schedule = ASHAScheduler(\n",
    "        metric=\"loss\", # metric to optimize. This metric should be reported with tune.report()\n",
    "        mode=\"min\",\n",
    "        max_t=10,\n",
    "        grace_period=1, # minimum stop epoch\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers = 1,\n",
    "        use_gpu = False,\n",
    "        #resource_per_worker = {\"CPU\":1, \"GPU\":1}\n",
    "    ),\n",
    "    # You can even grid search various datasets in Tune.\n",
    "    # \"datasets\": {\n",
    "    #     \"train\": tune.grid_search(\n",
    "    #         [ds1, ds2]\n",
    "    #     ),\n",
    "    # },\n",
    "    \"train_loop_config\": {\n",
    "                'epochs': tune.choice([10]),\n",
    "                'lr_max': tune.loguniform(1e-4,1e-2),\n",
    "                'lr_min': tune.loguniform(1e-5,1e-7),\n",
    "                'batch_size': tune.choice([4,8,16]),\n",
    "                'L2_norm'   : tune.choice([0]),\n",
    "                'verbose': False,\n",
    "                'DF'     : tune.choice([True,False]),\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': tune.choice([1,2,4]),\n",
    "                'num_repeat'  : tune.choice([1,2,4]),\n",
    "                'num_block'   : tune.choice([1,2,3]),\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set,\n",
    "            }\n",
    "\n",
    "}\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "\n",
    "train_loop_config = {\n",
    "                'epochs': 10,\n",
    "                'lr_max': 1e-4,\n",
    "                'lr_min': 2.5e-6,\n",
    "                'batch_size': 8,\n",
    "                'L2_norm'   : 0,\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 1,\n",
    "                'num_repeat'  : 4,\n",
    "                'num_block'   : 2,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'device'      : device,\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set\n",
    "                # You can even grid search various datasets in Tune.\n",
    "                # \"datasets\": tune.grid_search(\n",
    "                #         [ds1, ds2]\n",
    "                #     ),\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers = 1,\n",
    "    use_gpu = use_gpu,\n",
    "    #resource_per_worker = {\"CPU\":1, \"GPU\":1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=1))\n",
    "\n",
    "# def train_loop_per_worker(params):\n",
    "#     train_GM(train_set=train_set, valid_set=valid_set,  device=device, config=params)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker = train_GM,\n",
    "    train_loop_config = train_loop_config,\n",
    "    scaling_config = scaling_config,\n",
    "    run_config = run_config,\n",
    "\n",
    ")\n",
    "result = trainer.fit()\n",
    "# tuner = tune.Tuner(\n",
    "#     trainer,\n",
    "#     param_space = param_space,\n",
    "#     tune_config =tune.TuneConfig(\n",
    "#         scheduler=tune_schedule,\n",
    "#         num_samples=10, # number of samples of hyperparameter space\n",
    "#     ),\n",
    "#     # run_config = RunConfig(storage_path=\"./results\", name=\"test_experiment\")\n",
    "# )\n",
    "    \n",
    "# tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ray_results\n",
    "plot_ray_results(result, metrics_names=['rmse_train','rmse_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_percent 1.0\n",
      "----------------------------\n",
      "----------------------------\n",
      "Epoch 0, Iteration 158, loss = 0.1226, l1 loss=0.0660, grad loss=0.0566\n",
      "Got rmse 5.629888534545898\n",
      "Got rmse 5.465129852294922\n",
      "\n",
      "Epoch 1, Iteration 316, loss = 0.1300, l1 loss=0.0756, grad loss=0.0544\n",
      "Got rmse 5.793348789215088\n",
      "Got rmse 5.547918796539307\n",
      "\n",
      "Epoch 2, Iteration 474, loss = 0.1234, l1 loss=0.0698, grad loss=0.0536\n",
      "Got rmse 5.290348052978516\n",
      "Got rmse 5.138336181640625\n",
      "\n",
      "Epoch 3, Iteration 632, loss = 0.1272, l1 loss=0.0736, grad loss=0.0537\n",
      "Got rmse 5.928671360015869\n",
      "Got rmse 5.527541637420654\n",
      "\n",
      "Epoch 4, Iteration 790, loss = 0.1069, l1 loss=0.0541, grad loss=0.0528\n",
      "Got rmse 5.48106575012207\n",
      "Got rmse 5.058950424194336\n",
      "\n",
      "Epoch 5, Iteration 948, loss = 0.1115, l1 loss=0.0585, grad loss=0.0529\n",
      "Got rmse 5.0115156173706055\n",
      "Got rmse 4.748825550079346\n",
      "\n",
      "Epoch 6, Iteration 1106, loss = 0.1121, l1 loss=0.0591, grad loss=0.0531\n",
      "Got rmse 5.179642677307129\n",
      "Got rmse 4.768794536590576\n",
      "\n",
      "Epoch 7, Iteration 1264, loss = 0.1251, l1 loss=0.0718, grad loss=0.0533\n",
      "Got rmse 5.19078254699707\n",
      "Got rmse 4.743938446044922\n",
      "\n",
      "Epoch 8, Iteration 1422, loss = 0.1229, l1 loss=0.0704, grad loss=0.0525\n",
      "Got rmse 5.189374923706055\n",
      "Got rmse 4.852733135223389\n",
      "\n",
      "Epoch 9, Iteration 1580, loss = 0.1136, l1 loss=0.0610, grad loss=0.0526\n",
      "Got rmse 4.990812301635742\n",
      "Got rmse 4.6281280517578125\n",
      "\n",
      "Epoch 10, Iteration 1738, loss = 0.1053, l1 loss=0.0527, grad loss=0.0526\n",
      "Got rmse 5.07969856262207\n",
      "Got rmse 4.660131454467773\n",
      "\n",
      "Epoch 11, Iteration 1896, loss = 0.1066, l1 loss=0.0534, grad loss=0.0532\n",
      "Got rmse 4.928196907043457\n",
      "Got rmse 4.463584899902344\n",
      "\n",
      "Epoch 12, Iteration 2054, loss = 0.0983, l1 loss=0.0455, grad loss=0.0528\n",
      "Got rmse 4.837148666381836\n",
      "Got rmse 4.273417949676514\n",
      "\n",
      "Epoch 13, Iteration 2212, loss = 0.1154, l1 loss=0.0632, grad loss=0.0522\n",
      "Got rmse 4.894236087799072\n",
      "Got rmse 4.2774176597595215\n",
      "\n",
      "Epoch 14, Iteration 2370, loss = 0.1074, l1 loss=0.0546, grad loss=0.0527\n",
      "Got rmse 5.138469696044922\n",
      "Got rmse 4.468963146209717\n",
      "\n",
      "Epoch 15, Iteration 2528, loss = 0.0983, l1 loss=0.0457, grad loss=0.0526\n",
      "Got rmse 5.079639911651611\n",
      "Got rmse 4.351799964904785\n",
      "\n",
      "Epoch 16, Iteration 2686, loss = 0.1098, l1 loss=0.0577, grad loss=0.0522\n",
      "Got rmse 4.9305644035339355\n",
      "Got rmse 4.2490234375\n",
      "\n",
      "Epoch 17, Iteration 2844, loss = 0.0983, l1 loss=0.0453, grad loss=0.0530\n",
      "Got rmse 5.087932109832764\n",
      "Got rmse 4.322429656982422\n",
      "\n",
      "Epoch 18, Iteration 3002, loss = 0.0992, l1 loss=0.0466, grad loss=0.0527\n",
      "Got rmse 4.9056501388549805\n",
      "Got rmse 4.11154317855835\n",
      "\n",
      "Epoch 19, Iteration 3160, loss = 0.1011, l1 loss=0.0485, grad loss=0.0525\n",
      "Got rmse 4.989914894104004\n",
      "Got rmse 4.160730361938477\n",
      "\n",
      "Epoch 20, Iteration 3318, loss = 0.1000, l1 loss=0.0471, grad loss=0.0529\n",
      "Got rmse 5.128591060638428\n",
      "Got rmse 4.244074821472168\n",
      "\n",
      "Epoch 21, Iteration 3476, loss = 0.0978, l1 loss=0.0454, grad loss=0.0524\n",
      "Got rmse 5.021970748901367\n",
      "Got rmse 3.9903404712677\n",
      "\n",
      "Epoch 22, Iteration 3634, loss = 0.1009, l1 loss=0.0478, grad loss=0.0531\n",
      "Got rmse 5.159643173217773\n",
      "Got rmse 4.114781856536865\n",
      "\n",
      "Epoch 23, Iteration 3792, loss = 0.1035, l1 loss=0.0508, grad loss=0.0527\n",
      "Got rmse 5.010447025299072\n",
      "Got rmse 3.9973649978637695\n",
      "\n",
      "Epoch 24, Iteration 3950, loss = 0.1045, l1 loss=0.0520, grad loss=0.0525\n",
      "Got rmse 5.0681962966918945\n",
      "Got rmse 4.012216091156006\n",
      "\n",
      "Epoch 25, Iteration 4108, loss = 0.1042, l1 loss=0.0517, grad loss=0.0525\n",
      "Got rmse 4.990896701812744\n",
      "Got rmse 3.872882843017578\n",
      "\n",
      "Epoch 26, Iteration 4266, loss = 0.0968, l1 loss=0.0444, grad loss=0.0524\n",
      "Got rmse 4.999190807342529\n",
      "Got rmse 3.8768789768218994\n",
      "\n",
      "Epoch 27, Iteration 4424, loss = 0.0997, l1 loss=0.0473, grad loss=0.0523\n",
      "Got rmse 4.946561813354492\n",
      "Got rmse 3.7540369033813477\n",
      "\n",
      "Epoch 28, Iteration 4582, loss = 0.0967, l1 loss=0.0444, grad loss=0.0523\n",
      "Got rmse 4.996561527252197\n",
      "Got rmse 3.831882953643799\n",
      "\n",
      "Epoch 29, Iteration 4740, loss = 0.1029, l1 loss=0.0510, grad loss=0.0519\n",
      "Got rmse 5.074604034423828\n",
      "Got rmse 3.7879245281219482\n",
      "\n",
      "Epoch 30, Iteration 4898, loss = 0.0970, l1 loss=0.0446, grad loss=0.0524\n",
      "Got rmse 4.935826778411865\n",
      "Got rmse 3.6778063774108887\n",
      "\n",
      "Epoch 31, Iteration 5056, loss = 0.0996, l1 loss=0.0475, grad loss=0.0521\n",
      "Got rmse 5.0567216873168945\n",
      "Got rmse 3.8246829509735107\n",
      "\n",
      "Epoch 32, Iteration 5214, loss = 0.0966, l1 loss=0.0445, grad loss=0.0521\n",
      "Got rmse 5.038329124450684\n",
      "Got rmse 3.7235465049743652\n",
      "\n",
      "Epoch 33, Iteration 5372, loss = 0.0974, l1 loss=0.0451, grad loss=0.0524\n",
      "Got rmse 5.0269975662231445\n",
      "Got rmse 3.703300952911377\n",
      "\n",
      "Epoch 34, Iteration 5530, loss = 0.0953, l1 loss=0.0428, grad loss=0.0526\n",
      "Got rmse 5.02252721786499\n",
      "Got rmse 3.627831220626831\n",
      "\n",
      "Epoch 35, Iteration 5688, loss = 0.0964, l1 loss=0.0440, grad loss=0.0524\n",
      "Got rmse 5.0794219970703125\n",
      "Got rmse 3.6563949584960938\n",
      "\n",
      "Epoch 36, Iteration 5846, loss = 0.0949, l1 loss=0.0427, grad loss=0.0522\n",
      "Got rmse 4.9867401123046875\n",
      "Got rmse 3.6519272327423096\n",
      "\n",
      "Epoch 37, Iteration 6004, loss = 0.0993, l1 loss=0.0470, grad loss=0.0524\n",
      "Got rmse 5.059247970581055\n",
      "Got rmse 3.7096481323242188\n",
      "\n",
      "Epoch 38, Iteration 6162, loss = 0.0959, l1 loss=0.0437, grad loss=0.0522\n",
      "Got rmse 5.007058143615723\n",
      "Got rmse 3.5873374938964844\n",
      "\n",
      "Epoch 39, Iteration 6320, loss = 0.0979, l1 loss=0.0454, grad loss=0.0525\n",
      "Got rmse 5.1707234382629395\n",
      "Got rmse 3.754610061645508\n",
      "\n",
      "Epoch 40, Iteration 6478, loss = 0.0946, l1 loss=0.0421, grad loss=0.0525\n",
      "Got rmse 4.953366756439209\n",
      "Got rmse 3.5077426433563232\n",
      "\n",
      "Epoch 41, Iteration 6636, loss = 0.0939, l1 loss=0.0414, grad loss=0.0525\n",
      "Got rmse 4.966241836547852\n",
      "Got rmse 3.515235185623169\n",
      "\n",
      "Epoch 42, Iteration 6794, loss = 0.0962, l1 loss=0.0439, grad loss=0.0523\n",
      "Got rmse 5.032915115356445\n",
      "Got rmse 3.604447603225708\n",
      "\n",
      "Epoch 43, Iteration 6952, loss = 0.0933, l1 loss=0.0413, grad loss=0.0520\n",
      "Got rmse 4.965778827667236\n",
      "Got rmse 3.479353189468384\n",
      "\n",
      "Epoch 44, Iteration 7110, loss = 0.0931, l1 loss=0.0411, grad loss=0.0520\n",
      "Got rmse 4.960302829742432\n",
      "Got rmse 3.5227298736572266\n",
      "\n",
      "Epoch 45, Iteration 7268, loss = 0.0953, l1 loss=0.0431, grad loss=0.0522\n",
      "Got rmse 4.986090183258057\n",
      "Got rmse 3.54667329788208\n",
      "\n",
      "Epoch 46, Iteration 7426, loss = 0.0955, l1 loss=0.0435, grad loss=0.0520\n",
      "Got rmse 4.985886096954346\n",
      "Got rmse 3.4590256214141846\n",
      "\n",
      "Epoch 47, Iteration 7584, loss = 0.0959, l1 loss=0.0433, grad loss=0.0526\n",
      "Got rmse 5.046969413757324\n",
      "Got rmse 3.515253782272339\n",
      "\n",
      "Epoch 48, Iteration 7742, loss = 0.0941, l1 loss=0.0419, grad loss=0.0522\n",
      "Got rmse 5.051877975463867\n",
      "Got rmse 3.489413022994995\n",
      "\n",
      "Epoch 49, Iteration 7900, loss = 0.0955, l1 loss=0.0430, grad loss=0.0525\n",
      "Got rmse 4.982834815979004\n",
      "Got rmse 3.5209832191467285\n",
      "\n",
      "Epoch 50, Iteration 8058, loss = 0.0933, l1 loss=0.0412, grad loss=0.0521\n",
      "Got rmse 4.917410373687744\n",
      "Got rmse 3.393751621246338\n",
      "\n",
      "Epoch 51, Iteration 8216, loss = 0.0972, l1 loss=0.0451, grad loss=0.0521\n",
      "Got rmse 4.997910022735596\n",
      "Got rmse 3.4654881954193115\n",
      "\n",
      "Epoch 52, Iteration 8374, loss = 0.0951, l1 loss=0.0426, grad loss=0.0524\n",
      "Got rmse 5.078676223754883\n",
      "Got rmse 3.4970850944519043\n",
      "\n",
      "Epoch 53, Iteration 8532, loss = 0.0933, l1 loss=0.0409, grad loss=0.0524\n",
      "Got rmse 5.00152587890625\n",
      "Got rmse 3.4495491981506348\n",
      "\n",
      "Epoch 54, Iteration 8690, loss = 0.0928, l1 loss=0.0404, grad loss=0.0525\n",
      "Got rmse 5.006002426147461\n",
      "Got rmse 3.4002933502197266\n",
      "\n",
      "Epoch 55, Iteration 8848, loss = 0.0922, l1 loss=0.0402, grad loss=0.0520\n",
      "Got rmse 4.972916126251221\n",
      "Got rmse 3.3796093463897705\n",
      "\n",
      "Epoch 56, Iteration 9006, loss = 0.0933, l1 loss=0.0412, grad loss=0.0521\n",
      "Got rmse 4.915750026702881\n",
      "Got rmse 3.3648738861083984\n",
      "\n",
      "Epoch 57, Iteration 9164, loss = 0.0952, l1 loss=0.0432, grad loss=0.0520\n",
      "Got rmse 5.031928062438965\n",
      "Got rmse 3.4883854389190674\n",
      "\n",
      "Epoch 58, Iteration 9322, loss = 0.0927, l1 loss=0.0407, grad loss=0.0520\n",
      "Got rmse 4.921248912811279\n",
      "Got rmse 3.4300472736358643\n",
      "\n",
      "Epoch 59, Iteration 9480, loss = 0.0931, l1 loss=0.0409, grad loss=0.0522\n",
      "Got rmse 5.01666784286499\n",
      "Got rmse 3.4060964584350586\n",
      "\n",
      "Epoch 60, Iteration 9638, loss = 0.0925, l1 loss=0.0404, grad loss=0.0521\n",
      "Got rmse 4.977121829986572\n",
      "Got rmse 3.377808094024658\n",
      "\n",
      "Epoch 61, Iteration 9796, loss = 0.0934, l1 loss=0.0414, grad loss=0.0520\n",
      "Got rmse 4.992471218109131\n",
      "Got rmse 3.4363455772399902\n",
      "\n",
      "Epoch 62, Iteration 9954, loss = 0.0923, l1 loss=0.0401, grad loss=0.0522\n",
      "Got rmse 4.915332317352295\n",
      "Got rmse 3.3396990299224854\n",
      "\n",
      "Epoch 63, Iteration 10112, loss = 0.0925, l1 loss=0.0404, grad loss=0.0521\n",
      "Got rmse 4.934262752532959\n",
      "Got rmse 3.3311688899993896\n",
      "\n",
      "Epoch 64, Iteration 10270, loss = 0.0919, l1 loss=0.0400, grad loss=0.0519\n",
      "Got rmse 4.981259822845459\n",
      "Got rmse 3.3842709064483643\n",
      "\n",
      "Epoch 65, Iteration 10428, loss = 0.0915, l1 loss=0.0397, grad loss=0.0518\n",
      "Got rmse 4.973345756530762\n",
      "Got rmse 3.3853793144226074\n",
      "\n",
      "Epoch 66, Iteration 10586, loss = 0.0938, l1 loss=0.0419, grad loss=0.0519\n",
      "Got rmse 4.978349685668945\n",
      "Got rmse 3.420947551727295\n",
      "\n",
      "Epoch 67, Iteration 10744, loss = 0.0922, l1 loss=0.0401, grad loss=0.0521\n",
      "Got rmse 4.971617221832275\n",
      "Got rmse 3.4077322483062744\n",
      "\n",
      "Epoch 68, Iteration 10902, loss = 0.0933, l1 loss=0.0412, grad loss=0.0521\n",
      "Got rmse 4.952705383300781\n",
      "Got rmse 3.4619832038879395\n",
      "\n",
      "Epoch 69, Iteration 11060, loss = 0.0927, l1 loss=0.0404, grad loss=0.0522\n",
      "Got rmse 4.921625137329102\n",
      "Got rmse 3.3373570442199707\n",
      "\n",
      "Epoch 70, Iteration 11218, loss = 0.0931, l1 loss=0.0410, grad loss=0.0521\n",
      "Got rmse 4.885218143463135\n",
      "Got rmse 3.3163347244262695\n",
      "\n",
      "Epoch 71, Iteration 11376, loss = 0.0923, l1 loss=0.0404, grad loss=0.0519\n",
      "Got rmse 4.882554531097412\n",
      "Got rmse 3.2939140796661377\n",
      "\n",
      "Epoch 72, Iteration 11534, loss = 0.0922, l1 loss=0.0401, grad loss=0.0520\n",
      "Got rmse 4.908866882324219\n",
      "Got rmse 3.300748109817505\n",
      "\n",
      "Epoch 73, Iteration 11692, loss = 0.0914, l1 loss=0.0394, grad loss=0.0520\n",
      "Got rmse 4.827239036560059\n",
      "Got rmse 3.250506639480591\n",
      "\n",
      "Epoch 74, Iteration 11850, loss = 0.0927, l1 loss=0.0405, grad loss=0.0522\n",
      "Got rmse 4.897190570831299\n",
      "Got rmse 3.299081325531006\n",
      "\n",
      "Epoch 75, Iteration 12008, loss = 0.0925, l1 loss=0.0406, grad loss=0.0519\n",
      "Got rmse 4.900645732879639\n",
      "Got rmse 3.289363145828247\n",
      "\n",
      "Epoch 76, Iteration 12166, loss = 0.0921, l1 loss=0.0403, grad loss=0.0518\n",
      "Got rmse 4.940928936004639\n",
      "Got rmse 3.3637642860412598\n",
      "\n",
      "Epoch 77, Iteration 12324, loss = 0.0920, l1 loss=0.0396, grad loss=0.0524\n",
      "Got rmse 4.843396186828613\n",
      "Got rmse 3.2470896244049072\n",
      "\n",
      "Epoch 78, Iteration 12482, loss = 0.0913, l1 loss=0.0394, grad loss=0.0519\n",
      "Got rmse 4.921755313873291\n",
      "Got rmse 3.2585501670837402\n",
      "\n",
      "Epoch 79, Iteration 12640, loss = 0.0916, l1 loss=0.0398, grad loss=0.0518\n",
      "Got rmse 4.875509262084961\n",
      "Got rmse 3.3248186111450195\n",
      "\n",
      "Epoch 80, Iteration 12798, loss = 0.0908, l1 loss=0.0388, grad loss=0.0520\n",
      "Got rmse 4.85629415512085\n",
      "Got rmse 3.2244162559509277\n",
      "\n",
      "Epoch 81, Iteration 12956, loss = 0.0913, l1 loss=0.0396, grad loss=0.0517\n",
      "Got rmse 4.917206764221191\n",
      "Got rmse 3.324392557144165\n",
      "\n",
      "Epoch 82, Iteration 13114, loss = 0.0912, l1 loss=0.0393, grad loss=0.0519\n",
      "Got rmse 4.8689117431640625\n",
      "Got rmse 3.2698075771331787\n",
      "\n",
      "Epoch 83, Iteration 13272, loss = 0.0904, l1 loss=0.0387, grad loss=0.0517\n",
      "Got rmse 4.835912227630615\n",
      "Got rmse 3.2539217472076416\n",
      "\n",
      "Epoch 84, Iteration 13430, loss = 0.0915, l1 loss=0.0396, grad loss=0.0519\n",
      "Got rmse 4.859862327575684\n",
      "Got rmse 3.2079954147338867\n",
      "\n",
      "Epoch 85, Iteration 13588, loss = 0.0911, l1 loss=0.0391, grad loss=0.0519\n",
      "Got rmse 4.878713130950928\n",
      "Got rmse 3.24692702293396\n",
      "\n",
      "Epoch 86, Iteration 13746, loss = 0.0924, l1 loss=0.0404, grad loss=0.0520\n",
      "Got rmse 4.885995864868164\n",
      "Got rmse 3.276224374771118\n",
      "\n",
      "Epoch 87, Iteration 13904, loss = 0.0911, l1 loss=0.0395, grad loss=0.0516\n",
      "Got rmse 4.917728424072266\n",
      "Got rmse 3.238614559173584\n",
      "\n",
      "Epoch 88, Iteration 14062, loss = 0.0920, l1 loss=0.0401, grad loss=0.0519\n",
      "Got rmse 4.856840133666992\n",
      "Got rmse 3.2501845359802246\n",
      "\n",
      "Epoch 89, Iteration 14220, loss = 0.0915, l1 loss=0.0395, grad loss=0.0521\n",
      "Got rmse 4.9405059814453125\n",
      "Got rmse 3.367117166519165\n",
      "\n",
      "Epoch 90, Iteration 14378, loss = 0.0943, l1 loss=0.0424, grad loss=0.0520\n",
      "Got rmse 4.880446910858154\n",
      "Got rmse 3.271329641342163\n",
      "\n",
      "Epoch 91, Iteration 14536, loss = 0.0916, l1 loss=0.0394, grad loss=0.0522\n",
      "Got rmse 5.074391841888428\n",
      "Got rmse 3.402392864227295\n",
      "\n",
      "Epoch 92, Iteration 14694, loss = 0.0920, l1 loss=0.0399, grad loss=0.0521\n",
      "Got rmse 4.938565731048584\n",
      "Got rmse 3.2446937561035156\n",
      "\n",
      "Epoch 93, Iteration 14852, loss = 0.0913, l1 loss=0.0394, grad loss=0.0519\n",
      "Got rmse 4.88877010345459\n",
      "Got rmse 3.207303524017334\n",
      "\n",
      "Epoch 94, Iteration 15010, loss = 0.0917, l1 loss=0.0397, grad loss=0.0521\n",
      "Got rmse 4.963575839996338\n",
      "Got rmse 3.2851312160491943\n",
      "\n",
      "Epoch 95, Iteration 15168, loss = 0.0913, l1 loss=0.0392, grad loss=0.0521\n",
      "Got rmse 4.891312599182129\n",
      "Got rmse 3.234891176223755\n",
      "\n",
      "Epoch 96, Iteration 15326, loss = 0.0907, l1 loss=0.0387, grad loss=0.0519\n",
      "Got rmse 4.850488185882568\n",
      "Got rmse 3.220921277999878\n",
      "\n",
      "Epoch 97, Iteration 15484, loss = 0.0910, l1 loss=0.0388, grad loss=0.0521\n",
      "Got rmse 4.900288105010986\n",
      "Got rmse 3.2479465007781982\n",
      "\n",
      "Epoch 98, Iteration 15642, loss = 0.0911, l1 loss=0.0393, grad loss=0.0518\n",
      "Got rmse 4.880315780639648\n",
      "Got rmse 3.1924526691436768\n",
      "\n",
      "Epoch 99, Iteration 15800, loss = 0.0913, l1 loss=0.0393, grad loss=0.0520\n",
      "Got rmse 4.879331111907959\n",
      "Got rmse 3.238401174545288\n",
      "\n",
      "Epoch 100, Iteration 15958, loss = 0.0912, l1 loss=0.0394, grad loss=0.0518\n",
      "Got rmse 4.935991287231445\n",
      "Got rmse 3.199310541152954\n",
      "\n",
      "Epoch 101, Iteration 16116, loss = 0.0923, l1 loss=0.0404, grad loss=0.0520\n",
      "Got rmse 4.924242973327637\n",
      "Got rmse 3.275123357772827\n",
      "\n",
      "Epoch 102, Iteration 16274, loss = 0.0924, l1 loss=0.0404, grad loss=0.0519\n",
      "Got rmse 4.8818488121032715\n",
      "Got rmse 3.179107904434204\n",
      "\n",
      "Epoch 103, Iteration 16432, loss = 0.0914, l1 loss=0.0396, grad loss=0.0519\n",
      "Got rmse 4.844395637512207\n",
      "Got rmse 3.2018847465515137\n",
      "\n",
      "Epoch 104, Iteration 16590, loss = 0.0910, l1 loss=0.0390, grad loss=0.0520\n",
      "Got rmse 4.849217414855957\n",
      "Got rmse 3.157357692718506\n",
      "\n",
      "Epoch 105, Iteration 16748, loss = 0.0906, l1 loss=0.0386, grad loss=0.0519\n",
      "Got rmse 4.847235679626465\n",
      "Got rmse 3.1621530055999756\n",
      "\n",
      "Epoch 106, Iteration 16906, loss = 0.0903, l1 loss=0.0384, grad loss=0.0519\n",
      "Got rmse 4.929351329803467\n",
      "Got rmse 3.234266519546509\n",
      "\n",
      "Epoch 107, Iteration 17064, loss = 0.0906, l1 loss=0.0387, grad loss=0.0519\n",
      "Got rmse 4.885404586791992\n",
      "Got rmse 3.1668131351470947\n",
      "\n",
      "Epoch 108, Iteration 17222, loss = 0.0906, l1 loss=0.0388, grad loss=0.0518\n",
      "Got rmse 4.908998966217041\n",
      "Got rmse 3.264047145843506\n",
      "\n",
      "Epoch 109, Iteration 17380, loss = 0.0904, l1 loss=0.0383, grad loss=0.0522\n",
      "Got rmse 4.879599571228027\n",
      "Got rmse 3.1968538761138916\n",
      "\n",
      "Epoch 110, Iteration 17538, loss = 0.0908, l1 loss=0.0389, grad loss=0.0519\n",
      "Got rmse 4.82581901550293\n",
      "Got rmse 3.1925673484802246\n",
      "\n",
      "Epoch 111, Iteration 17696, loss = 0.0904, l1 loss=0.0385, grad loss=0.0518\n",
      "Got rmse 4.839038372039795\n",
      "Got rmse 3.1515157222747803\n",
      "\n",
      "Epoch 112, Iteration 17854, loss = 0.0900, l1 loss=0.0382, grad loss=0.0518\n",
      "Got rmse 4.822651386260986\n",
      "Got rmse 3.164189100265503\n",
      "\n",
      "Epoch 113, Iteration 18012, loss = 0.0905, l1 loss=0.0384, grad loss=0.0521\n",
      "Got rmse 4.817181587219238\n",
      "Got rmse 3.134382963180542\n",
      "\n",
      "Epoch 114, Iteration 18170, loss = 0.0898, l1 loss=0.0378, grad loss=0.0520\n",
      "Got rmse 4.880498886108398\n",
      "Got rmse 3.160097360610962\n",
      "\n",
      "Epoch 115, Iteration 18328, loss = 0.0896, l1 loss=0.0379, grad loss=0.0517\n",
      "Got rmse 4.785138130187988\n",
      "Got rmse 3.134262800216675\n",
      "\n",
      "Epoch 116, Iteration 18486, loss = 0.0898, l1 loss=0.0378, grad loss=0.0520\n",
      "Got rmse 4.818055629730225\n",
      "Got rmse 3.135105848312378\n",
      "\n",
      "Epoch 117, Iteration 18644, loss = 0.0900, l1 loss=0.0381, grad loss=0.0519\n",
      "Got rmse 4.8476176261901855\n",
      "Got rmse 3.155928373336792\n",
      "\n",
      "Epoch 118, Iteration 18802, loss = 0.0911, l1 loss=0.0388, grad loss=0.0523\n",
      "Got rmse 4.8132853507995605\n",
      "Got rmse 3.182058334350586\n",
      "\n",
      "Epoch 119, Iteration 18960, loss = 0.0898, l1 loss=0.0380, grad loss=0.0519\n",
      "Got rmse 4.823232173919678\n",
      "Got rmse 3.144151449203491\n",
      "\n",
      "Epoch 120, Iteration 19118, loss = 0.0887, l1 loss=0.0374, grad loss=0.0513\n",
      "Got rmse 4.842765808105469\n",
      "Got rmse 3.157522201538086\n",
      "\n",
      "Epoch 121, Iteration 19276, loss = 0.0905, l1 loss=0.0386, grad loss=0.0519\n",
      "Got rmse 4.833702087402344\n",
      "Got rmse 3.129087209701538\n",
      "\n",
      "Epoch 122, Iteration 19434, loss = 0.0897, l1 loss=0.0381, grad loss=0.0516\n",
      "Got rmse 4.823368549346924\n",
      "Got rmse 3.137791395187378\n",
      "\n",
      "Epoch 123, Iteration 19592, loss = 0.0898, l1 loss=0.0381, grad loss=0.0517\n",
      "Got rmse 4.8361921310424805\n",
      "Got rmse 3.163459300994873\n",
      "\n",
      "Epoch 124, Iteration 19750, loss = 0.0900, l1 loss=0.0380, grad loss=0.0520\n",
      "Got rmse 4.845440864562988\n",
      "Got rmse 3.159721851348877\n",
      "\n",
      "Epoch 125, Iteration 19908, loss = 0.0902, l1 loss=0.0379, grad loss=0.0523\n",
      "Got rmse 4.794406890869141\n",
      "Got rmse 3.103924512863159\n",
      "\n",
      "Epoch 126, Iteration 20066, loss = 0.0896, l1 loss=0.0378, grad loss=0.0517\n",
      "Got rmse 4.784762382507324\n",
      "Got rmse 3.1143789291381836\n",
      "\n",
      "Epoch 127, Iteration 20224, loss = 0.0896, l1 loss=0.0380, grad loss=0.0517\n",
      "Got rmse 4.802726745605469\n",
      "Got rmse 3.1296980381011963\n",
      "\n",
      "Epoch 128, Iteration 20382, loss = 0.0897, l1 loss=0.0380, grad loss=0.0517\n",
      "Got rmse 4.834296703338623\n",
      "Got rmse 3.1411163806915283\n",
      "\n",
      "Epoch 129, Iteration 20540, loss = 0.0901, l1 loss=0.0383, grad loss=0.0518\n",
      "Got rmse 4.8113298416137695\n",
      "Got rmse 3.1152400970458984\n",
      "\n",
      "Epoch 130, Iteration 20698, loss = 0.0898, l1 loss=0.0379, grad loss=0.0519\n",
      "Got rmse 4.825563430786133\n",
      "Got rmse 3.114595651626587\n",
      "\n",
      "Epoch 131, Iteration 20856, loss = 0.0898, l1 loss=0.0379, grad loss=0.0519\n",
      "Got rmse 4.799136161804199\n",
      "Got rmse 3.0905818939208984\n",
      "\n",
      "Epoch 132, Iteration 21014, loss = 0.0893, l1 loss=0.0375, grad loss=0.0518\n",
      "Got rmse 4.790996551513672\n",
      "Got rmse 3.075446605682373\n",
      "\n",
      "Epoch 133, Iteration 21172, loss = 0.0899, l1 loss=0.0380, grad loss=0.0518\n",
      "Got rmse 4.7793474197387695\n",
      "Got rmse 3.0960066318511963\n",
      "\n",
      "Epoch 134, Iteration 21330, loss = 0.0898, l1 loss=0.0380, grad loss=0.0518\n",
      "Got rmse 4.860318183898926\n",
      "Got rmse 3.1290464401245117\n",
      "\n",
      "Epoch 135, Iteration 21488, loss = 0.0891, l1 loss=0.0377, grad loss=0.0514\n",
      "Got rmse 4.799771308898926\n",
      "Got rmse 3.0741381645202637\n",
      "\n",
      "Epoch 136, Iteration 21646, loss = 0.0897, l1 loss=0.0379, grad loss=0.0518\n",
      "Got rmse 4.835089206695557\n",
      "Got rmse 3.096235513687134\n",
      "\n",
      "Epoch 137, Iteration 21804, loss = 0.0915, l1 loss=0.0399, grad loss=0.0517\n",
      "Got rmse 4.796069145202637\n",
      "Got rmse 3.131957530975342\n",
      "\n",
      "Epoch 138, Iteration 21962, loss = 0.0894, l1 loss=0.0377, grad loss=0.0518\n",
      "Got rmse 4.797200679779053\n",
      "Got rmse 3.0937447547912598\n",
      "\n",
      "Epoch 139, Iteration 22120, loss = 0.0895, l1 loss=0.0379, grad loss=0.0516\n",
      "Got rmse 4.800748348236084\n",
      "Got rmse 3.1055734157562256\n",
      "\n",
      "Epoch 140, Iteration 22278, loss = 0.0896, l1 loss=0.0377, grad loss=0.0519\n",
      "Got rmse 4.786590099334717\n",
      "Got rmse 3.0694217681884766\n",
      "\n",
      "Epoch 141, Iteration 22436, loss = 0.0885, l1 loss=0.0373, grad loss=0.0512\n",
      "Got rmse 4.8027472496032715\n",
      "Got rmse 3.105250358581543\n",
      "\n",
      "Epoch 142, Iteration 22594, loss = 0.0888, l1 loss=0.0374, grad loss=0.0514\n",
      "Got rmse 4.836155891418457\n",
      "Got rmse 3.0988929271698\n",
      "\n",
      "Epoch 143, Iteration 22752, loss = 0.0887, l1 loss=0.0370, grad loss=0.0517\n",
      "Got rmse 4.79299259185791\n",
      "Got rmse 3.068227529525757\n",
      "\n",
      "Epoch 144, Iteration 22910, loss = 0.0894, l1 loss=0.0375, grad loss=0.0519\n",
      "Got rmse 4.826147556304932\n",
      "Got rmse 3.077840566635132\n",
      "\n",
      "Epoch 145, Iteration 23068, loss = 0.0896, l1 loss=0.0379, grad loss=0.0517\n",
      "Got rmse 4.859567165374756\n",
      "Got rmse 3.122026205062866\n",
      "\n",
      "Epoch 146, Iteration 23226, loss = 0.0891, l1 loss=0.0377, grad loss=0.0514\n",
      "Got rmse 4.91374397277832\n",
      "Got rmse 3.1518330574035645\n",
      "\n",
      "Epoch 147, Iteration 23384, loss = 0.0884, l1 loss=0.0370, grad loss=0.0515\n",
      "Got rmse 4.780267715454102\n",
      "Got rmse 3.0759193897247314\n",
      "\n",
      "Epoch 148, Iteration 23542, loss = 0.0896, l1 loss=0.0376, grad loss=0.0519\n",
      "Got rmse 4.807672500610352\n",
      "Got rmse 3.0634243488311768\n",
      "\n",
      "Epoch 149, Iteration 23700, loss = 0.0895, l1 loss=0.0378, grad loss=0.0517\n",
      "Got rmse 4.832597255706787\n",
      "Got rmse 3.0902292728424072\n",
      "\n",
      "Epoch 150, Iteration 23858, loss = 0.0907, l1 loss=0.0389, grad loss=0.0517\n",
      "Got rmse 4.864485740661621\n",
      "Got rmse 3.165449380874634\n",
      "\n",
      "Epoch 151, Iteration 24016, loss = 0.0892, l1 loss=0.0375, grad loss=0.0517\n",
      "Got rmse 4.774090766906738\n",
      "Got rmse 3.070686101913452\n",
      "\n",
      "Epoch 152, Iteration 24174, loss = 0.0888, l1 loss=0.0374, grad loss=0.0514\n",
      "Got rmse 4.7922258377075195\n",
      "Got rmse 3.067504644393921\n",
      "\n",
      "Epoch 153, Iteration 24332, loss = 0.0898, l1 loss=0.0378, grad loss=0.0520\n",
      "Got rmse 4.841006755828857\n",
      "Got rmse 3.0676028728485107\n",
      "\n",
      "Epoch 154, Iteration 24490, loss = 0.0884, l1 loss=0.0370, grad loss=0.0514\n",
      "Got rmse 4.789453983306885\n",
      "Got rmse 3.047485828399658\n",
      "\n",
      "Epoch 155, Iteration 24648, loss = 0.0898, l1 loss=0.0377, grad loss=0.0521\n",
      "Got rmse 4.7710065841674805\n",
      "Got rmse 3.071843385696411\n",
      "\n",
      "Epoch 156, Iteration 24806, loss = 0.0899, l1 loss=0.0382, grad loss=0.0517\n",
      "Got rmse 4.833340644836426\n",
      "Got rmse 3.104222059249878\n",
      "\n",
      "Epoch 157, Iteration 24964, loss = 0.0895, l1 loss=0.0377, grad loss=0.0517\n",
      "Got rmse 4.764980316162109\n",
      "Got rmse 3.0641894340515137\n",
      "\n",
      "Epoch 158, Iteration 25122, loss = 0.0898, l1 loss=0.0379, grad loss=0.0518\n",
      "Got rmse 4.797109603881836\n",
      "Got rmse 3.0833184719085693\n",
      "\n",
      "Epoch 159, Iteration 25280, loss = 0.0893, l1 loss=0.0373, grad loss=0.0520\n",
      "Got rmse 4.83148717880249\n",
      "Got rmse 3.0896382331848145\n",
      "\n",
      "Epoch 160, Iteration 25438, loss = 0.0899, l1 loss=0.0380, grad loss=0.0519\n",
      "Got rmse 4.835776329040527\n",
      "Got rmse 3.1096320152282715\n",
      "\n",
      "Epoch 161, Iteration 25596, loss = 0.0888, l1 loss=0.0372, grad loss=0.0516\n",
      "Got rmse 4.7837958335876465\n",
      "Got rmse 3.0399208068847656\n",
      "\n",
      "Epoch 162, Iteration 25754, loss = 0.0893, l1 loss=0.0375, grad loss=0.0518\n",
      "Got rmse 4.788924217224121\n",
      "Got rmse 3.089656114578247\n",
      "\n",
      "Epoch 163, Iteration 25912, loss = 0.0900, l1 loss=0.0384, grad loss=0.0516\n",
      "Got rmse 4.780618667602539\n",
      "Got rmse 3.0540740489959717\n",
      "\n",
      "Epoch 164, Iteration 26070, loss = 0.0884, l1 loss=0.0369, grad loss=0.0516\n",
      "Got rmse 4.776775360107422\n",
      "Got rmse 3.0512871742248535\n",
      "\n",
      "Epoch 165, Iteration 26228, loss = 0.0891, l1 loss=0.0375, grad loss=0.0516\n",
      "Got rmse 4.813294887542725\n",
      "Got rmse 3.063300609588623\n",
      "\n",
      "Epoch 166, Iteration 26386, loss = 0.0887, l1 loss=0.0371, grad loss=0.0516\n",
      "Got rmse 4.809402942657471\n",
      "Got rmse 3.0524935722351074\n",
      "\n",
      "Epoch 167, Iteration 26544, loss = 0.0890, l1 loss=0.0373, grad loss=0.0517\n",
      "Got rmse 4.82199239730835\n",
      "Got rmse 3.0514447689056396\n",
      "\n",
      "Epoch 168, Iteration 26702, loss = 0.0889, l1 loss=0.0371, grad loss=0.0518\n",
      "Got rmse 4.773787975311279\n",
      "Got rmse 3.0288453102111816\n",
      "\n",
      "Epoch 169, Iteration 26860, loss = 0.0884, l1 loss=0.0369, grad loss=0.0515\n",
      "Got rmse 4.803606986999512\n",
      "Got rmse 3.0428450107574463\n",
      "\n",
      "Epoch 170, Iteration 27018, loss = 0.0886, l1 loss=0.0371, grad loss=0.0515\n",
      "Got rmse 4.823959827423096\n",
      "Got rmse 3.1039397716522217\n",
      "\n",
      "Epoch 171, Iteration 27176, loss = 0.0884, l1 loss=0.0370, grad loss=0.0515\n",
      "Got rmse 4.817981719970703\n",
      "Got rmse 3.0466365814208984\n",
      "\n",
      "Epoch 172, Iteration 27334, loss = 0.0886, l1 loss=0.0371, grad loss=0.0515\n",
      "Got rmse 4.7757062911987305\n",
      "Got rmse 3.034325361251831\n",
      "\n",
      "Epoch 173, Iteration 27492, loss = 0.0890, l1 loss=0.0371, grad loss=0.0519\n",
      "Got rmse 4.776030540466309\n",
      "Got rmse 3.025559425354004\n",
      "\n",
      "Epoch 174, Iteration 27650, loss = 0.0887, l1 loss=0.0370, grad loss=0.0517\n",
      "Got rmse 4.783413887023926\n",
      "Got rmse 3.050133228302002\n",
      "\n",
      "Epoch 175, Iteration 27808, loss = 0.0889, l1 loss=0.0372, grad loss=0.0517\n",
      "Got rmse 4.763696670532227\n",
      "Got rmse 3.030674457550049\n",
      "\n",
      "Epoch 176, Iteration 27966, loss = 0.0889, l1 loss=0.0373, grad loss=0.0516\n",
      "Got rmse 4.797337532043457\n",
      "Got rmse 3.0451204776763916\n",
      "\n",
      "Epoch 177, Iteration 28124, loss = 0.0884, l1 loss=0.0367, grad loss=0.0517\n",
      "Got rmse 4.776157379150391\n",
      "Got rmse 3.0199575424194336\n",
      "\n",
      "Epoch 178, Iteration 28282, loss = 0.0884, l1 loss=0.0369, grad loss=0.0515\n",
      "Got rmse 4.786141395568848\n",
      "Got rmse 3.0225822925567627\n",
      "\n",
      "Epoch 179, Iteration 28440, loss = 0.0890, l1 loss=0.0371, grad loss=0.0519\n",
      "Got rmse 4.753869533538818\n",
      "Got rmse 3.006042957305908\n",
      "\n",
      "Epoch 180, Iteration 28598, loss = 0.0895, l1 loss=0.0375, grad loss=0.0520\n",
      "Got rmse 4.775793075561523\n",
      "Got rmse 3.0318198204040527\n",
      "\n",
      "Epoch 181, Iteration 28756, loss = 0.0885, l1 loss=0.0370, grad loss=0.0514\n",
      "Got rmse 4.773551940917969\n",
      "Got rmse 3.0284640789031982\n",
      "\n",
      "Epoch 182, Iteration 28914, loss = 0.0879, l1 loss=0.0365, grad loss=0.0514\n",
      "Got rmse 4.754583358764648\n",
      "Got rmse 3.0169296264648438\n",
      "\n",
      "Epoch 183, Iteration 29072, loss = 0.0881, l1 loss=0.0367, grad loss=0.0514\n",
      "Got rmse 4.784345626831055\n",
      "Got rmse 3.02120041847229\n",
      "\n",
      "Epoch 184, Iteration 29230, loss = 0.0881, l1 loss=0.0367, grad loss=0.0514\n",
      "Got rmse 4.794063568115234\n",
      "Got rmse 3.0209672451019287\n",
      "\n",
      "Epoch 185, Iteration 29388, loss = 0.0885, l1 loss=0.0367, grad loss=0.0517\n",
      "Got rmse 4.803435325622559\n",
      "Got rmse 3.023632049560547\n",
      "\n",
      "Epoch 186, Iteration 29546, loss = 0.0905, l1 loss=0.0384, grad loss=0.0521\n",
      "Got rmse 4.786288738250732\n",
      "Got rmse 3.0358986854553223\n",
      "\n",
      "Epoch 187, Iteration 29704, loss = 0.0880, l1 loss=0.0367, grad loss=0.0513\n",
      "Got rmse 4.822428226470947\n",
      "Got rmse 3.0249369144439697\n",
      "\n",
      "Epoch 188, Iteration 29862, loss = 0.0884, l1 loss=0.0367, grad loss=0.0517\n",
      "Got rmse 4.781238555908203\n",
      "Got rmse 2.9996283054351807\n",
      "\n",
      "Epoch 189, Iteration 30020, loss = 0.0884, l1 loss=0.0369, grad loss=0.0514\n",
      "Got rmse 4.831392765045166\n",
      "Got rmse 3.0423061847686768\n",
      "\n",
      "Epoch 190, Iteration 30178, loss = 0.0887, l1 loss=0.0371, grad loss=0.0516\n",
      "Got rmse 4.766788959503174\n",
      "Got rmse 3.0188300609588623\n",
      "\n",
      "Epoch 191, Iteration 30336, loss = 0.0881, l1 loss=0.0367, grad loss=0.0514\n",
      "Got rmse 4.800897121429443\n",
      "Got rmse 3.016218900680542\n",
      "\n",
      "Epoch 192, Iteration 30494, loss = 0.0883, l1 loss=0.0368, grad loss=0.0515\n",
      "Got rmse 4.783606052398682\n",
      "Got rmse 3.0350043773651123\n",
      "\n",
      "Epoch 193, Iteration 30652, loss = 0.0885, l1 loss=0.0368, grad loss=0.0516\n",
      "Got rmse 4.78750467300415\n",
      "Got rmse 3.0013678073883057\n",
      "\n",
      "Epoch 194, Iteration 30810, loss = 0.0882, l1 loss=0.0367, grad loss=0.0515\n",
      "Got rmse 4.785521030426025\n",
      "Got rmse 3.0235090255737305\n",
      "\n",
      "Epoch 195, Iteration 30968, loss = 0.0883, l1 loss=0.0366, grad loss=0.0518\n",
      "Got rmse 4.787630081176758\n",
      "Got rmse 3.0009119510650635\n",
      "\n",
      "Epoch 196, Iteration 31126, loss = 0.0887, l1 loss=0.0369, grad loss=0.0519\n",
      "Got rmse 4.7751545906066895\n",
      "Got rmse 3.009370803833008\n",
      "\n",
      "Epoch 197, Iteration 31284, loss = 0.0883, l1 loss=0.0367, grad loss=0.0516\n",
      "Got rmse 4.766853332519531\n",
      "Got rmse 3.0001280307769775\n",
      "\n",
      "Epoch 198, Iteration 31442, loss = 0.0882, l1 loss=0.0368, grad loss=0.0514\n",
      "Got rmse 4.786566257476807\n",
      "Got rmse 2.999110460281372\n",
      "\n",
      "Epoch 199, Iteration 31600, loss = 0.0889, l1 loss=0.0368, grad loss=0.0521\n",
      "Got rmse 4.777338027954102\n",
      "Got rmse 3.0033533573150635\n",
      "\n",
      "Epoch 200, Iteration 31758, loss = 0.0887, l1 loss=0.0368, grad loss=0.0518\n",
      "Got rmse 4.790232181549072\n",
      "Got rmse 2.9998433589935303\n",
      "\n",
      "Epoch 201, Iteration 31916, loss = 0.0879, l1 loss=0.0366, grad loss=0.0514\n",
      "Got rmse 4.769564628601074\n",
      "Got rmse 2.9912054538726807\n",
      "\n",
      "Epoch 202, Iteration 32074, loss = 0.0883, l1 loss=0.0366, grad loss=0.0516\n",
      "Got rmse 4.769143104553223\n",
      "Got rmse 3.003817319869995\n",
      "\n",
      "Epoch 203, Iteration 32232, loss = 0.0884, l1 loss=0.0367, grad loss=0.0517\n",
      "Got rmse 4.769225597381592\n",
      "Got rmse 2.992142677307129\n",
      "\n",
      "Epoch 204, Iteration 32390, loss = 0.0876, l1 loss=0.0363, grad loss=0.0513\n",
      "Got rmse 4.773489952087402\n",
      "Got rmse 2.991138458251953\n",
      "\n",
      "Epoch 205, Iteration 32548, loss = 0.0887, l1 loss=0.0369, grad loss=0.0518\n",
      "Got rmse 4.773606777191162\n",
      "Got rmse 3.002168893814087\n",
      "\n",
      "Epoch 206, Iteration 32706, loss = 0.0885, l1 loss=0.0368, grad loss=0.0517\n",
      "Got rmse 4.7854228019714355\n",
      "Got rmse 2.991806983947754\n",
      "\n",
      "Epoch 207, Iteration 32864, loss = 0.0885, l1 loss=0.0368, grad loss=0.0517\n",
      "Got rmse 4.755733489990234\n",
      "Got rmse 2.9893085956573486\n",
      "\n",
      "Epoch 208, Iteration 33022, loss = 0.0882, l1 loss=0.0367, grad loss=0.0516\n",
      "Got rmse 4.785186290740967\n",
      "Got rmse 2.9910125732421875\n",
      "\n",
      "Epoch 209, Iteration 33180, loss = 0.0881, l1 loss=0.0365, grad loss=0.0516\n",
      "Got rmse 4.784701347351074\n",
      "Got rmse 2.9885106086730957\n",
      "\n",
      "Epoch 210, Iteration 33338, loss = 0.0883, l1 loss=0.0367, grad loss=0.0516\n",
      "Got rmse 4.764748573303223\n",
      "Got rmse 3.001117706298828\n",
      "\n",
      "Epoch 211, Iteration 33496, loss = 0.0883, l1 loss=0.0366, grad loss=0.0517\n",
      "Got rmse 4.769055366516113\n",
      "Got rmse 2.9908552169799805\n",
      "\n",
      "Epoch 212, Iteration 33654, loss = 0.0884, l1 loss=0.0367, grad loss=0.0517\n",
      "Got rmse 4.7839789390563965\n",
      "Got rmse 2.9933669567108154\n",
      "\n",
      "Epoch 213, Iteration 33812, loss = 0.0888, l1 loss=0.0369, grad loss=0.0518\n",
      "Got rmse 4.778846263885498\n",
      "Got rmse 2.9920895099639893\n",
      "\n",
      "Epoch 214, Iteration 33970, loss = 0.0879, l1 loss=0.0364, grad loss=0.0515\n",
      "Got rmse 4.7758002281188965\n",
      "Got rmse 2.9807558059692383\n",
      "\n",
      "Epoch 215, Iteration 34128, loss = 0.0887, l1 loss=0.0367, grad loss=0.0520\n",
      "Got rmse 4.784072399139404\n",
      "Got rmse 2.982895612716675\n",
      "\n",
      "Epoch 216, Iteration 34286, loss = 0.0881, l1 loss=0.0365, grad loss=0.0516\n",
      "Got rmse 4.7712812423706055\n",
      "Got rmse 2.9823861122131348\n",
      "\n",
      "Epoch 217, Iteration 34444, loss = 0.0878, l1 loss=0.0364, grad loss=0.0514\n",
      "Got rmse 4.771025657653809\n",
      "Got rmse 2.984572172164917\n",
      "\n",
      "Epoch 218, Iteration 34602, loss = 0.0881, l1 loss=0.0366, grad loss=0.0515\n",
      "Got rmse 4.775660037994385\n",
      "Got rmse 2.9850831031799316\n",
      "\n",
      "Epoch 219, Iteration 34760, loss = 0.0884, l1 loss=0.0366, grad loss=0.0518\n",
      "Got rmse 4.774438858032227\n",
      "Got rmse 2.9832963943481445\n",
      "\n",
      "Epoch 220, Iteration 34918, loss = 0.0876, l1 loss=0.0363, grad loss=0.0513\n",
      "Got rmse 4.799912452697754\n",
      "Got rmse 2.9858267307281494\n",
      "\n",
      "Epoch 221, Iteration 35076, loss = 0.0875, l1 loss=0.0363, grad loss=0.0513\n",
      "Got rmse 4.759905815124512\n",
      "Got rmse 2.9742908477783203\n",
      "\n",
      "Epoch 222, Iteration 35234, loss = 0.0882, l1 loss=0.0366, grad loss=0.0516\n",
      "Got rmse 4.786770343780518\n",
      "Got rmse 2.9807732105255127\n",
      "\n",
      "Epoch 223, Iteration 35392, loss = 0.0879, l1 loss=0.0364, grad loss=0.0515\n",
      "Got rmse 4.782363414764404\n",
      "Got rmse 2.9804227352142334\n",
      "\n",
      "Epoch 224, Iteration 35550, loss = 0.0881, l1 loss=0.0365, grad loss=0.0516\n",
      "Got rmse 4.778339385986328\n",
      "Got rmse 2.9771969318389893\n",
      "\n",
      "Epoch 225, Iteration 35708, loss = 0.0879, l1 loss=0.0365, grad loss=0.0514\n",
      "Got rmse 4.796452522277832\n",
      "Got rmse 2.9978935718536377\n",
      "\n",
      "Epoch 226, Iteration 35866, loss = 0.0875, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.77904748916626\n",
      "Got rmse 2.9749696254730225\n",
      "\n",
      "Epoch 227, Iteration 36024, loss = 0.0883, l1 loss=0.0366, grad loss=0.0517\n",
      "Got rmse 4.794979572296143\n",
      "Got rmse 2.9870800971984863\n",
      "\n",
      "Epoch 228, Iteration 36182, loss = 0.0883, l1 loss=0.0367, grad loss=0.0516\n",
      "Got rmse 4.7944655418396\n",
      "Got rmse 2.9845893383026123\n",
      "\n",
      "Epoch 229, Iteration 36340, loss = 0.0880, l1 loss=0.0364, grad loss=0.0516\n",
      "Got rmse 4.778731822967529\n",
      "Got rmse 2.9737589359283447\n",
      "\n",
      "Epoch 230, Iteration 36498, loss = 0.0877, l1 loss=0.0363, grad loss=0.0514\n",
      "Got rmse 4.7883195877075195\n",
      "Got rmse 2.9751250743865967\n",
      "\n",
      "Epoch 231, Iteration 36656, loss = 0.0880, l1 loss=0.0365, grad loss=0.0515\n",
      "Got rmse 4.774984359741211\n",
      "Got rmse 2.9754478931427\n",
      "\n",
      "Epoch 232, Iteration 36814, loss = 0.0877, l1 loss=0.0361, grad loss=0.0516\n",
      "Got rmse 4.7770233154296875\n",
      "Got rmse 2.9674863815307617\n",
      "\n",
      "Epoch 233, Iteration 36972, loss = 0.0878, l1 loss=0.0363, grad loss=0.0515\n",
      "Got rmse 4.763378620147705\n",
      "Got rmse 2.9695231914520264\n",
      "\n",
      "Epoch 234, Iteration 37130, loss = 0.0879, l1 loss=0.0364, grad loss=0.0515\n",
      "Got rmse 4.771116733551025\n",
      "Got rmse 2.9674839973449707\n",
      "\n",
      "Epoch 235, Iteration 37288, loss = 0.0879, l1 loss=0.0364, grad loss=0.0515\n",
      "Got rmse 4.7769904136657715\n",
      "Got rmse 2.9733352661132812\n",
      "\n",
      "Epoch 236, Iteration 37446, loss = 0.0872, l1 loss=0.0361, grad loss=0.0511\n",
      "Got rmse 4.785666465759277\n",
      "Got rmse 2.968336820602417\n",
      "\n",
      "Epoch 237, Iteration 37604, loss = 0.0877, l1 loss=0.0363, grad loss=0.0514\n",
      "Got rmse 4.770846366882324\n",
      "Got rmse 2.9692187309265137\n",
      "\n",
      "Epoch 238, Iteration 37762, loss = 0.0874, l1 loss=0.0362, grad loss=0.0512\n",
      "Got rmse 4.7908616065979\n",
      "Got rmse 2.970582962036133\n",
      "\n",
      "Epoch 239, Iteration 37920, loss = 0.0885, l1 loss=0.0366, grad loss=0.0520\n",
      "Got rmse 4.77306604385376\n",
      "Got rmse 2.9671249389648438\n",
      "\n",
      "Epoch 240, Iteration 38078, loss = 0.0879, l1 loss=0.0364, grad loss=0.0515\n",
      "Got rmse 4.771787166595459\n",
      "Got rmse 2.9726245403289795\n",
      "\n",
      "Epoch 241, Iteration 38236, loss = 0.0879, l1 loss=0.0363, grad loss=0.0516\n",
      "Got rmse 4.77297306060791\n",
      "Got rmse 2.96675181388855\n",
      "\n",
      "Epoch 242, Iteration 38394, loss = 0.0881, l1 loss=0.0364, grad loss=0.0517\n",
      "Got rmse 4.779660224914551\n",
      "Got rmse 2.9684011936187744\n",
      "\n",
      "Epoch 243, Iteration 38552, loss = 0.0879, l1 loss=0.0362, grad loss=0.0517\n",
      "Got rmse 4.786934852600098\n",
      "Got rmse 2.9668991565704346\n",
      "\n",
      "Epoch 244, Iteration 38710, loss = 0.0876, l1 loss=0.0362, grad loss=0.0514\n",
      "Got rmse 4.778445243835449\n",
      "Got rmse 2.9652974605560303\n",
      "\n",
      "Epoch 245, Iteration 38868, loss = 0.0879, l1 loss=0.0364, grad loss=0.0515\n",
      "Got rmse 4.79348087310791\n",
      "Got rmse 2.972813367843628\n",
      "\n",
      "Epoch 246, Iteration 39026, loss = 0.0879, l1 loss=0.0363, grad loss=0.0516\n",
      "Got rmse 4.783486843109131\n",
      "Got rmse 2.961958885192871\n",
      "\n",
      "Epoch 247, Iteration 39184, loss = 0.0876, l1 loss=0.0362, grad loss=0.0514\n",
      "Got rmse 4.792365550994873\n",
      "Got rmse 2.9640066623687744\n",
      "\n",
      "Epoch 248, Iteration 39342, loss = 0.0872, l1 loss=0.0361, grad loss=0.0512\n",
      "Got rmse 4.7832159996032715\n",
      "Got rmse 2.9639110565185547\n",
      "\n",
      "Epoch 249, Iteration 39500, loss = 0.0881, l1 loss=0.0364, grad loss=0.0517\n",
      "Got rmse 4.790780544281006\n",
      "Got rmse 2.963160276412964\n",
      "\n",
      "Epoch 250, Iteration 39658, loss = 0.0882, l1 loss=0.0364, grad loss=0.0518\n",
      "Got rmse 4.779682636260986\n",
      "Got rmse 2.9624061584472656\n",
      "\n",
      "Epoch 251, Iteration 39816, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.772182464599609\n",
      "Got rmse 2.961144208908081\n",
      "\n",
      "Epoch 252, Iteration 39974, loss = 0.0875, l1 loss=0.0362, grad loss=0.0514\n",
      "Got rmse 4.776422023773193\n",
      "Got rmse 2.9616878032684326\n",
      "\n",
      "Epoch 253, Iteration 40132, loss = 0.0877, l1 loss=0.0362, grad loss=0.0515\n",
      "Got rmse 4.792135715484619\n",
      "Got rmse 2.9613306522369385\n",
      "\n",
      "Epoch 254, Iteration 40290, loss = 0.0871, l1 loss=0.0360, grad loss=0.0512\n",
      "Got rmse 4.775998592376709\n",
      "Got rmse 2.9633283615112305\n",
      "\n",
      "Epoch 255, Iteration 40448, loss = 0.0879, l1 loss=0.0362, grad loss=0.0516\n",
      "Got rmse 4.783598899841309\n",
      "Got rmse 2.960000514984131\n",
      "\n",
      "Epoch 256, Iteration 40606, loss = 0.0878, l1 loss=0.0363, grad loss=0.0515\n",
      "Got rmse 4.784775733947754\n",
      "Got rmse 2.960178852081299\n",
      "\n",
      "Epoch 257, Iteration 40764, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.785780906677246\n",
      "Got rmse 2.963001012802124\n",
      "\n",
      "Epoch 258, Iteration 40922, loss = 0.0875, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.786355018615723\n",
      "Got rmse 2.9588332176208496\n",
      "\n",
      "Epoch 259, Iteration 41080, loss = 0.0878, l1 loss=0.0362, grad loss=0.0516\n",
      "Got rmse 4.77476692199707\n",
      "Got rmse 2.9600272178649902\n",
      "\n",
      "Epoch 260, Iteration 41238, loss = 0.0878, l1 loss=0.0361, grad loss=0.0516\n",
      "Got rmse 4.785487651824951\n",
      "Got rmse 2.959291458129883\n",
      "\n",
      "Epoch 261, Iteration 41396, loss = 0.0876, l1 loss=0.0362, grad loss=0.0514\n",
      "Got rmse 4.790685653686523\n",
      "Got rmse 2.9637808799743652\n",
      "\n",
      "Epoch 262, Iteration 41554, loss = 0.0874, l1 loss=0.0361, grad loss=0.0513\n",
      "Got rmse 4.781276226043701\n",
      "Got rmse 2.958111047744751\n",
      "\n",
      "Epoch 263, Iteration 41712, loss = 0.0877, l1 loss=0.0361, grad loss=0.0516\n",
      "Got rmse 4.782839775085449\n",
      "Got rmse 2.9568750858306885\n",
      "\n",
      "Epoch 264, Iteration 41870, loss = 0.0882, l1 loss=0.0363, grad loss=0.0519\n",
      "Got rmse 4.781843185424805\n",
      "Got rmse 2.957824230194092\n",
      "\n",
      "Epoch 265, Iteration 42028, loss = 0.0878, l1 loss=0.0362, grad loss=0.0516\n",
      "Got rmse 4.7821245193481445\n",
      "Got rmse 2.956782817840576\n",
      "\n",
      "Epoch 266, Iteration 42186, loss = 0.0879, l1 loss=0.0363, grad loss=0.0516\n",
      "Got rmse 4.786776542663574\n",
      "Got rmse 2.9575862884521484\n",
      "\n",
      "Epoch 267, Iteration 42344, loss = 0.0871, l1 loss=0.0360, grad loss=0.0512\n",
      "Got rmse 4.782107353210449\n",
      "Got rmse 2.956188201904297\n",
      "\n",
      "Epoch 268, Iteration 42502, loss = 0.0871, l1 loss=0.0360, grad loss=0.0511\n",
      "Got rmse 4.785644054412842\n",
      "Got rmse 2.956068515777588\n",
      "\n",
      "Epoch 269, Iteration 42660, loss = 0.0875, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.785436630249023\n",
      "Got rmse 2.9562795162200928\n",
      "\n",
      "Epoch 270, Iteration 42818, loss = 0.0877, l1 loss=0.0362, grad loss=0.0515\n",
      "Got rmse 4.785714626312256\n",
      "Got rmse 2.9564504623413086\n",
      "\n",
      "Epoch 271, Iteration 42976, loss = 0.0880, l1 loss=0.0363, grad loss=0.0517\n",
      "Got rmse 4.7916154861450195\n",
      "Got rmse 2.9553394317626953\n",
      "\n",
      "Epoch 272, Iteration 43134, loss = 0.0873, l1 loss=0.0359, grad loss=0.0514\n",
      "Got rmse 4.7776103019714355\n",
      "Got rmse 2.954866647720337\n",
      "\n",
      "Epoch 273, Iteration 43292, loss = 0.0876, l1 loss=0.0361, grad loss=0.0515\n",
      "Got rmse 4.7888288497924805\n",
      "Got rmse 2.955096960067749\n",
      "\n",
      "Epoch 274, Iteration 43450, loss = 0.0881, l1 loss=0.0363, grad loss=0.0518\n",
      "Got rmse 4.783231258392334\n",
      "Got rmse 2.954610824584961\n",
      "\n",
      "Epoch 275, Iteration 43608, loss = 0.0875, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.787755012512207\n",
      "Got rmse 2.954639434814453\n",
      "\n",
      "Epoch 276, Iteration 43766, loss = 0.0879, l1 loss=0.0363, grad loss=0.0516\n",
      "Got rmse 4.789579391479492\n",
      "Got rmse 2.9547383785247803\n",
      "\n",
      "Epoch 277, Iteration 43924, loss = 0.0875, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.786102771759033\n",
      "Got rmse 2.953939199447632\n",
      "\n",
      "Epoch 278, Iteration 44082, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.78352689743042\n",
      "Got rmse 2.953753709793091\n",
      "\n",
      "Epoch 279, Iteration 44240, loss = 0.0879, l1 loss=0.0363, grad loss=0.0516\n",
      "Got rmse 4.785676002502441\n",
      "Got rmse 2.954031229019165\n",
      "\n",
      "Epoch 280, Iteration 44398, loss = 0.0877, l1 loss=0.0362, grad loss=0.0515\n",
      "Got rmse 4.787195682525635\n",
      "Got rmse 2.953666925430298\n",
      "\n",
      "Epoch 281, Iteration 44556, loss = 0.0879, l1 loss=0.0362, grad loss=0.0517\n",
      "Got rmse 4.787787437438965\n",
      "Got rmse 2.9534764289855957\n",
      "\n",
      "Epoch 282, Iteration 44714, loss = 0.0880, l1 loss=0.0362, grad loss=0.0518\n",
      "Got rmse 4.784437656402588\n",
      "Got rmse 2.953105926513672\n",
      "\n",
      "Epoch 283, Iteration 44872, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.789175510406494\n",
      "Got rmse 2.9528141021728516\n",
      "\n",
      "Epoch 284, Iteration 45030, loss = 0.0874, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.786075115203857\n",
      "Got rmse 2.952979564666748\n",
      "\n",
      "Epoch 285, Iteration 45188, loss = 0.0870, l1 loss=0.0359, grad loss=0.0512\n",
      "Got rmse 4.787019729614258\n",
      "Got rmse 2.9525606632232666\n",
      "\n",
      "Epoch 286, Iteration 45346, loss = 0.0876, l1 loss=0.0361, grad loss=0.0515\n",
      "Got rmse 4.788187026977539\n",
      "Got rmse 2.952664375305176\n",
      "\n",
      "Epoch 287, Iteration 45504, loss = 0.0874, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.78896427154541\n",
      "Got rmse 2.95225191116333\n",
      "\n",
      "Epoch 288, Iteration 45662, loss = 0.0874, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.788198947906494\n",
      "Got rmse 2.952207326889038\n",
      "\n",
      "Epoch 289, Iteration 45820, loss = 0.0870, l1 loss=0.0359, grad loss=0.0512\n",
      "Got rmse 4.786358833312988\n",
      "Got rmse 2.9519903659820557\n",
      "\n",
      "Epoch 290, Iteration 45978, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.78914737701416\n",
      "Got rmse 2.9520692825317383\n",
      "\n",
      "Epoch 291, Iteration 46136, loss = 0.0876, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.788910865783691\n",
      "Got rmse 2.9517414569854736\n",
      "\n",
      "Epoch 292, Iteration 46294, loss = 0.0871, l1 loss=0.0359, grad loss=0.0512\n",
      "Got rmse 4.786552429199219\n",
      "Got rmse 2.9518887996673584\n",
      "\n",
      "Epoch 293, Iteration 46452, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.78869104385376\n",
      "Got rmse 2.9519455432891846\n",
      "\n",
      "Epoch 294, Iteration 46610, loss = 0.0874, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.791469573974609\n",
      "Got rmse 2.951552629470825\n",
      "\n",
      "Epoch 295, Iteration 46768, loss = 0.0875, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.792511940002441\n",
      "Got rmse 2.9513490200042725\n",
      "\n",
      "Epoch 296, Iteration 46926, loss = 0.0876, l1 loss=0.0361, grad loss=0.0515\n",
      "Got rmse 4.789050579071045\n",
      "Got rmse 2.9514048099517822\n",
      "\n",
      "Epoch 297, Iteration 47084, loss = 0.0875, l1 loss=0.0360, grad loss=0.0515\n",
      "Got rmse 4.791510105133057\n",
      "Got rmse 2.9514002799987793\n",
      "\n",
      "Epoch 298, Iteration 47242, loss = 0.0871, l1 loss=0.0359, grad loss=0.0512\n",
      "Got rmse 4.789833068847656\n",
      "Got rmse 2.951216459274292\n",
      "\n",
      "Epoch 299, Iteration 47400, loss = 0.0876, l1 loss=0.0361, grad loss=0.0515\n",
      "Got rmse 4.789525032043457\n",
      "Got rmse 2.951051950454712\n",
      "\n",
      "Epoch 300, Iteration 47558, loss = 0.0873, l1 loss=0.0359, grad loss=0.0514\n",
      "Got rmse 4.7897114753723145\n",
      "Got rmse 2.9510574340820312\n",
      "\n",
      "Epoch 301, Iteration 47716, loss = 0.0874, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.791640281677246\n",
      "Got rmse 2.951077461242676\n",
      "\n",
      "Epoch 302, Iteration 47874, loss = 0.0872, l1 loss=0.0359, grad loss=0.0513\n",
      "Got rmse 4.7892680168151855\n",
      "Got rmse 2.950864553451538\n",
      "\n",
      "Epoch 303, Iteration 48032, loss = 0.0872, l1 loss=0.0359, grad loss=0.0513\n",
      "Got rmse 4.791430473327637\n",
      "Got rmse 2.9507339000701904\n",
      "\n",
      "Epoch 304, Iteration 48190, loss = 0.0873, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.792174339294434\n",
      "Got rmse 2.950739622116089\n",
      "\n",
      "Epoch 305, Iteration 48348, loss = 0.0871, l1 loss=0.0359, grad loss=0.0512\n",
      "Got rmse 4.790923595428467\n",
      "Got rmse 2.9506983757019043\n",
      "\n",
      "Epoch 306, Iteration 48506, loss = 0.0876, l1 loss=0.0361, grad loss=0.0515\n",
      "Got rmse 4.791518211364746\n",
      "Got rmse 2.9506185054779053\n",
      "\n",
      "Epoch 307, Iteration 48664, loss = 0.0881, l1 loss=0.0363, grad loss=0.0518\n",
      "Got rmse 4.793636322021484\n",
      "Got rmse 2.950601577758789\n",
      "\n",
      "Epoch 308, Iteration 48822, loss = 0.0867, l1 loss=0.0357, grad loss=0.0510\n",
      "Got rmse 4.794144153594971\n",
      "Got rmse 2.9505257606506348\n",
      "\n",
      "Epoch 309, Iteration 48980, loss = 0.0875, l1 loss=0.0361, grad loss=0.0514\n",
      "Got rmse 4.793218612670898\n",
      "Got rmse 2.9504945278167725\n",
      "\n",
      "Epoch 310, Iteration 49138, loss = 0.0873, l1 loss=0.0359, grad loss=0.0513\n",
      "Got rmse 4.792876243591309\n",
      "Got rmse 2.950315475463867\n",
      "\n",
      "Epoch 311, Iteration 49296, loss = 0.0870, l1 loss=0.0358, grad loss=0.0512\n",
      "Got rmse 4.793684005737305\n",
      "Got rmse 2.950357437133789\n",
      "\n",
      "Epoch 312, Iteration 49454, loss = 0.0872, l1 loss=0.0359, grad loss=0.0513\n",
      "Got rmse 4.793778896331787\n",
      "Got rmse 2.9503695964813232\n",
      "\n",
      "Epoch 313, Iteration 49612, loss = 0.0870, l1 loss=0.0358, grad loss=0.0512\n",
      "Got rmse 4.79199743270874\n",
      "Got rmse 2.9502463340759277\n",
      "\n",
      "Epoch 314, Iteration 49770, loss = 0.0875, l1 loss=0.0360, grad loss=0.0515\n",
      "Got rmse 4.793790817260742\n",
      "Got rmse 2.950237512588501\n",
      "\n",
      "Epoch 315, Iteration 49928, loss = 0.0880, l1 loss=0.0362, grad loss=0.0518\n",
      "Got rmse 4.794123649597168\n",
      "Got rmse 2.950218677520752\n",
      "\n",
      "Epoch 316, Iteration 50086, loss = 0.0875, l1 loss=0.0360, grad loss=0.0516\n",
      "Got rmse 4.795501708984375\n",
      "Got rmse 2.9501638412475586\n",
      "\n",
      "Epoch 317, Iteration 50244, loss = 0.0878, l1 loss=0.0362, grad loss=0.0516\n",
      "Got rmse 4.79502010345459\n",
      "Got rmse 2.950143337249756\n",
      "\n",
      "Epoch 318, Iteration 50402, loss = 0.0878, l1 loss=0.0362, grad loss=0.0516\n",
      "Got rmse 4.7937469482421875\n",
      "Got rmse 2.950071096420288\n",
      "\n",
      "Epoch 319, Iteration 50560, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.795012950897217\n",
      "Got rmse 2.9500439167022705\n",
      "\n",
      "Epoch 320, Iteration 50718, loss = 0.0879, l1 loss=0.0362, grad loss=0.0517\n",
      "Got rmse 4.794740676879883\n",
      "Got rmse 2.9500186443328857\n",
      "\n",
      "Epoch 321, Iteration 50876, loss = 0.0873, l1 loss=0.0359, grad loss=0.0514\n",
      "Got rmse 4.795143127441406\n",
      "Got rmse 2.9499964714050293\n",
      "\n",
      "Epoch 322, Iteration 51034, loss = 0.0876, l1 loss=0.0361, grad loss=0.0515\n",
      "Got rmse 4.794692516326904\n",
      "Got rmse 2.9500014781951904\n",
      "\n",
      "Epoch 323, Iteration 51192, loss = 0.0873, l1 loss=0.0359, grad loss=0.0514\n",
      "Got rmse 4.794731140136719\n",
      "Got rmse 2.9499471187591553\n",
      "\n",
      "Epoch 324, Iteration 51350, loss = 0.0875, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.794913291931152\n",
      "Got rmse 2.949941635131836\n",
      "\n",
      "Epoch 325, Iteration 51508, loss = 0.0874, l1 loss=0.0359, grad loss=0.0514\n",
      "Got rmse 4.794803142547607\n",
      "Got rmse 2.949904441833496\n",
      "\n",
      "Epoch 326, Iteration 51666, loss = 0.0874, l1 loss=0.0359, grad loss=0.0515\n",
      "Got rmse 4.795393943786621\n",
      "Got rmse 2.9499099254608154\n",
      "\n",
      "Epoch 327, Iteration 51824, loss = 0.0879, l1 loss=0.0362, grad loss=0.0517\n",
      "Got rmse 4.794989585876465\n",
      "Got rmse 2.949890375137329\n",
      "\n",
      "Epoch 328, Iteration 51982, loss = 0.0878, l1 loss=0.0361, grad loss=0.0517\n",
      "Got rmse 4.7953338623046875\n",
      "Got rmse 2.949864625930786\n",
      "\n",
      "Epoch 329, Iteration 52140, loss = 0.0875, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.7954277992248535\n",
      "Got rmse 2.949857711791992\n",
      "\n",
      "Epoch 330, Iteration 52298, loss = 0.0874, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.795610427856445\n",
      "Got rmse 2.949835777282715\n",
      "\n",
      "Epoch 331, Iteration 52456, loss = 0.0876, l1 loss=0.0360, grad loss=0.0516\n",
      "Got rmse 4.79551887512207\n",
      "Got rmse 2.9498250484466553\n",
      "\n",
      "Epoch 332, Iteration 52614, loss = 0.0876, l1 loss=0.0360, grad loss=0.0516\n",
      "Got rmse 4.795833587646484\n",
      "Got rmse 2.949807643890381\n",
      "\n",
      "Epoch 333, Iteration 52772, loss = 0.0872, l1 loss=0.0359, grad loss=0.0514\n",
      "Got rmse 4.796200275421143\n",
      "Got rmse 2.949803113937378\n",
      "\n",
      "Epoch 334, Iteration 52930, loss = 0.0879, l1 loss=0.0362, grad loss=0.0517\n",
      "Got rmse 4.79587459564209\n",
      "Got rmse 2.949798583984375\n",
      "\n",
      "Epoch 335, Iteration 53088, loss = 0.0874, l1 loss=0.0359, grad loss=0.0515\n",
      "Got rmse 4.795647144317627\n",
      "Got rmse 2.9497759342193604\n",
      "\n",
      "Epoch 336, Iteration 53246, loss = 0.0875, l1 loss=0.0359, grad loss=0.0515\n",
      "Got rmse 4.796113967895508\n",
      "Got rmse 2.9497878551483154\n",
      "\n",
      "Epoch 337, Iteration 53404, loss = 0.0878, l1 loss=0.0361, grad loss=0.0517\n",
      "Got rmse 4.796263217926025\n",
      "Got rmse 2.9497663974761963\n",
      "\n",
      "Epoch 338, Iteration 53562, loss = 0.0872, l1 loss=0.0359, grad loss=0.0513\n",
      "Got rmse 4.796324729919434\n",
      "Got rmse 2.9497618675231934\n",
      "\n",
      "Epoch 339, Iteration 53720, loss = 0.0879, l1 loss=0.0362, grad loss=0.0517\n",
      "Got rmse 4.796232223510742\n",
      "Got rmse 2.949754238128662\n",
      "\n",
      "Epoch 340, Iteration 53878, loss = 0.0877, l1 loss=0.0362, grad loss=0.0515\n",
      "Got rmse 4.7961273193359375\n",
      "Got rmse 2.9497568607330322\n",
      "\n",
      "Epoch 341, Iteration 54036, loss = 0.0868, l1 loss=0.0358, grad loss=0.0510\n",
      "Got rmse 4.796089172363281\n",
      "Got rmse 2.94974422454834\n",
      "\n",
      "Epoch 342, Iteration 54194, loss = 0.0873, l1 loss=0.0360, grad loss=0.0513\n",
      "Got rmse 4.795996189117432\n",
      "Got rmse 2.9497437477111816\n",
      "\n",
      "Epoch 343, Iteration 54352, loss = 0.0869, l1 loss=0.0358, grad loss=0.0511\n",
      "Got rmse 4.796222686767578\n",
      "Got rmse 2.949735403060913\n",
      "\n",
      "Epoch 344, Iteration 54510, loss = 0.0868, l1 loss=0.0358, grad loss=0.0510\n",
      "Got rmse 4.796172618865967\n",
      "Got rmse 2.9497323036193848\n",
      "\n",
      "Epoch 345, Iteration 54668, loss = 0.0878, l1 loss=0.0361, grad loss=0.0517\n",
      "Got rmse 4.7960896492004395\n",
      "Got rmse 2.9497334957122803\n",
      "\n",
      "Epoch 346, Iteration 54826, loss = 0.0874, l1 loss=0.0359, grad loss=0.0515\n",
      "Got rmse 4.796403408050537\n",
      "Got rmse 2.9497246742248535\n",
      "\n",
      "Epoch 347, Iteration 54984, loss = 0.0878, l1 loss=0.0361, grad loss=0.0517\n",
      "Got rmse 4.7961955070495605\n",
      "Got rmse 2.949723243713379\n",
      "\n",
      "Epoch 348, Iteration 55142, loss = 0.0877, l1 loss=0.0361, grad loss=0.0516\n",
      "Got rmse 4.796060562133789\n",
      "Got rmse 2.949721097946167\n",
      "\n",
      "Epoch 349, Iteration 55300, loss = 0.0873, l1 loss=0.0360, grad loss=0.0514\n",
      "Got rmse 4.796593189239502\n",
      "Got rmse 2.9497270584106445\n",
      "\n",
      "training stop at epoch: 349\n",
      "training stop at epoch: tensor(0.9993, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'epochs': 350,\n",
    "    'lr_max': 5e-4,\n",
    "    'lr_min': 2.5e-6,\n",
    "    'batch_size': 8,\n",
    "    'L2_norm'   : 0,\n",
    "    'verbose': False,\n",
    "    'DF'     : False,\n",
    "    'schedule': [],\n",
    "    'grid_space': 16**3,\n",
    "    'learning_rate_decay': 0.5,\n",
    "    'skip_spacing': 2,\n",
    "    'num_repeat'  : 1,\n",
    "    'num_block'   : 3,\n",
    "    'device'      : device,\n",
    "}\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "\n",
    "    # split the dataset to train, validation, test\n",
    "    train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "    # normailzation\n",
    "    extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "    config['maxB'] = extremes[2]\n",
    "    config['minB'] = extremes[3]\n",
    "    config['train_set'] = train_set \n",
    "    config['valid_set'] = valid_set\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    print(\"----------------------------\")\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare = train_GM(\n",
    "        config=config)\n",
    "        \n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "# plt.ylim([0,10])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([15,20])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "# plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop]*1e6,'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop]*1e6,'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(epoch_stop)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
