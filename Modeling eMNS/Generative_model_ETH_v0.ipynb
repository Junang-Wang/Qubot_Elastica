{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ETH data to CNN generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReadData import ReadETHFolder, ReadETHFile\n",
    "foldername=\"./ETH_Data/v/\"\n",
    "currentname = \"./ETH_Data/\"+\"currents_3787.h5\"\n",
    "file_num = 1000\n",
    "data_shape = (16,16,16,3)\n",
    "Bfield = torch.tensor(ReadETHFolder(foldername,file_num, data_shape)).permute(0,4,1,2,3)\n",
    "current = torch.tensor(ReadETHFile(currentname))\n",
    "current = current[0:Bfield.shape[0],:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 16, 16, 16])\n",
      "torch.Size([1000, 8])\n"
     ]
    }
   ],
   "source": [
    "print(Bfield.shape)\n",
    "print(current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 16, 16, 16])\n",
      "tensor(-0.4876, dtype=torch.float64)\n",
      "tensor(0.0031, dtype=torch.float64)\n",
      "tensor(55.3219, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Bfield.shape)\n",
    "print(Bfield.mean()*1e3)\n",
    "print(Bfield.var())\n",
    "print(Bfield.std()*1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4591],\n",
      "        [0.4241],\n",
      "        [0.3447]], dtype=torch.float64)\n",
      "tensor([[-0.4902],\n",
      "        [-0.4390],\n",
      "        [-0.3529]], dtype=torch.float64)\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#data normalization\n",
    "#find min and max value of input position and Bfield\n",
    "max_current, max_current_index = torch.max(current, dim=0, keepdim=True)\n",
    "# print(max_current)\n",
    "min_current, min_current_index = torch.min(current, dim=0, keepdim=True)\n",
    "# print(min_current)\n",
    "\n",
    "max_Bfield, max_Bfield_index = torch.max(Bfield.transpose(0,1).reshape(3,-1), dim=1, keepdim=True)\n",
    "print(max_Bfield)\n",
    "min_Bfield, min_Bfield_index = torch.min(Bfield.transpose(0,1).reshape(3,-1), dim=1, keepdim=True)\n",
    "print(min_Bfield)\n",
    "\n",
    "dimB = Bfield.shape\n",
    "dimc = current.shape\n",
    "\n",
    "minB=min_Bfield.expand(3,int(Bfield.numel()/3)).reshape(3,dimB[0],dimB[2],dimB[3],dimB[4]).transpose(0,1)\n",
    "maxB=max_Bfield.expand(3,int(Bfield.numel()/3)).reshape(3,dimB[0],dimB[2],dimB[3],dimB[4]).transpose(0,1)\n",
    "\n",
    "ave_current=0.5*(max_current.expand(dimc[0],dimc[1])+min_current.expand(dimc[0],dimc[1]))\n",
    "diff_current=0.5*(max_current.expand(dimc[0],dimc[1])-min_current.expand(dimc[0],dimc[1]))\n",
    "\n",
    "current_norm = (current-ave_current)/diff_current\n",
    "Bfield_norm = (Bfield-(minB+maxB)*0.5)/(0.5*(maxB-minB))\n",
    "\n",
    "print(min_current.shape)\n",
    "print(max_current.shape)\n",
    "print(min_Bfield.shape)\n",
    "print(max_Bfield.shape)\n",
    "\n",
    "torch.save(min_current, \"./normalize_data/cnn_min_current_ETH.pt\")\n",
    "torch.save(max_current, \"./normalize_data/cnn_max_current_ETH.pt\")\n",
    "torch.save(min_Bfield, \"./normalize_data/cnn_min_Bfield_ETH.pt\")\n",
    "torch.save(max_Bfield, \"./normalize_data/cnn_max_Bfield_ETH.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MaxB=maxB.cuda(0)\n",
    "MinB=minB.cuda(0)\n",
    "print(MaxB.device)\n",
    "print(MinB.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative_net(\n",
      "  (proj): Linear(in_features=8, out_features=4096, bias=True)\n",
      "  (conv3d): Conv3d(64, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (total_net): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=4096, bias=True)\n",
      "    (1): Unflatten(dim=1, unflattened_size=(64, 4, 4, 4))\n",
      "    (2): BigBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (1): UpsampleBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): BigBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (1): UpsampleBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Conv3d(64, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop import train_part_GM,get_mean_of_dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 8\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,4,1) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,2) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_percent 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "----------------------------\n",
      "epoch================================================= 0\n",
      "Epoch 0, Iteration 113, loss = 1.3291, l1 loss=0.7287, grad loss=0.6004\n",
      "Got rmse 538.5375693768783\n",
      "Got rmse 492.97613408708963\n",
      "\n",
      "epoch================================================= 1\n",
      "Epoch 1, Iteration 226, loss = 1.4064, l1 loss=0.7725, grad loss=0.6339\n",
      "Got rmse 353.9587398686266\n",
      "Got rmse 322.3187487010227\n",
      "\n",
      "epoch================================================= 2\n",
      "Epoch 2, Iteration 339, loss = 0.9817, l1 loss=0.5304, grad loss=0.4513\n",
      "Got rmse 262.817792583636\n",
      "Got rmse 238.19523379964588\n",
      "\n",
      "epoch================================================= 3\n",
      "Epoch 3, Iteration 452, loss = 0.6410, l1 loss=0.3418, grad loss=0.2993\n",
      "Got rmse 202.63789406332802\n",
      "Got rmse 182.9357077497776\n",
      "\n",
      "epoch================================================= 4\n",
      "Epoch 4, Iteration 565, loss = 0.6682, l1 loss=0.3510, grad loss=0.3171\n",
      "Got rmse 159.7087844664934\n",
      "Got rmse 143.86488256867247\n",
      "\n",
      "epoch================================================= 5\n",
      "Epoch 5, Iteration 678, loss = 0.4955, l1 loss=0.2570, grad loss=0.2385\n",
      "Got rmse 127.17883897272722\n",
      "Got rmse 114.41116589418573\n",
      "\n",
      "epoch================================================= 6\n",
      "Epoch 6, Iteration 791, loss = 0.4029, l1 loss=0.2079, grad loss=0.1950\n",
      "Got rmse 103.14961163321323\n",
      "Got rmse 92.6583859196713\n",
      "\n",
      "epoch================================================= 7\n",
      "Epoch 7, Iteration 904, loss = 0.2923, l1 loss=0.1487, grad loss=0.1435\n",
      "Got rmse 85.24854729577726\n",
      "Got rmse 76.30402859681998\n",
      "\n",
      "epoch================================================= 8\n",
      "Epoch 8, Iteration 1017, loss = 0.1712, l1 loss=0.0871, grad loss=0.0841\n",
      "Got rmse 70.97521394042913\n",
      "Got rmse 63.43748989000087\n",
      "\n",
      "epoch================================================= 9\n",
      "Epoch 9, Iteration 1130, loss = 0.2346, l1 loss=0.1177, grad loss=0.1168\n",
      "Got rmse 61.09773460988471\n",
      "Got rmse 54.52536511717175\n",
      "\n",
      "epoch================================================= 10\n",
      "Epoch 10, Iteration 1243, loss = 0.0989, l1 loss=0.0498, grad loss=0.0491\n",
      "Got rmse 52.868197883251575\n",
      "Got rmse 47.01539262420638\n",
      "\n",
      "epoch================================================= 11\n",
      "Epoch 11, Iteration 1356, loss = 0.1281, l1 loss=0.0643, grad loss=0.0639\n",
      "Got rmse 47.43756489677484\n",
      "Got rmse 42.05768982957004\n",
      "\n",
      "epoch================================================= 12\n",
      "Epoch 12, Iteration 1469, loss = 0.1355, l1 loss=0.0680, grad loss=0.0675\n",
      "Got rmse 42.653044139550126\n",
      "Got rmse 37.67958191843042\n",
      "\n",
      "epoch================================================= 13\n",
      "Epoch 13, Iteration 1582, loss = 0.0872, l1 loss=0.0436, grad loss=0.0435\n",
      "Got rmse 38.954249510604825\n",
      "Got rmse 34.28643638342876\n",
      "\n",
      "epoch================================================= 14\n",
      "Epoch 14, Iteration 1695, loss = 0.0810, l1 loss=0.0407, grad loss=0.0403\n",
      "Got rmse 36.350524010165906\n",
      "Got rmse 31.911319619566022\n",
      "\n",
      "epoch================================================= 15\n",
      "Epoch 15, Iteration 1808, loss = 0.0875, l1 loss=0.0437, grad loss=0.0438\n",
      "Got rmse 33.48963906021664\n",
      "Got rmse 29.29215461646145\n",
      "\n",
      "epoch================================================= 16\n",
      "Epoch 16, Iteration 1921, loss = 0.0791, l1 loss=0.0395, grad loss=0.0396\n",
      "Got rmse 31.64092817479248\n",
      "Got rmse 27.610038275468565\n",
      "\n",
      "epoch================================================= 17\n",
      "Epoch 17, Iteration 2034, loss = 0.1127, l1 loss=0.0562, grad loss=0.0565\n",
      "Got rmse 30.978294464270324\n",
      "Got rmse 27.166496427681302\n",
      "\n",
      "epoch================================================= 18\n",
      "Epoch 18, Iteration 2147, loss = 0.0571, l1 loss=0.0285, grad loss=0.0286\n",
      "Got rmse 28.366581302626873\n",
      "Got rmse 24.641790884709284\n",
      "\n",
      "epoch================================================= 19\n",
      "Epoch 19, Iteration 2260, loss = 0.0872, l1 loss=0.0434, grad loss=0.0438\n",
      "Got rmse 27.715875865694446\n",
      "Got rmse 24.079443363429423\n",
      "\n",
      "epoch================================================= 20\n",
      "Epoch 20, Iteration 2373, loss = 0.0565, l1 loss=0.0281, grad loss=0.0284\n",
      "Got rmse 25.80643059146786\n",
      "Got rmse 22.309790424111508\n",
      "\n",
      "epoch================================================= 21\n",
      "Epoch 21, Iteration 2486, loss = 0.0761, l1 loss=0.0394, grad loss=0.0367\n",
      "Got rmse 26.200928375007052\n",
      "Got rmse 22.711593257931025\n",
      "\n",
      "epoch================================================= 22\n",
      "Epoch 22, Iteration 2599, loss = 0.0846, l1 loss=0.0420, grad loss=0.0427\n",
      "Got rmse 24.25048917126724\n",
      "Got rmse 20.944901892341356\n",
      "\n",
      "epoch================================================= 23\n",
      "Epoch 23, Iteration 2712, loss = 0.0846, l1 loss=0.0419, grad loss=0.0428\n",
      "Got rmse 23.66164520402992\n",
      "Got rmse 20.446011808262135\n",
      "\n",
      "epoch================================================= 24\n",
      "Epoch 24, Iteration 2825, loss = 0.0504, l1 loss=0.0257, grad loss=0.0248\n",
      "Got rmse 22.353439461914373\n",
      "Got rmse 19.246423101457\n",
      "\n",
      "epoch================================================= 25\n",
      "Epoch 25, Iteration 2938, loss = 0.0876, l1 loss=0.0434, grad loss=0.0441\n",
      "Got rmse 22.89034474745377\n",
      "Got rmse 19.8190887643514\n",
      "\n",
      "epoch================================================= 26\n",
      "Epoch 26, Iteration 3051, loss = 0.0583, l1 loss=0.0289, grad loss=0.0294\n",
      "Got rmse 21.46621357333363\n",
      "Got rmse 18.476487845585865\n",
      "\n",
      "epoch================================================= 27\n",
      "Epoch 27, Iteration 3164, loss = 0.0671, l1 loss=0.0371, grad loss=0.0301\n",
      "Got rmse 22.61981396661456\n",
      "Got rmse 19.583387698484724\n",
      "\n",
      "epoch================================================= 28\n",
      "Epoch 28, Iteration 3277, loss = 0.0809, l1 loss=0.0400, grad loss=0.0409\n",
      "Got rmse 20.296638862423617\n",
      "Got rmse 17.407121682114116\n",
      "\n",
      "epoch================================================= 29\n",
      "Epoch 29, Iteration 3390, loss = 0.0581, l1 loss=0.0292, grad loss=0.0289\n",
      "Got rmse 20.6028940852782\n",
      "Got rmse 17.671509842170302\n",
      "\n",
      "epoch================================================= 30\n",
      "Epoch 30, Iteration 3503, loss = 0.0435, l1 loss=0.0216, grad loss=0.0219\n",
      "Got rmse 19.31368404935527\n",
      "Got rmse 16.4743259631437\n",
      "\n",
      "epoch================================================= 31\n",
      "Epoch 31, Iteration 3616, loss = 0.0583, l1 loss=0.0291, grad loss=0.0292\n",
      "Got rmse 19.876661514815876\n",
      "Got rmse 17.050007781106824\n",
      "\n",
      "epoch================================================= 32\n",
      "Epoch 32, Iteration 3729, loss = 0.0459, l1 loss=0.0229, grad loss=0.0230\n",
      "Got rmse 18.485568053529686\n",
      "Got rmse 15.746324735117046\n",
      "\n",
      "epoch================================================= 33\n",
      "Epoch 33, Iteration 3842, loss = 0.0625, l1 loss=0.0312, grad loss=0.0312\n",
      "Got rmse 18.295530708307282\n",
      "Got rmse 15.50893927159622\n",
      "\n",
      "epoch================================================= 34\n",
      "Epoch 34, Iteration 3955, loss = 0.0553, l1 loss=0.0281, grad loss=0.0272\n",
      "Got rmse 18.5704302258588\n",
      "Got rmse 15.841968321108723\n",
      "\n",
      "epoch================================================= 35\n",
      "Epoch 35, Iteration 4068, loss = 0.0445, l1 loss=0.0222, grad loss=0.0223\n",
      "Got rmse 18.07327376515736\n",
      "Got rmse 15.329284432594331\n",
      "\n",
      "epoch================================================= 36\n",
      "Epoch 36, Iteration 4181, loss = 0.0630, l1 loss=0.0326, grad loss=0.0304\n",
      "Got rmse 19.30944251660095\n",
      "Got rmse 16.6232709782229\n",
      "\n",
      "epoch================================================= 37\n",
      "Epoch 37, Iteration 4294, loss = 0.0633, l1 loss=0.0316, grad loss=0.0317\n",
      "Got rmse 18.129839457275914\n",
      "Got rmse 15.572433003030525\n",
      "\n",
      "epoch================================================= 38\n",
      "Epoch 38, Iteration 4407, loss = 0.0397, l1 loss=0.0212, grad loss=0.0185\n",
      "Got rmse 16.71662574518427\n",
      "Got rmse 14.081894948683383\n",
      "\n",
      "epoch================================================= 39\n",
      "Epoch 39, Iteration 4520, loss = 0.0283, l1 loss=0.0142, grad loss=0.0141\n",
      "Got rmse 16.41938076876266\n",
      "Got rmse 13.881588093984396\n",
      "\n",
      "epoch================================================= 40\n",
      "Epoch 40, Iteration 4633, loss = 0.0425, l1 loss=0.0213, grad loss=0.0211\n",
      "Got rmse 17.375130949629607\n",
      "Got rmse 14.987352605537927\n",
      "\n",
      "epoch================================================= 41\n",
      "Epoch 41, Iteration 4746, loss = 0.0493, l1 loss=0.0248, grad loss=0.0245\n",
      "Got rmse 16.43120762291563\n",
      "Got rmse 13.899981584098713\n",
      "\n",
      "epoch================================================= 42\n",
      "Epoch 42, Iteration 4859, loss = 0.0392, l1 loss=0.0198, grad loss=0.0194\n",
      "Got rmse 15.558381911646023\n",
      "Got rmse 13.08429551981085\n",
      "\n",
      "epoch================================================= 43\n",
      "Epoch 43, Iteration 4972, loss = 0.0487, l1 loss=0.0245, grad loss=0.0242\n",
      "Got rmse 16.258300887132002\n",
      "Got rmse 13.7444849541199\n",
      "\n",
      "epoch================================================= 44\n",
      "Epoch 44, Iteration 5085, loss = 0.0342, l1 loss=0.0169, grad loss=0.0173\n",
      "Got rmse 15.29290737880726\n",
      "Got rmse 12.808130322364272\n",
      "\n",
      "epoch================================================= 45\n",
      "Epoch 45, Iteration 5198, loss = 0.0456, l1 loss=0.0226, grad loss=0.0230\n",
      "Got rmse 15.853820865855042\n",
      "Got rmse 13.475860265225556\n",
      "\n",
      "epoch================================================= 46\n",
      "Epoch 46, Iteration 5311, loss = 0.0429, l1 loss=0.0217, grad loss=0.0213\n",
      "Got rmse 15.709965144172914\n",
      "Got rmse 13.292046396635913\n",
      "\n",
      "epoch================================================= 47\n",
      "Epoch 47, Iteration 5424, loss = 0.0298, l1 loss=0.0157, grad loss=0.0141\n",
      "Got rmse 14.56204892214636\n",
      "Got rmse 12.214804683915379\n",
      "\n",
      "epoch================================================= 48\n",
      "Epoch 48, Iteration 5537, loss = 0.0503, l1 loss=0.0255, grad loss=0.0247\n",
      "Got rmse 15.602068214778873\n",
      "Got rmse 13.319627371513818\n",
      "\n",
      "epoch================================================= 49\n",
      "Epoch 49, Iteration 5650, loss = 0.0327, l1 loss=0.0166, grad loss=0.0161\n",
      "Got rmse 14.487834586714836\n",
      "Got rmse 12.115074699845449\n",
      "\n",
      "epoch================================================= 50\n",
      "Epoch 50, Iteration 5763, loss = 0.0434, l1 loss=0.0236, grad loss=0.0199\n",
      "Got rmse 14.680953132543927\n",
      "Got rmse 12.30660478096999\n",
      "\n",
      "epoch================================================= 51\n",
      "Epoch 51, Iteration 5876, loss = 0.0450, l1 loss=0.0231, grad loss=0.0219\n",
      "Got rmse 14.041923967376698\n",
      "Got rmse 11.81834389370854\n",
      "\n",
      "epoch================================================= 52\n",
      "Epoch 52, Iteration 5989, loss = 0.0466, l1 loss=0.0234, grad loss=0.0233\n",
      "Got rmse 14.03756367013036\n",
      "Got rmse 11.732896378345442\n",
      "\n",
      "epoch================================================= 53\n",
      "Epoch 53, Iteration 6102, loss = 0.0487, l1 loss=0.0246, grad loss=0.0241\n",
      "Got rmse 15.31266648791285\n",
      "Got rmse 13.143552350945065\n",
      "\n",
      "epoch================================================= 54\n",
      "Epoch 54, Iteration 6215, loss = 0.0435, l1 loss=0.0218, grad loss=0.0217\n",
      "Got rmse 14.047104832498492\n",
      "Got rmse 11.721969256026458\n",
      "\n",
      "epoch================================================= 55\n",
      "Epoch 55, Iteration 6328, loss = 0.0505, l1 loss=0.0257, grad loss=0.0248\n",
      "Got rmse 14.406357477181837\n",
      "Got rmse 12.07916684023662\n",
      "\n",
      "epoch================================================= 56\n",
      "Epoch 56, Iteration 6441, loss = 0.0480, l1 loss=0.0252, grad loss=0.0228\n",
      "Got rmse 17.36080005491758\n",
      "Got rmse 15.19753633465575\n",
      "\n",
      "epoch================================================= 57\n",
      "Epoch 57, Iteration 6554, loss = 0.0517, l1 loss=0.0272, grad loss=0.0245\n",
      "Got rmse 14.997803876544442\n",
      "Got rmse 12.85836170548385\n",
      "\n",
      "epoch================================================= 58\n",
      "Epoch 58, Iteration 6667, loss = 0.0456, l1 loss=0.0226, grad loss=0.0230\n",
      "Got rmse 13.096259844843633\n",
      "Got rmse 10.96539313954253\n",
      "\n",
      "epoch================================================= 59\n",
      "Epoch 59, Iteration 6780, loss = 0.0408, l1 loss=0.0207, grad loss=0.0200\n",
      "Got rmse 13.614841138829105\n",
      "Got rmse 11.532485010827132\n",
      "\n",
      "epoch================================================= 60\n",
      "Epoch 60, Iteration 6893, loss = 0.0245, l1 loss=0.0129, grad loss=0.0115\n",
      "Got rmse 12.745280677851255\n",
      "Got rmse 10.468879133576833\n",
      "\n",
      "epoch================================================= 61\n",
      "Epoch 61, Iteration 7006, loss = 0.0432, l1 loss=0.0220, grad loss=0.0212\n",
      "Got rmse 13.407520960978225\n",
      "Got rmse 11.272095593047412\n",
      "\n",
      "epoch================================================= 62\n",
      "Epoch 62, Iteration 7119, loss = 0.0380, l1 loss=0.0200, grad loss=0.0180\n",
      "Got rmse 12.912929865191554\n",
      "Got rmse 10.670298247599884\n",
      "\n",
      "epoch================================================= 63\n",
      "Epoch 63, Iteration 7232, loss = 0.0387, l1 loss=0.0192, grad loss=0.0195\n",
      "Got rmse 12.628063920493856\n",
      "Got rmse 10.557166024208176\n",
      "\n",
      "epoch================================================= 64\n",
      "Epoch 64, Iteration 7345, loss = 0.0321, l1 loss=0.0162, grad loss=0.0158\n",
      "Got rmse 12.070988976909518\n",
      "Got rmse 10.000676873635635\n",
      "\n",
      "epoch================================================= 65\n",
      "Epoch 65, Iteration 7458, loss = 0.0495, l1 loss=0.0253, grad loss=0.0242\n",
      "Got rmse 14.754008155517054\n",
      "Got rmse 12.691712356296343\n",
      "\n",
      "epoch================================================= 66\n",
      "Epoch 66, Iteration 7571, loss = 0.0458, l1 loss=0.0252, grad loss=0.0206\n",
      "Got rmse 13.922339886392242\n",
      "Got rmse 12.089041370466171\n",
      "\n",
      "epoch================================================= 67\n",
      "Epoch 67, Iteration 7684, loss = 0.0347, l1 loss=0.0191, grad loss=0.0157\n",
      "Got rmse 12.607759883060158\n",
      "Got rmse 10.487419749352417\n",
      "\n",
      "epoch================================================= 68\n",
      "Epoch 68, Iteration 7797, loss = 0.0421, l1 loss=0.0222, grad loss=0.0199\n",
      "Got rmse 13.538257744220106\n",
      "Got rmse 11.49894622341793\n",
      "\n",
      "epoch================================================= 69\n",
      "Epoch 69, Iteration 7910, loss = 0.0383, l1 loss=0.0199, grad loss=0.0185\n",
      "Got rmse 11.98635780410392\n",
      "Got rmse 10.04656427037671\n",
      "\n",
      "epoch================================================= 70\n",
      "Epoch 70, Iteration 8023, loss = 0.0402, l1 loss=0.0203, grad loss=0.0198\n",
      "Got rmse 12.236278244167913\n",
      "Got rmse 10.202623012922706\n",
      "\n",
      "epoch================================================= 71\n",
      "Epoch 71, Iteration 8136, loss = 0.0387, l1 loss=0.0196, grad loss=0.0191\n",
      "Got rmse 12.229419771385814\n",
      "Got rmse 10.22622116416559\n",
      "\n",
      "epoch================================================= 72\n",
      "Epoch 72, Iteration 8249, loss = 0.0396, l1 loss=0.0203, grad loss=0.0193\n",
      "Got rmse 11.68556055402614\n",
      "Got rmse 9.656685366145403\n",
      "\n",
      "epoch================================================= 73\n",
      "Epoch 73, Iteration 8362, loss = 0.0320, l1 loss=0.0162, grad loss=0.0159\n",
      "Got rmse 12.349744346465249\n",
      "Got rmse 10.325926436607427\n",
      "\n",
      "epoch================================================= 74\n",
      "Epoch 74, Iteration 8475, loss = 0.0331, l1 loss=0.0183, grad loss=0.0148\n",
      "Got rmse 11.361357450931617\n",
      "Got rmse 9.289969468541571\n",
      "\n",
      "epoch================================================= 75\n",
      "Epoch 75, Iteration 8588, loss = 0.0301, l1 loss=0.0152, grad loss=0.0149\n",
      "Got rmse 11.185405705929089\n",
      "Got rmse 9.198131716806039\n",
      "\n",
      "epoch================================================= 76\n",
      "Epoch 76, Iteration 8701, loss = 0.0184, l1 loss=0.0097, grad loss=0.0087\n",
      "Got rmse 11.071610649515758\n",
      "Got rmse 9.10569771227846\n",
      "\n",
      "epoch================================================= 77\n",
      "Epoch 77, Iteration 8814, loss = 0.0296, l1 loss=0.0165, grad loss=0.0131\n",
      "Got rmse 11.718844586870285\n",
      "Got rmse 9.769085538868124\n",
      "\n",
      "epoch================================================= 78\n",
      "Epoch 78, Iteration 8927, loss = 0.0234, l1 loss=0.0119, grad loss=0.0115\n",
      "Got rmse 11.088583699427593\n",
      "Got rmse 9.157358947573218\n",
      "\n",
      "epoch================================================= 79\n",
      "Epoch 79, Iteration 9040, loss = 0.0274, l1 loss=0.0138, grad loss=0.0136\n",
      "Got rmse 10.665055674397685\n",
      "Got rmse 8.775267090467898\n",
      "\n",
      "epoch================================================= 80\n",
      "Epoch 80, Iteration 9153, loss = 0.0175, l1 loss=0.0089, grad loss=0.0085\n",
      "Got rmse 10.717802709316723\n",
      "Got rmse 8.8083625046271\n",
      "\n",
      "epoch================================================= 81\n",
      "Epoch 81, Iteration 9266, loss = 0.0329, l1 loss=0.0166, grad loss=0.0163\n",
      "Got rmse 11.455470895986918\n",
      "Got rmse 9.61075487449887\n",
      "\n",
      "epoch================================================= 82\n",
      "Epoch 82, Iteration 9379, loss = 0.0360, l1 loss=0.0180, grad loss=0.0181\n",
      "Got rmse 11.233117360576644\n",
      "Got rmse 9.337787550789107\n",
      "\n",
      "epoch================================================= 83\n",
      "Epoch 83, Iteration 9492, loss = 0.0299, l1 loss=0.0157, grad loss=0.0141\n",
      "Got rmse 10.540538561984183\n",
      "Got rmse 8.707416366325095\n",
      "\n",
      "epoch================================================= 84\n",
      "Epoch 84, Iteration 9605, loss = 0.0346, l1 loss=0.0175, grad loss=0.0171\n",
      "Got rmse 10.693505393761862\n",
      "Got rmse 8.897782702694538\n",
      "\n",
      "epoch================================================= 85\n",
      "Epoch 85, Iteration 9718, loss = 0.0366, l1 loss=0.0209, grad loss=0.0157\n",
      "Got rmse 12.641812785203246\n",
      "Got rmse 10.958028561249634\n",
      "\n",
      "epoch================================================= 86\n",
      "Epoch 86, Iteration 9831, loss = 0.0383, l1 loss=0.0198, grad loss=0.0186\n",
      "Got rmse 12.499640058942765\n",
      "Got rmse 10.711378514060426\n",
      "\n",
      "epoch================================================= 87\n",
      "Epoch 87, Iteration 9944, loss = 0.0342, l1 loss=0.0203, grad loss=0.0139\n",
      "Got rmse 11.236310587983137\n",
      "Got rmse 9.499295362145933\n",
      "\n",
      "epoch================================================= 88\n",
      "Epoch 88, Iteration 10057, loss = 0.0256, l1 loss=0.0133, grad loss=0.0123\n",
      "Got rmse 10.394496210454967\n",
      "Got rmse 8.591567768428694\n",
      "\n",
      "epoch================================================= 89\n",
      "Epoch 89, Iteration 10170, loss = 0.0276, l1 loss=0.0139, grad loss=0.0138\n",
      "Got rmse 9.954627367363507\n",
      "Got rmse 8.18088235534016\n",
      "\n",
      "epoch================================================= 90\n",
      "Epoch 90, Iteration 10283, loss = 0.0242, l1 loss=0.0124, grad loss=0.0118\n",
      "Got rmse 10.477875891573328\n",
      "Got rmse 8.714480121436068\n",
      "\n",
      "epoch================================================= 91\n",
      "Epoch 91, Iteration 10396, loss = 0.0228, l1 loss=0.0123, grad loss=0.0105\n",
      "Got rmse 10.025856095154971\n",
      "Got rmse 8.27032647592091\n",
      "\n",
      "epoch================================================= 92\n",
      "Epoch 92, Iteration 10509, loss = 0.0190, l1 loss=0.0096, grad loss=0.0094\n",
      "Got rmse 10.615592091483558\n",
      "Got rmse 9.026636753692063\n",
      "\n",
      "epoch================================================= 93\n",
      "Epoch 93, Iteration 10622, loss = 0.0268, l1 loss=0.0150, grad loss=0.0119\n",
      "Got rmse 10.366329107985855\n",
      "Got rmse 8.693541312809444\n",
      "\n",
      "epoch================================================= 94\n",
      "Epoch 94, Iteration 10735, loss = 0.0299, l1 loss=0.0171, grad loss=0.0129\n",
      "Got rmse 10.501620604426202\n",
      "Got rmse 8.765984856401191\n",
      "\n",
      "epoch================================================= 95\n",
      "Epoch 95, Iteration 10848, loss = 0.0370, l1 loss=0.0196, grad loss=0.0174\n",
      "Got rmse 10.93224332237708\n",
      "Got rmse 9.147220192950119\n",
      "\n",
      "epoch================================================= 96\n",
      "Epoch 96, Iteration 10961, loss = 0.0213, l1 loss=0.0111, grad loss=0.0103\n",
      "Got rmse 9.977188087417286\n",
      "Got rmse 8.264679731153528\n",
      "\n",
      "epoch================================================= 97\n",
      "Epoch 97, Iteration 11074, loss = 0.0253, l1 loss=0.0128, grad loss=0.0125\n",
      "Got rmse 9.62253334669509\n",
      "Got rmse 7.919886158031846\n",
      "\n",
      "epoch================================================= 98\n",
      "Epoch 98, Iteration 11187, loss = 0.0304, l1 loss=0.0168, grad loss=0.0136\n",
      "Got rmse 9.997830708028845\n",
      "Got rmse 8.391470462320811\n",
      "\n",
      "epoch================================================= 99\n",
      "Epoch 99, Iteration 11300, loss = 0.0369, l1 loss=0.0200, grad loss=0.0169\n",
      "Got rmse 11.326413694853532\n",
      "Got rmse 9.721207632955045\n",
      "\n",
      "epoch================================================= 100\n",
      "Epoch 100, Iteration 11413, loss = 0.0200, l1 loss=0.0100, grad loss=0.0099\n",
      "Got rmse 9.382113456092855\n",
      "Got rmse 7.656427558285908\n",
      "\n",
      "epoch================================================= 101\n",
      "Epoch 101, Iteration 11526, loss = 0.0272, l1 loss=0.0137, grad loss=0.0135\n",
      "Got rmse 9.310481546654467\n",
      "Got rmse 7.652592240073472\n",
      "\n",
      "epoch================================================= 102\n",
      "Epoch 102, Iteration 11639, loss = 0.0310, l1 loss=0.0156, grad loss=0.0153\n",
      "Got rmse 9.566159252267044\n",
      "Got rmse 7.9186015545326045\n",
      "\n",
      "epoch================================================= 103\n",
      "Epoch 103, Iteration 11752, loss = 0.0275, l1 loss=0.0137, grad loss=0.0138\n",
      "Got rmse 10.415865394068094\n",
      "Got rmse 8.76735724956107\n",
      "\n",
      "epoch================================================= 104\n",
      "Epoch 104, Iteration 11865, loss = 0.0386, l1 loss=0.0197, grad loss=0.0190\n",
      "Got rmse 12.368765808109542\n",
      "Got rmse 10.734168162502774\n",
      "\n",
      "epoch================================================= 105\n",
      "Epoch 105, Iteration 11978, loss = 0.0359, l1 loss=0.0226, grad loss=0.0133\n",
      "Got rmse 11.325709271079772\n",
      "Got rmse 9.602004951259453\n",
      "\n",
      "epoch================================================= 106\n",
      "Epoch 106, Iteration 12091, loss = 0.0258, l1 loss=0.0143, grad loss=0.0115\n",
      "Got rmse 9.547471008621647\n",
      "Got rmse 7.884851316405994\n",
      "\n",
      "epoch================================================= 107\n",
      "Epoch 107, Iteration 12204, loss = 0.0225, l1 loss=0.0115, grad loss=0.0110\n",
      "Got rmse 9.194814557648813\n",
      "Got rmse 7.529370770996377\n",
      "\n",
      "epoch================================================= 108\n",
      "Epoch 108, Iteration 12317, loss = 0.0225, l1 loss=0.0114, grad loss=0.0111\n",
      "Got rmse 9.01307338040031\n",
      "Got rmse 7.430862842233911\n",
      "\n",
      "epoch================================================= 109\n",
      "Epoch 109, Iteration 12430, loss = 0.0205, l1 loss=0.0104, grad loss=0.0101\n",
      "Got rmse 8.798452305531477\n",
      "Got rmse 7.1639060563515615\n",
      "\n",
      "epoch================================================= 110\n",
      "Epoch 110, Iteration 12543, loss = 0.0252, l1 loss=0.0127, grad loss=0.0125\n",
      "Got rmse 9.254656547238636\n",
      "Got rmse 7.618096998852528\n",
      "\n",
      "epoch================================================= 111\n",
      "Epoch 111, Iteration 12656, loss = 0.0230, l1 loss=0.0122, grad loss=0.0108\n",
      "Got rmse 9.562685052524797\n",
      "Got rmse 8.069045888947942\n",
      "\n",
      "epoch================================================= 112\n",
      "Epoch 112, Iteration 12769, loss = 0.0435, l1 loss=0.0229, grad loss=0.0206\n",
      "Got rmse 12.089336880510087\n",
      "Got rmse 10.615434886845607\n",
      "\n",
      "epoch================================================= 113\n",
      "Epoch 113, Iteration 12882, loss = 0.0194, l1 loss=0.0102, grad loss=0.0092\n",
      "Got rmse 8.886168369791617\n",
      "Got rmse 7.264853875854348\n",
      "\n",
      "epoch================================================= 114\n",
      "Epoch 114, Iteration 12995, loss = 0.0215, l1 loss=0.0125, grad loss=0.0090\n",
      "Got rmse 8.780620285512308\n",
      "Got rmse 7.169969763477335\n",
      "\n",
      "epoch================================================= 115\n",
      "Epoch 115, Iteration 13108, loss = 0.0286, l1 loss=0.0149, grad loss=0.0138\n",
      "Got rmse 9.356887111571654\n",
      "Got rmse 7.768041462996788\n",
      "\n",
      "epoch================================================= 116\n",
      "Epoch 116, Iteration 13221, loss = 0.0207, l1 loss=0.0102, grad loss=0.0105\n",
      "Got rmse 8.542812035780262\n",
      "Got rmse 6.9064754969746\n",
      "\n",
      "epoch================================================= 117\n",
      "Epoch 117, Iteration 13334, loss = 0.0278, l1 loss=0.0152, grad loss=0.0126\n",
      "Got rmse 10.628772932361056\n",
      "Got rmse 9.043557218031031\n",
      "\n",
      "epoch================================================= 118\n",
      "Epoch 118, Iteration 13447, loss = 0.0144, l1 loss=0.0076, grad loss=0.0068\n",
      "Got rmse 8.55047024459142\n",
      "Got rmse 6.96184429818612\n",
      "\n",
      "epoch================================================= 119\n",
      "Epoch 119, Iteration 13560, loss = 0.0290, l1 loss=0.0146, grad loss=0.0145\n",
      "Got rmse 9.188474918530694\n",
      "Got rmse 7.728445576545443\n",
      "\n",
      "epoch================================================= 120\n",
      "Epoch 120, Iteration 13673, loss = 0.0266, l1 loss=0.0138, grad loss=0.0128\n",
      "Got rmse 9.535760283286224\n",
      "Got rmse 8.02336943540927\n",
      "\n",
      "epoch================================================= 121\n",
      "Epoch 121, Iteration 13786, loss = 0.0304, l1 loss=0.0154, grad loss=0.0150\n",
      "Got rmse 9.162485894742394\n",
      "Got rmse 7.673670469797754\n",
      "\n",
      "epoch================================================= 122\n",
      "Epoch 122, Iteration 13899, loss = 0.0234, l1 loss=0.0121, grad loss=0.0113\n",
      "Got rmse 8.411546827571424\n",
      "Got rmse 6.888064420460803\n",
      "\n",
      "epoch================================================= 123\n",
      "Epoch 123, Iteration 14012, loss = 0.0321, l1 loss=0.0185, grad loss=0.0136\n",
      "Got rmse 10.761126942432742\n",
      "Got rmse 9.270267019991367\n",
      "\n",
      "epoch================================================= 124\n",
      "Epoch 124, Iteration 14125, loss = 0.0253, l1 loss=0.0143, grad loss=0.0109\n",
      "Got rmse 9.289507860349797\n",
      "Got rmse 7.8380063355353915\n",
      "\n",
      "epoch================================================= 125\n",
      "Epoch 125, Iteration 14238, loss = 0.0173, l1 loss=0.0088, grad loss=0.0085\n",
      "Got rmse 8.278042688075171\n",
      "Got rmse 6.743545471456244\n",
      "\n",
      "epoch================================================= 126\n",
      "Epoch 126, Iteration 14351, loss = 0.0181, l1 loss=0.0091, grad loss=0.0090\n",
      "Got rmse 8.068861539357055\n",
      "Got rmse 6.511903031860852\n",
      "\n",
      "epoch================================================= 127\n",
      "Epoch 127, Iteration 14464, loss = 0.0119, l1 loss=0.0062, grad loss=0.0057\n",
      "Got rmse 8.657040358684487\n",
      "Got rmse 7.144031476462128\n",
      "\n",
      "epoch================================================= 128\n",
      "Epoch 128, Iteration 14577, loss = 0.0188, l1 loss=0.0103, grad loss=0.0085\n",
      "Got rmse 8.343908664369861\n",
      "Got rmse 6.855125058186245\n",
      "\n",
      "epoch================================================= 129\n",
      "Epoch 129, Iteration 14690, loss = 0.0254, l1 loss=0.0132, grad loss=0.0122\n",
      "Got rmse 8.563183278869067\n",
      "Got rmse 7.134295353204024\n",
      "\n",
      "epoch================================================= 130\n",
      "Epoch 130, Iteration 14803, loss = 0.0202, l1 loss=0.0102, grad loss=0.0099\n",
      "Got rmse 8.222806028322424\n",
      "Got rmse 6.767633830125142\n",
      "\n",
      "epoch================================================= 131\n",
      "Epoch 131, Iteration 14916, loss = 0.0281, l1 loss=0.0168, grad loss=0.0113\n",
      "Got rmse 9.107173447170625\n",
      "Got rmse 7.550001186768242\n",
      "\n",
      "epoch================================================= 132\n",
      "Epoch 132, Iteration 15029, loss = 0.0168, l1 loss=0.0084, grad loss=0.0084\n",
      "Got rmse 8.156982288522874\n",
      "Got rmse 6.667454627402859\n",
      "\n",
      "epoch================================================= 133\n",
      "Epoch 133, Iteration 15142, loss = 0.0226, l1 loss=0.0114, grad loss=0.0112\n",
      "Got rmse 7.891432831608598\n",
      "Got rmse 6.389375836565504\n",
      "\n",
      "epoch================================================= 134\n",
      "Epoch 134, Iteration 15255, loss = 0.0310, l1 loss=0.0158, grad loss=0.0152\n",
      "Got rmse 8.595184750059545\n",
      "Got rmse 7.191793181362836\n",
      "\n",
      "epoch================================================= 135\n",
      "Epoch 135, Iteration 15368, loss = 0.0287, l1 loss=0.0147, grad loss=0.0140\n",
      "Got rmse 9.220777960027005\n",
      "Got rmse 7.787852017480312\n",
      "\n",
      "epoch================================================= 136\n",
      "Epoch 136, Iteration 15481, loss = 0.0210, l1 loss=0.0107, grad loss=0.0103\n",
      "Got rmse 7.783266053786984\n",
      "Got rmse 6.2883858276133475\n",
      "\n",
      "epoch================================================= 137\n",
      "Epoch 137, Iteration 15594, loss = 0.0240, l1 loss=0.0129, grad loss=0.0111\n",
      "Got rmse 8.206092365901819\n",
      "Got rmse 6.760194056536542\n",
      "\n",
      "epoch================================================= 138\n",
      "Epoch 138, Iteration 15707, loss = 0.0138, l1 loss=0.0072, grad loss=0.0066\n",
      "Got rmse 8.29462214675426\n",
      "Got rmse 6.873854201363881\n",
      "\n",
      "epoch================================================= 139\n",
      "Epoch 139, Iteration 15820, loss = 0.0295, l1 loss=0.0152, grad loss=0.0143\n",
      "Got rmse 9.752405574921335\n",
      "Got rmse 8.476106719148072\n",
      "\n",
      "epoch================================================= 140\n",
      "Epoch 140, Iteration 15933, loss = 0.0257, l1 loss=0.0140, grad loss=0.0117\n",
      "Got rmse 8.931203256533797\n",
      "Got rmse 7.515907450238736\n",
      "\n",
      "epoch================================================= 141\n",
      "Epoch 141, Iteration 16046, loss = 0.0202, l1 loss=0.0103, grad loss=0.0099\n",
      "Got rmse 7.708362515505389\n",
      "Got rmse 6.227235744740101\n",
      "\n",
      "epoch================================================= 142\n",
      "Epoch 142, Iteration 16159, loss = 0.0226, l1 loss=0.0120, grad loss=0.0106\n",
      "Got rmse 8.647573037011533\n",
      "Got rmse 7.281894705071138\n",
      "\n",
      "epoch================================================= 143\n",
      "Epoch 143, Iteration 16272, loss = 0.0176, l1 loss=0.0091, grad loss=0.0085\n",
      "Got rmse 7.438391202075488\n",
      "Got rmse 6.019113541038849\n",
      "\n",
      "epoch================================================= 144\n",
      "Epoch 144, Iteration 16385, loss = 0.0191, l1 loss=0.0105, grad loss=0.0086\n",
      "Got rmse 7.539160898081789\n",
      "Got rmse 6.080322852533725\n",
      "\n",
      "epoch================================================= 145\n",
      "Epoch 145, Iteration 16498, loss = 0.0258, l1 loss=0.0132, grad loss=0.0126\n",
      "Got rmse 7.860916988566052\n",
      "Got rmse 6.465509934018286\n",
      "\n",
      "epoch================================================= 146\n",
      "Epoch 146, Iteration 16611, loss = 0.0144, l1 loss=0.0075, grad loss=0.0068\n",
      "Got rmse 7.389992790995861\n",
      "Got rmse 5.928450138342281\n",
      "\n",
      "epoch================================================= 147\n",
      "Epoch 147, Iteration 16724, loss = 0.0279, l1 loss=0.0150, grad loss=0.0129\n",
      "Got rmse 8.985388801245122\n",
      "Got rmse 7.560381933872613\n",
      "\n",
      "epoch================================================= 148\n",
      "Epoch 148, Iteration 16837, loss = 0.0278, l1 loss=0.0147, grad loss=0.0131\n",
      "Got rmse 8.357504431206584\n",
      "Got rmse 7.005413516625653\n",
      "\n",
      "epoch================================================= 149\n",
      "Epoch 149, Iteration 16950, loss = 0.0216, l1 loss=0.0115, grad loss=0.0101\n",
      "Got rmse 7.942742931826735\n",
      "Got rmse 6.536616966497364\n",
      "\n",
      "epoch================================================= 150\n",
      "Epoch 150, Iteration 17063, loss = 0.0225, l1 loss=0.0116, grad loss=0.0109\n",
      "Got rmse 7.649878597125731\n",
      "Got rmse 6.2570561143048975\n",
      "\n",
      "epoch================================================= 151\n",
      "Epoch 151, Iteration 17176, loss = 0.0150, l1 loss=0.0077, grad loss=0.0073\n",
      "Got rmse 7.200346135204605\n",
      "Got rmse 5.751779071084307\n",
      "\n",
      "epoch================================================= 152\n",
      "Epoch 152, Iteration 17289, loss = 0.0229, l1 loss=0.0117, grad loss=0.0111\n",
      "Got rmse 9.028528463310067\n",
      "Got rmse 7.7137058754888645\n",
      "\n",
      "epoch================================================= 153\n",
      "Epoch 153, Iteration 17402, loss = 0.0121, l1 loss=0.0063, grad loss=0.0057\n",
      "Got rmse 7.4734763491476475\n",
      "Got rmse 6.099431430025623\n",
      "\n",
      "epoch================================================= 154\n",
      "Epoch 154, Iteration 17515, loss = 0.0182, l1 loss=0.0108, grad loss=0.0075\n",
      "Got rmse 7.2850984396766085\n",
      "Got rmse 5.955622379947464\n",
      "\n",
      "epoch================================================= 155\n",
      "Epoch 155, Iteration 17628, loss = 0.0145, l1 loss=0.0073, grad loss=0.0073\n",
      "Got rmse 7.073507224615042\n",
      "Got rmse 5.658962718741299\n",
      "\n",
      "epoch================================================= 156\n",
      "Epoch 156, Iteration 17741, loss = 0.0150, l1 loss=0.0075, grad loss=0.0075\n",
      "Got rmse 7.1035241104081335\n",
      "Got rmse 5.691140171206908\n",
      "\n",
      "epoch================================================= 157\n",
      "Epoch 157, Iteration 17854, loss = 0.0239, l1 loss=0.0127, grad loss=0.0112\n",
      "Got rmse 7.618762036927515\n",
      "Got rmse 6.217342959609115\n",
      "\n",
      "epoch================================================= 158\n",
      "Epoch 158, Iteration 17967, loss = 0.0128, l1 loss=0.0067, grad loss=0.0061\n",
      "Got rmse 7.107546013211203\n",
      "Got rmse 5.737766907619027\n",
      "\n",
      "epoch================================================= 159\n",
      "Epoch 159, Iteration 18080, loss = 0.0236, l1 loss=0.0120, grad loss=0.0116\n",
      "Got rmse 7.240829601307724\n",
      "Got rmse 5.861680028400825\n",
      "\n",
      "epoch================================================= 160\n",
      "Epoch 160, Iteration 18193, loss = 0.0170, l1 loss=0.0087, grad loss=0.0084\n",
      "Got rmse 7.178855619493017\n",
      "Got rmse 5.797867332649895\n",
      "\n",
      "epoch================================================= 161\n",
      "Epoch 161, Iteration 18306, loss = 0.0198, l1 loss=0.0099, grad loss=0.0098\n",
      "Got rmse 7.047392099874372\n",
      "Got rmse 5.656657585841519\n",
      "\n",
      "epoch================================================= 162\n",
      "Epoch 162, Iteration 18419, loss = 0.0157, l1 loss=0.0083, grad loss=0.0074\n",
      "Got rmse 6.949072024707045\n",
      "Got rmse 5.595933707293756\n",
      "\n",
      "epoch================================================= 163\n",
      "Epoch 163, Iteration 18532, loss = 0.0159, l1 loss=0.0082, grad loss=0.0077\n",
      "Got rmse 6.909974167014888\n",
      "Got rmse 5.532623403811843\n",
      "\n",
      "epoch================================================= 164\n",
      "Epoch 164, Iteration 18645, loss = 0.0142, l1 loss=0.0073, grad loss=0.0070\n",
      "Got rmse 6.862382067223346\n",
      "Got rmse 5.503368285607992\n",
      "\n",
      "epoch================================================= 165\n",
      "Epoch 165, Iteration 18758, loss = 0.0187, l1 loss=0.0101, grad loss=0.0086\n",
      "Got rmse 7.242009683906082\n",
      "Got rmse 5.8468396323171135\n",
      "\n",
      "epoch================================================= 166\n",
      "Epoch 166, Iteration 18871, loss = 0.0191, l1 loss=0.0099, grad loss=0.0091\n",
      "Got rmse 7.118597118125799\n",
      "Got rmse 5.79929116137033\n",
      "\n",
      "epoch================================================= 167\n",
      "Epoch 167, Iteration 18984, loss = 0.0163, l1 loss=0.0081, grad loss=0.0081\n",
      "Got rmse 6.738712355938078\n",
      "Got rmse 5.355603554193924\n",
      "\n",
      "epoch================================================= 168\n",
      "Epoch 168, Iteration 19097, loss = 0.0222, l1 loss=0.0112, grad loss=0.0111\n",
      "Got rmse 6.937494090541609\n",
      "Got rmse 5.589067355102475\n",
      "\n",
      "epoch================================================= 169\n",
      "Epoch 169, Iteration 19210, loss = 0.0242, l1 loss=0.0131, grad loss=0.0111\n",
      "Got rmse 8.477579516266902\n",
      "Got rmse 7.180541307946205\n",
      "\n",
      "epoch================================================= 170\n",
      "Epoch 170, Iteration 19323, loss = 0.0231, l1 loss=0.0116, grad loss=0.0115\n",
      "Got rmse 7.371535477927106\n",
      "Got rmse 6.053189286422668\n",
      "\n",
      "epoch================================================= 171\n",
      "Epoch 171, Iteration 19436, loss = 0.0197, l1 loss=0.0101, grad loss=0.0096\n",
      "Got rmse 6.792018006428153\n",
      "Got rmse 5.449184599423799\n",
      "\n",
      "epoch================================================= 172\n",
      "Epoch 172, Iteration 19549, loss = 0.0176, l1 loss=0.0090, grad loss=0.0086\n",
      "Got rmse 6.966242119462904\n",
      "Got rmse 5.653417693525838\n",
      "\n",
      "epoch================================================= 173\n",
      "Epoch 173, Iteration 19662, loss = 0.0186, l1 loss=0.0092, grad loss=0.0094\n",
      "Got rmse 6.91266236633632\n",
      "Got rmse 5.582755291889141\n",
      "\n",
      "epoch================================================= 174\n",
      "Epoch 174, Iteration 19775, loss = 0.0157, l1 loss=0.0079, grad loss=0.0078\n",
      "Got rmse 6.745106781203933\n",
      "Got rmse 5.406732671893771\n",
      "\n",
      "epoch================================================= 175\n",
      "Epoch 175, Iteration 19888, loss = 0.0135, l1 loss=0.0068, grad loss=0.0067\n",
      "Got rmse 6.642646039799327\n",
      "Got rmse 5.305273038492001\n",
      "\n",
      "epoch================================================= 176\n",
      "Epoch 176, Iteration 20001, loss = 0.0194, l1 loss=0.0107, grad loss=0.0088\n",
      "Got rmse 6.9998012039740285\n",
      "Got rmse 5.7219943328386345\n",
      "\n",
      "epoch================================================= 177\n",
      "Epoch 177, Iteration 20114, loss = 0.0180, l1 loss=0.0093, grad loss=0.0088\n",
      "Got rmse 6.642541354909345\n",
      "Got rmse 5.325968265325632\n",
      "\n",
      "epoch================================================= 178\n",
      "Epoch 178, Iteration 20227, loss = 0.0182, l1 loss=0.0090, grad loss=0.0092\n",
      "Got rmse 6.527122159187973\n",
      "Got rmse 5.213655797404406\n",
      "\n",
      "epoch================================================= 179\n",
      "Epoch 179, Iteration 20340, loss = 0.0238, l1 loss=0.0138, grad loss=0.0100\n",
      "Got rmse 8.089926562891092\n",
      "Got rmse 6.6517967182354685\n",
      "\n",
      "epoch================================================= 180\n",
      "Epoch 180, Iteration 20453, loss = 0.0272, l1 loss=0.0148, grad loss=0.0124\n",
      "Got rmse 8.152212265970407\n",
      "Got rmse 7.006907534893094\n",
      "\n",
      "epoch================================================= 181\n",
      "Epoch 181, Iteration 20566, loss = 0.0183, l1 loss=0.0093, grad loss=0.0090\n",
      "Got rmse 6.606556486992294\n",
      "Got rmse 5.300813970961199\n",
      "\n",
      "epoch================================================= 182\n",
      "Epoch 182, Iteration 20679, loss = 0.0187, l1 loss=0.0099, grad loss=0.0088\n",
      "Got rmse 7.590103647376341\n",
      "Got rmse 6.316978487402299\n",
      "\n",
      "epoch================================================= 183\n",
      "Epoch 183, Iteration 20792, loss = 0.0134, l1 loss=0.0070, grad loss=0.0064\n",
      "Got rmse 6.758650581386684\n",
      "Got rmse 5.469960569499586\n",
      "\n",
      "epoch================================================= 184\n",
      "Epoch 184, Iteration 20905, loss = 0.0188, l1 loss=0.0095, grad loss=0.0094\n",
      "Got rmse 7.418952707125561\n",
      "Got rmse 6.152155414796506\n",
      "\n",
      "epoch================================================= 185\n",
      "Epoch 185, Iteration 21018, loss = 0.0205, l1 loss=0.0104, grad loss=0.0101\n",
      "Got rmse 6.7735765400408745\n",
      "Got rmse 5.5466218143840855\n",
      "\n",
      "epoch================================================= 186\n",
      "Epoch 186, Iteration 21131, loss = 0.0179, l1 loss=0.0093, grad loss=0.0086\n",
      "Got rmse 6.578771101465257\n",
      "Got rmse 5.263602101116187\n",
      "\n",
      "epoch================================================= 187\n",
      "Epoch 187, Iteration 21244, loss = 0.0171, l1 loss=0.0086, grad loss=0.0085\n",
      "Got rmse 6.630922646712184\n",
      "Got rmse 5.3509211495385145\n",
      "\n",
      "epoch================================================= 188\n",
      "Epoch 188, Iteration 21357, loss = 0.0215, l1 loss=0.0110, grad loss=0.0105\n",
      "Got rmse 6.952793251210163\n",
      "Got rmse 5.763210738680146\n",
      "\n",
      "epoch================================================= 189\n",
      "Epoch 189, Iteration 21470, loss = 0.0114, l1 loss=0.0059, grad loss=0.0055\n",
      "Got rmse 6.249731176231965\n",
      "Got rmse 4.954817588863225\n",
      "\n",
      "epoch================================================= 190\n",
      "Epoch 190, Iteration 21583, loss = 0.0163, l1 loss=0.0083, grad loss=0.0080\n",
      "Got rmse 6.434588862264574\n",
      "Got rmse 5.19666762009984\n",
      "\n",
      "epoch================================================= 191\n",
      "Epoch 191, Iteration 21696, loss = 0.0213, l1 loss=0.0107, grad loss=0.0106\n",
      "Got rmse 6.825385056244939\n",
      "Got rmse 5.5960690661653665\n",
      "\n",
      "epoch================================================= 192\n",
      "Epoch 192, Iteration 21809, loss = 0.0149, l1 loss=0.0076, grad loss=0.0073\n",
      "Got rmse 6.251066232876496\n",
      "Got rmse 4.950653850714741\n",
      "\n",
      "epoch================================================= 193\n",
      "Epoch 193, Iteration 21922, loss = 0.0199, l1 loss=0.0099, grad loss=0.0100\n",
      "Got rmse 6.386450079057705\n",
      "Got rmse 5.145603354821681\n",
      "\n",
      "epoch================================================= 194\n",
      "Epoch 194, Iteration 22035, loss = 0.0176, l1 loss=0.0091, grad loss=0.0085\n",
      "Got rmse 6.768863764112967\n",
      "Got rmse 5.544493507197506\n",
      "\n",
      "epoch================================================= 195\n",
      "Epoch 195, Iteration 22148, loss = 0.0146, l1 loss=0.0074, grad loss=0.0072\n",
      "Got rmse 6.3638274521605\n",
      "Got rmse 5.073686979651783\n",
      "\n",
      "epoch================================================= 196\n",
      "Epoch 196, Iteration 22261, loss = 0.0130, l1 loss=0.0065, grad loss=0.0065\n",
      "Got rmse 6.24243773635373\n",
      "Got rmse 4.981629512105587\n",
      "\n",
      "epoch================================================= 197\n",
      "Epoch 197, Iteration 22374, loss = 0.0161, l1 loss=0.0082, grad loss=0.0079\n",
      "Got rmse 6.3746164480385445\n",
      "Got rmse 5.089915104984759\n",
      "\n",
      "epoch================================================= 198\n",
      "Epoch 198, Iteration 22487, loss = 0.0199, l1 loss=0.0100, grad loss=0.0099\n",
      "Got rmse 6.160328482977928\n",
      "Got rmse 4.873384720793745\n",
      "\n",
      "epoch================================================= 199\n",
      "Epoch 199, Iteration 22600, loss = 0.0172, l1 loss=0.0092, grad loss=0.0080\n",
      "Got rmse 7.0135466003005895\n",
      "Got rmse 5.870538482399195\n",
      "\n",
      "epoch================================================= 200\n",
      "Epoch 200, Iteration 22713, loss = 0.0199, l1 loss=0.0101, grad loss=0.0098\n",
      "Got rmse 6.597798832596279\n",
      "Got rmse 5.380431570463135\n",
      "\n",
      "epoch================================================= 201\n",
      "Epoch 201, Iteration 22826, loss = 0.0177, l1 loss=0.0091, grad loss=0.0085\n",
      "Got rmse 6.60193804643475\n",
      "Got rmse 5.343693519300333\n",
      "\n",
      "epoch================================================= 202\n",
      "Epoch 202, Iteration 22939, loss = 0.0200, l1 loss=0.0105, grad loss=0.0095\n",
      "Got rmse 8.378670615742903\n",
      "Got rmse 7.295961504595633\n",
      "\n",
      "epoch================================================= 203\n",
      "Epoch 203, Iteration 23052, loss = 0.0178, l1 loss=0.0097, grad loss=0.0081\n",
      "Got rmse 6.625959070034898\n",
      "Got rmse 5.420115463007833\n",
      "\n",
      "epoch================================================= 204\n",
      "Epoch 204, Iteration 23165, loss = 0.0152, l1 loss=0.0077, grad loss=0.0074\n",
      "Got rmse 6.21923397344276\n",
      "Got rmse 4.934862429955102\n",
      "\n",
      "epoch================================================= 205\n",
      "Epoch 205, Iteration 23278, loss = 0.0190, l1 loss=0.0096, grad loss=0.0093\n",
      "Got rmse 6.149057370033535\n",
      "Got rmse 4.864044041008978\n",
      "\n",
      "epoch================================================= 206\n",
      "Epoch 206, Iteration 23391, loss = 0.0193, l1 loss=0.0108, grad loss=0.0086\n",
      "Got rmse 6.726382408202249\n",
      "Got rmse 5.505402278080005\n",
      "\n",
      "epoch================================================= 207\n",
      "Epoch 207, Iteration 23504, loss = 0.0239, l1 loss=0.0134, grad loss=0.0105\n",
      "Got rmse 7.557773980286002\n",
      "Got rmse 6.475306854967707\n",
      "\n",
      "epoch================================================= 208\n",
      "Epoch 208, Iteration 23617, loss = 0.0125, l1 loss=0.0066, grad loss=0.0059\n",
      "Got rmse 6.062146533323868\n",
      "Got rmse 4.827014333631494\n",
      "\n",
      "epoch================================================= 209\n",
      "Epoch 209, Iteration 23730, loss = 0.0205, l1 loss=0.0103, grad loss=0.0102\n",
      "Got rmse 5.991719373175846\n",
      "Got rmse 4.73455495774116\n",
      "\n",
      "epoch================================================= 210\n",
      "Epoch 210, Iteration 23843, loss = 0.0147, l1 loss=0.0074, grad loss=0.0073\n",
      "Got rmse 6.082695659340554\n",
      "Got rmse 4.865245007344393\n",
      "\n",
      "epoch================================================= 211\n",
      "Epoch 211, Iteration 23956, loss = 0.0219, l1 loss=0.0111, grad loss=0.0107\n",
      "Got rmse 6.668505211360291\n",
      "Got rmse 5.505821060354665\n",
      "\n",
      "epoch================================================= 212\n",
      "Epoch 212, Iteration 24069, loss = 0.0165, l1 loss=0.0087, grad loss=0.0078\n",
      "Got rmse 6.097102782533374\n",
      "Got rmse 4.860933747660816\n",
      "\n",
      "epoch================================================= 213\n",
      "Epoch 213, Iteration 24182, loss = 0.0133, l1 loss=0.0069, grad loss=0.0064\n",
      "Got rmse 6.057497121513578\n",
      "Got rmse 4.832052826877844\n",
      "\n",
      "epoch================================================= 214\n",
      "Epoch 214, Iteration 24295, loss = 0.0211, l1 loss=0.0108, grad loss=0.0103\n",
      "Got rmse 6.441823800302921\n",
      "Got rmse 5.300987883527683\n",
      "\n",
      "epoch================================================= 215\n",
      "Epoch 215, Iteration 24408, loss = 0.0161, l1 loss=0.0082, grad loss=0.0079\n",
      "Got rmse 6.024320045812787\n",
      "Got rmse 4.769314318419408\n",
      "\n",
      "epoch================================================= 216\n",
      "Epoch 216, Iteration 24521, loss = 0.0159, l1 loss=0.0079, grad loss=0.0080\n",
      "Got rmse 5.848049645808251\n",
      "Got rmse 4.601589889193461\n",
      "\n",
      "epoch================================================= 217\n",
      "Epoch 217, Iteration 24634, loss = 0.0188, l1 loss=0.0105, grad loss=0.0083\n",
      "Got rmse 6.290761293515732\n",
      "Got rmse 5.055648454861779\n",
      "\n",
      "epoch================================================= 218\n",
      "Epoch 218, Iteration 24747, loss = 0.0121, l1 loss=0.0062, grad loss=0.0060\n",
      "Got rmse 5.8568768356416125\n",
      "Got rmse 4.620628386872155\n",
      "\n",
      "epoch================================================= 219\n",
      "Epoch 219, Iteration 24860, loss = 0.0130, l1 loss=0.0067, grad loss=0.0063\n",
      "Got rmse 5.9180814657328975\n",
      "Got rmse 4.716842227858482\n",
      "\n",
      "epoch================================================= 220\n",
      "Epoch 220, Iteration 24973, loss = 0.0167, l1 loss=0.0097, grad loss=0.0071\n",
      "Got rmse 6.233969940570325\n",
      "Got rmse 5.01355623657397\n",
      "\n",
      "epoch================================================= 221\n",
      "Epoch 221, Iteration 25086, loss = 0.0107, l1 loss=0.0056, grad loss=0.0051\n",
      "Got rmse 5.781075604854717\n",
      "Got rmse 4.593544452698228\n",
      "\n",
      "epoch================================================= 222\n",
      "Epoch 222, Iteration 25199, loss = 0.0139, l1 loss=0.0071, grad loss=0.0068\n",
      "Got rmse 5.788633055717136\n",
      "Got rmse 4.557113881663674\n",
      "\n",
      "epoch================================================= 223\n",
      "Epoch 223, Iteration 25312, loss = 0.0197, l1 loss=0.0114, grad loss=0.0083\n",
      "Got rmse 6.3039569782413665\n",
      "Got rmse 5.1797931160593125\n",
      "\n",
      "epoch================================================= 224\n",
      "Epoch 224, Iteration 25425, loss = 0.0165, l1 loss=0.0083, grad loss=0.0082\n",
      "Got rmse 6.1563114373411985\n",
      "Got rmse 4.944786523099645\n",
      "\n",
      "epoch================================================= 225\n",
      "Epoch 225, Iteration 25538, loss = 0.0158, l1 loss=0.0079, grad loss=0.0080\n",
      "Got rmse 5.757109305838196\n",
      "Got rmse 4.518519418200176\n",
      "\n",
      "epoch================================================= 226\n",
      "Epoch 226, Iteration 25651, loss = 0.0111, l1 loss=0.0063, grad loss=0.0048\n",
      "Got rmse 5.738633771026661\n",
      "Got rmse 4.4721447503057\n",
      "\n",
      "epoch================================================= 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [04:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m         Generative_network\u001b[38;5;241m.\u001b[39mapply(weight_init)\n\u001b[0;32m     64\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m:Generative_network\u001b[38;5;241m.\u001b[39mparameters()}], lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39m weight_decay, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.99\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m         RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare,loss_train,loss_val\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_part_GM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerative_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdimB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdimB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDF\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMaxB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMinB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     RMSE_lr\u001b[38;5;241m.\u001b[39mappend(RMSE_val_history[epoch_stop]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#save RMSE and loss after early stopping\u001b[39;00m\n",
      "File \u001b[1;32md:\\QubotGit\\QubotGitFile\\Qubot_Elastica\\Modeling eMNS\\Training_loop.py:325\u001b[0m, in \u001b[0;36mtrain_part_GM\u001b[1;34m(model, optimizer, train_loader, valid_loader, epochs, learning_rate_decay, weight_decay, schedule, grid_space, DF, verbose, device, maxB, minB, lr_max, lr_min, max_epoch, linear_lr)\u001b[0m\n\u001b[0;32m    321\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# x,_,_ = max_min_norm(x,device)\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# y,_,_ = max_min_norm(y,device)\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#zero out all of gradient\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DF: \n\u001b[0;32m    327\u001b[0m   preds \u001b[38;5;241m=\u001b[39m compute_discrete_curl(model(x),device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\14032\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14032\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\14032\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:815\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    813\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14032\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\14032\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop import train_part_GM,get_mean_of_dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current_norm,\n",
    "    train_y=Bfield_norm\n",
    ")\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 8\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,2) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "epochs = 400\n",
    "learning_rate_decay = .5\n",
    "learning_rates = [1e-4]\n",
    "RMSE_lr = []\n",
    "schedule = []\n",
    "linear_lr = False\n",
    "weight_decays = [0]\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "    for learning_rate in tqdm(learning_rates):\n",
    "        for weight_decay in weight_decays:\n",
    "\n",
    "            # split the dataset to train, validation, test\n",
    "            train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "            #Using Dataloader for batch train\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,shuffle=True)\n",
    "            valid_loader = torch.utils.data.DataLoader(dataset=valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "            get_mean_of_dataloader(valid_loader,model=Generative_network,device=device)\n",
    "            print(\"----------------------------\")\n",
    "            \n",
    "            print(\"----------------------------\")\n",
    "            # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "            Generative_network.apply(weight_init)\n",
    "            optimizer = torch.optim.Adam([{'params':Generative_network.parameters()}], lr=learning_rate, weight_decay= weight_decay, betas=(0.5,0.99))\n",
    "            RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare,loss_train,loss_val= train_part_GM(\n",
    "                model=Generative_network, optimizer=optimizer, train_loader=train_loader, valid_loader=valid_loader, epochs=epochs, \n",
    "                learning_rate_decay=learning_rate_decay, weight_decay=weight_decay, schedule=schedule, grid_space=dimB[2]*dimB[3]*dimB[4], DF=DF,verbose=False, device=device, maxB=MaxB[0,:], minB=MinB[0,:],\n",
    "                lr_max=learning_rate, lr_min=2.5e-6,max_epoch=epochs, linear_lr=linear_lr)\n",
    "        \n",
    "\n",
    "        RMSE_lr.append(RMSE_val_history[epoch_stop].item())\n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)\n",
    "torch.save(Generative_network, 'EMS_CNN.pt')\t# 这里会存储迄今最优模型的参数\n",
    "print(RMSE_lr)\n",
    "print(learning_rates)\n",
    "print(RMSE_lr[0],learning_rates[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([3,80])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop]*1e6,'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop]*1e6,'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(epoch_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN_ETH.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
