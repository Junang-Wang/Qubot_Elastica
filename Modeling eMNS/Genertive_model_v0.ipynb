{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook employs a generative network based on convolution neural network (CNN) layers to learn the mapping between input current configuration between output magnetic field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qubot/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 6, 21, 21, 21])\n",
      "current shape torch.Size([1000, 12])\n",
      "Bfield shape torch.Size([1000, 3, 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "from ReadData import ReadCurrentAndField_CNN\n",
    "import glob\n",
    "import os \n",
    "\n",
    "# TODO zhoujing edit this Data loading \n",
    "# print(os.getcwd())\n",
    "foldername=\"./Data/\"\n",
    "filepattern = \"MagneticField[0-9]*.txt\"\n",
    "#data = ReadFolder(foldername,filepattern)\n",
    "current,data = ReadCurrentAndField_CNN (foldername,filepattern)\n",
    "\n",
    "fileList = glob.glob(foldername+filepattern)\n",
    "position = data[:,0:3,0:20,0:20,0:20]\n",
    "Bfield = data[:,3:,0:20,0:20,0:20]\n",
    "# print(fileList)\n",
    "print(data.shape)\n",
    "print('current shape', current.shape)\n",
    "print('Bfield shape', Bfield.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative_net(\n",
      "  (proj): Linear(in_features=12, out_features=8000, bias=True)\n",
      "  (conv3d): Conv3d(64, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (total_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=8000, bias=True)\n",
      "    (1): Unflatten(dim=0, unflattened_size=(64, 5, 5, 5))\n",
      "    (2): BigBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (1): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (2): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (3): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (4): UpsampleBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): BigBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (1): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (2): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (3): ResidualEMNSBlock_3d(\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (2): LeakyReLU(negative_slope=0.01)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "        )\n",
      "        (4): UpsampleBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Conv3d(64, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from Neural_network import Generative_net, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "num_input = 12\n",
    "output_shape = (3,20,20,20)\n",
    "SB_args = (64,64,4) # (Cin, Cout, num_block)\n",
    "BB_args = (2,2) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "\n",
    "Generative_network = Generative_net(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_percent 0.9\n",
      "Epoch 0, Iteration 7, loss = 0.3510\n",
      "Got RMSE 0.32754695415496826\n",
      "Got RMSE 0.32418933510780334\n",
      "\n",
      "Validation loss decreased (inf --> 0.350983).  Saving model ...\n",
      "Epoch 1, Iteration 14, loss = 0.2121\n",
      "Got RMSE 0.172689288854599\n",
      "Got RMSE 0.17061683535575867\n",
      "\n",
      "Validation loss decreased (0.350983 --> 0.212098).  Saving model ...\n",
      "Epoch 2, Iteration 21, loss = 0.1391\n",
      "Got RMSE 0.11217285692691803\n",
      "Got RMSE 0.11062062531709671\n",
      "\n",
      "Validation loss decreased (0.212098 --> 0.139108).  Saving model ...\n",
      "Epoch 3, Iteration 28, loss = 0.1029\n",
      "Got RMSE 0.08286809921264648\n",
      "Got RMSE 0.08175507187843323\n",
      "\n",
      "Validation loss decreased (0.139108 --> 0.102948).  Saving model ...\n",
      "Epoch 4, Iteration 35, loss = 0.0791\n",
      "Got RMSE 0.06362708657979965\n",
      "Got RMSE 0.06284409761428833\n",
      "\n",
      "Validation loss decreased (0.102948 --> 0.079143).  Saving model ...\n",
      "Epoch 5, Iteration 42, loss = 0.0631\n",
      "Got RMSE 0.05140509828925133\n",
      "Got RMSE 0.050583772361278534\n",
      "\n",
      "Validation loss decreased (0.079143 --> 0.063142).  Saving model ...\n",
      "Epoch 6, Iteration 49, loss = 0.0528\n",
      "Got RMSE 0.04265155643224716\n",
      "Got RMSE 0.04214133322238922\n",
      "\n",
      "Validation loss decreased (0.063142 --> 0.052829).  Saving model ...\n",
      "Epoch 7, Iteration 56, loss = 0.0461\n",
      "Got RMSE 0.037536486983299255\n",
      "Got RMSE 0.03704578056931496\n",
      "\n",
      "Validation loss decreased (0.052829 --> 0.046067).  Saving model ...\n",
      "Epoch 8, Iteration 63, loss = 0.0451\n",
      "Got RMSE 0.03338546305894852\n",
      "Got RMSE 0.03275797888636589\n",
      "\n",
      "Validation loss decreased (0.046067 --> 0.045055).  Saving model ...\n",
      "Epoch 9, Iteration 70, loss = 0.0400\n",
      "Got RMSE 0.03064017929136753\n",
      "Got RMSE 0.029906103387475014\n",
      "\n",
      "Validation loss decreased (0.045055 --> 0.039980).  Saving model ...\n",
      "Epoch 10, Iteration 77, loss = 0.0368\n",
      "Got RMSE 0.02855002135038376\n",
      "Got RMSE 0.028041211888194084\n",
      "\n",
      "Validation loss decreased (0.039980 --> 0.036841).  Saving model ...\n",
      "Epoch 11, Iteration 84, loss = 0.0329\n",
      "Got RMSE 0.026944860816001892\n",
      "Got RMSE 0.026317913085222244\n",
      "\n",
      "Validation loss decreased (0.036841 --> 0.032892).  Saving model ...\n",
      "Epoch 12, Iteration 91, loss = 0.0331\n",
      "Got RMSE 0.024812057614326477\n",
      "Got RMSE 0.024366017431020737\n",
      "\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13, Iteration 98, loss = 0.0290\n",
      "Got RMSE 0.024349229410290718\n",
      "Got RMSE 0.02376655861735344\n",
      "\n",
      "Validation loss decreased (0.032892 --> 0.029036).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m         Generative_network\u001b[38;5;241m.\u001b[39mapply(weight_init)\n\u001b[1;32m     57\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m:Generative_network\u001b[38;5;241m.\u001b[39mparameters()}], lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39m weight_decay)\n\u001b[0;32m---> 58\u001b[0m         RMSE_history, RMSE_val_history, loss_history, iter_history, loss_val_history,epoch_stop \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_part_GM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerative_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#save RMSE and loss after early stopping\u001b[39;00m\n\u001b[1;32m     61\u001b[0m RMSE_history_end[index] \u001b[38;5;241m=\u001b[39m RMSE_history[epoch_stop]\n",
      "File \u001b[0;32m/home/rslsync/Qubot/Codes/Qubot_Elastica/Qubot_Elastica/Modeling eMNS/Training_loop.py:286\u001b[0m, in \u001b[0;36mtrain_part_GM\u001b[0;34m(model, optimizer, train_loader, valid_loader, epochs, learning_rate_decay, weight_decay, schedule, verbose, device)\u001b[0m\n\u001b[1;32m    284\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(preds, y) \u001b[38;5;241m+\u001b[39m grad_loss(preds,y)\n\u001b[1;32m    285\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m#zero out all of gradient\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# compute gradient of loss\u001b[39;00m\n\u001b[1;32m    287\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m#update parameters\u001b[39;00m\n\u001b[1;32m    289\u001b[0m tt \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m+\u001b[39m epoch\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Neural_network import Generative_net, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop import train_part_GM\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 12\n",
    "output_shape = (3,20,20,20)\n",
    "SB_args = (64,64,4) # (Cin, Cout, num_block)\n",
    "BB_args = (2,2) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "\n",
    "Generative_network = Generative_net(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "epochs = 50\n",
    "learning_rate_decay = .1\n",
    "learning_rates = [1e-3]\n",
    "schedule = []\n",
    "weight_decays = [0]\n",
    "\n",
    "train_percents = np.arange(0.9,1.0,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "loss_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "    for learning_rate in learning_rates:\n",
    "        for weight_decay in weight_decays:\n",
    "\n",
    "            # split the dataset to train, validation, test\n",
    "            train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [train_percent*0.9,train_percent*0.1,1.0-train_percent])\n",
    "\n",
    "            #Using Dataloader for batch train\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,shuffle=True)\n",
    "            valid_loader = torch.utils.data.DataLoader(dataset=valid_set,batch_size=batch_size,shuffle=True)\n",
    "            test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "            Generative_network.apply(weight_init)\n",
    "            optimizer = torch.optim.Adam([{'params':Generative_network.parameters()}], lr=learning_rate, weight_decay= weight_decay)\n",
    "            RMSE_history, RMSE_val_history, loss_history, iter_history, loss_val_history,epoch_stop = train_part_GM(model=Generative_network, optimizer=optimizer, train_loader=train_loader, valid_loader=valid_loader, epochs=epochs, learning_rate_decay=learning_rate_decay, schedule=schedule, weight_decay=weight_decay, verbose=False, device=device)\n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    loss_val_history_end[index] = loss_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "plt.title('Train and Val RMSE')\n",
    "plt.plot(iter_history,RMSE_history,'-o')\n",
    "plt.plot(iter_history,RMSE_val_history,'-o')\n",
    "plt.legend(['train acc','val acc'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
