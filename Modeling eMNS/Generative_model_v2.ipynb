{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ETH data to CNN generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"ray[data,train,tune,serve]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    use_gpu = True\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    use_gpu = False\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 21, 21, 21])\n",
      "current shape torch.Size([10, 12])\n",
      "Bfield shape torch.Size([10, 3, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from ReadData import ReadCurrentAndField_CNN\n",
    "import glob\n",
    "import os \n",
    "\n",
    "# TODO zhoujing edit this Data loading \n",
    "# print(os.getcwd())\n",
    "foldername=\"./Data/\"\n",
    "filepattern = \"MagneticField[0-9]*.txt\"\n",
    "train_file_num= 10\n",
    "#data = ReadFolder(foldername,filepattern)\n",
    "current,data = ReadCurrentAndField_CNN (foldername,filepattern,train_file_num)\n",
    "\n",
    "fileList = glob.glob(foldername+filepattern)\n",
    "position = data[:,0:3,2:18,2:18,2:18]\n",
    "Bfield = data[:,3:,2:18,2:18,2:18]\n",
    "\n",
    "# print(fileList)\n",
    "print(data.shape)\n",
    "print('current shape', current.shape)\n",
    "print('Bfield shape', Bfield.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net,Generative_net_test ,ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 8\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,3) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net_test(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)\n",
    "\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from Training_loop import grad_loss_Jacobain\n",
    "x = torch.randn(2,8)\n",
    "y = Bfield[0:2]\n",
    "preds = Generative_network(x)\n",
    "print(preds.shape)\n",
    "loss =   F.l1_loss(preds,y)+grad_loss_Jacobain(preds,y)\n",
    "        # optimizer.zero_grad() #zero out all of gradient\n",
    "loss.backward()\n",
    "\n",
    "make_dot(loss, params=dict(Generative_network.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-21 15:20:55,592\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-03-21 15:20:58,015\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-03-21 15:20:58,119\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-03-21 15:21:07,788\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-03-21 15:21:10,410\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Trainer(...)`.\n",
      "2024-03-21 15:21:10,412\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "URI has empty scheme: '~/Trained_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m\n\u001b[0;32m    114\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TorchTrainer(\n\u001b[0;32m    115\u001b[0m     train_loop_per_worker \u001b[38;5;241m=\u001b[39m train_GM,\n\u001b[0;32m    116\u001b[0m     train_loop_config \u001b[38;5;241m=\u001b[39m train_loop_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#----------------------------------------------\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# tuner = tune.Tuner(\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m#     trainer,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# # tune the model    \u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# results = tuner.fit()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\base_trainer.py:625\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m restore_msg \u001b[38;5;241m=\u001b[39m TrainingFailedError\u001b[38;5;241m.\u001b[39m_RESTORE_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    620\u001b[0m     trainer_cls_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    621\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(experiment_local_path),\n\u001b[0;32m    622\u001b[0m )\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 625\u001b[0m     result_grid \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TuneError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;66;03m# Catch any `TuneError`s raised by the `Tuner.fit` call.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# Unwrap the `TuneError` if needed.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     parent_error \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;129;01mor\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\tuner.py:381\u001b[0m, in \u001b[0;36mTuner.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_ray_client:\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TuneError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\n\u001b[0;32m    384\u001b[0m             _TUNER_FAILED_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    385\u001b[0m                 path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_tuner\u001b[38;5;241m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[0;32m    386\u001b[0m             )\n\u001b[0;32m    387\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:509\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    507\u001b[0m param_space \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_space)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_restored:\n\u001b[1;32m--> 509\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    511\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resume(trainable, param_space)\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:628\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[1;34m(self, trainable, param_space)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[0;32m    616\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuner_kwargs,\n\u001b[0;32m    627\u001b[0m }\n\u001b[1;32m--> 628\u001b[0m analysis \u001b[38;5;241m=\u001b[39m run(\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    630\u001b[0m )\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_remote_string_queue()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\tune.py:772\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(experiments):\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exp, Experiment):\n\u001b[1;32m--> 772\u001b[0m         experiments[i] \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_budget_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_budget_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstorage_filesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_filesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m            \u001b[49m\u001b[43msync_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrial_name_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_name_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrial_dirname_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_dirname_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexport_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fail_fast \u001b[38;5;129;01mand\u001b[39;00m max_failures \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_failures must be 0 if fail_fast=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\experiment\\experiment.py:166\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[1;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, sync_config, checkpoint_config, trial_name_creator, trial_dirname_creator, log_to_file, export_formats, max_failures, restore, local_dir)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[0;32m    164\u001b[0m     name \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mget_experiment_dir_name(run)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage_context_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_filesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_filesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_dir_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the DRIVER:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    174\u001b[0m config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\storage.py:452\u001b[0m, in \u001b[0;36mStorageContext.__init__\u001b[1;34m(self, storage_path, experiment_dir_name, sync_config, storage_filesystem, trial_dir_name, current_checkpoint_index)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_checkpoint_index \u001b[38;5;241m=\u001b[39m current_checkpoint_index\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    449\u001b[0m     dataclasses\u001b[38;5;241m.\u001b[39mreplace(sync_config) \u001b[38;5;28;01mif\u001b[39;00m sync_config \u001b[38;5;28;01melse\u001b[39;00m SyncConfig()\n\u001b[0;32m    450\u001b[0m )\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_filesystem, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_fs_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_fs_and_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_filesystem\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_fs_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_fs_path)\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;66;03m# Syncing is always needed if a custom `storage_filesystem` is provided.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# Otherwise, syncing is only needed if storage_local_path\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# and storage_fs_path point to different locations.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\storage.py:306\u001b[0m, in \u001b[0;36mget_fs_and_path\u001b[1;34m(storage_path, storage_filesystem)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage_filesystem:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m storage_filesystem, storage_path\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileSystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\pyarrow\\_fs.pyx:348\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.from_uri\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\pyarrow\\error.pxi:143\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\pyarrow\\error.pxi:99\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: URI has empty scheme: '~/Trained_model'"
     ]
    }
   ],
   "source": [
    "from Neural_network import eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    x=current,\n",
    "    y=Bfield\n",
    ")\n",
    "# split the dataset to train, validation, test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [0.85,0.1,0.05])\n",
    "\n",
    "# normailzation\n",
    "extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "tune_schedule = ASHAScheduler(\n",
    "        metric=\"rmse_val\", # metric to optimize. This metric should be reported with tune.report()\n",
    "        mode=\"min\",\n",
    "        max_t=350,\n",
    "        grace_period=350, # minimum stop epoch\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers = 1,\n",
    "        use_gpu = use_gpu,\n",
    "        resources_per_worker = {\"CPU\":4, \"GPU\":0}\n",
    "    ),\n",
    "    # You can even grid search various datasets in Tune.\n",
    "    # \"datasets\": {\n",
    "    #     \"train\": tune.grid_search(\n",
    "    #         [ds1, ds2]\n",
    "    #     ),\n",
    "    # },\n",
    "    \"train_loop_config\": {\n",
    "                'epochs': 350,\n",
    "                'lr_max': tune.grid_search([1e-3,1e-4,5e-4]),\n",
    "                'lr_min': tune.grid_search([1e-5,2.5e-6,2.5e-7]),\n",
    "                'batch_size': 8,\n",
    "                'L2_norm'   : 0,\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 2,\n",
    "                'num_repeat'  : 1,\n",
    "                'num_block'   : 3,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set,\n",
    "                'num_input'   : 12,\n",
    "            }\n",
    "\n",
    "}\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "\n",
    "train_loop_config = {\n",
    "                'epochs': 10,\n",
    "                'lr_max': 5e-4,\n",
    "                'lr_min': 2.5e-6,\n",
    "                'batch_size': 8,\n",
    "                'L2_norm'   : 0,\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 2,\n",
    "                'num_repeat'  : 1,\n",
    "                'num_block'   : 3,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'device'      : device,\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set,\n",
    "                'num_input'   : 12,\n",
    "                # You can even grid search various datasets in Tune.\n",
    "                # \"datasets\": tune.grid_search(\n",
    "                #         [ds1, ds2]\n",
    "                #     ),\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers = 1,\n",
    "    use_gpu = use_gpu,\n",
    "    # resources_per_worker = {\"CPU\":4, \"GPU\":1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=1),storage_path='~/Trained_model', \n",
    "                       name='EMS_CNN_'+'s_'+str(train_loop_config['skip_spacing'])+'r_'+str(train_loop_config['num_repeat'])+'b_'+str(train_loop_config['num_block']) )\n",
    "\n",
    "# def train_loop_per_worker(params):\n",
    "#     train_GM(train_set=train_set, valid_set=valid_set,  device=device, config=params)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker = train_GM,\n",
    "    train_loop_config = train_loop_config,\n",
    "    scaling_config = scaling_config,\n",
    "    run_config = run_config,\n",
    "\n",
    ")\n",
    "# train the model\n",
    "result = trainer.fit()\n",
    "#----------------------------------------------\n",
    "# tuner = tune.Tuner(\n",
    "#     trainer,\n",
    "#     param_space = param_space,\n",
    "#     tune_config =tune.TuneConfig(\n",
    "#         scheduler=tune_schedule,\n",
    "#         num_samples=1, # number of samples of hyperparameter space\n",
    "#     ),\n",
    "#     # run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=2),storage_path=\"/home/qubot/ray_results\", name=\"test_experiment\"),\n",
    "                            # checkpoint_score_attribute='rmse_val', checkpoint_score_order='min\n",
    "# )\n",
    "# # tune the model    \n",
    "# results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ray_results\n",
    "print(result)\n",
    "plot_ray_results(result, metrics_names=['rmse_train','rmse_val'],ylim=[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric='rmse_val',mode='min')\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ray_results\n",
    "plot_ray_results(best_result, metrics_names=['rmse_train','rmse_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import estimate_test_set \n",
    "test_estimator = estimate_test_set(result.checkpoint, test_set, train_loop_config)\n",
    "test_estimator.fit()\n",
    "test_estimator.peek_z(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_estimator.peek_3D(length=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old version of training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eMNS_Dataset.__init__() got an unexpected keyword argument 'train_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# construct dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43meMNS_Dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBfield\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_max\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_input\u001b[39m\u001b[38;5;124m'\u001b[39m   : \u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     28\u001b[0m train_percents \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m1.01\u001b[39m,\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: eMNS_Dataset.__init__() got an unexpected keyword argument 'train_x'"
     ]
    }
   ],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'epochs': 350,\n",
    "    'lr_max': 1e-4,\n",
    "    'lr_min': 2.5e-6,\n",
    "    'batch_size': 8,\n",
    "    'L2_norm'   : 0,\n",
    "    'verbose': False,\n",
    "    'DF'     : False,\n",
    "    'schedule': [],\n",
    "    'grid_space': 16**3,\n",
    "    'learning_rate_decay': 0.5,\n",
    "    'skip_spacing': 2,\n",
    "    'num_repeat'  : 2,\n",
    "    'num_block'   : 3,\n",
    "    'device'      : device,\n",
    "    'num_input'   : 12,\n",
    "}\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "\n",
    "    # split the dataset to train, validation, test\n",
    "    train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "   \n",
    "    # normailzation\n",
    "    extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "    config['maxB'] = extremes[2]\n",
    "    config['minB'] = extremes[3]\n",
    "    config['train_set'] = train_set \n",
    "    config['valid_set'] = valid_set\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    print(\"----------------------------\")\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare = train_GM(\n",
    "        config=config)\n",
    "        \n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN_ETH.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,10])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([15,20])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop]*1e6,'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop]*1e6,'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(epoch_stop)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
