{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ETH data to CNN generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    use_gpu = True\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    use_gpu = False\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReadData import ReadETHFolder, ReadETHFile\n",
    "foldername=\"./ETH_Data/v/\"\n",
    "currentname = \"./ETH_Data/\"+\"currents_3787.h5\"\n",
    "file_num = 300\n",
    "data_shape = (16,16,16,3)\n",
    "Bfield = torch.tensor(ReadETHFolder(foldername,file_num, data_shape)).permute(0,4,1,2,3)\n",
    "current = torch.tensor(ReadETHFile(currentname))\n",
    "current = current[0:Bfield.shape[0],:]\n",
    "\n",
    "print(Bfield.shape)\n",
    "print(current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net,Generative_net_test ,ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 8\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,3) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net_test(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)\n",
    "\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from Training_loop import grad_loss_Jacobain\n",
    "x = torch.randn(2,8)\n",
    "y = Bfield[0:2]\n",
    "preds = Generative_network(x)\n",
    "print(preds.shape)\n",
    "loss =   F.l1_loss(preds,y)+grad_loss_Jacobain(preds,y)\n",
    "        # optimizer.zero_grad() #zero out all of gradient\n",
    "loss.backward()\n",
    "\n",
    "make_dot(loss, params=dict(Generative_network.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from functools import partial\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "# split the dataset to train, validation, test\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "# normailzation\n",
    "extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "tune_schedule = ASHAScheduler(\n",
    "        metric=\"rmse_val\", # metric to optimize. This metric should be reported with tune.report()\n",
    "        mode=\"min\",\n",
    "        max_t=120,\n",
    "        grace_period=10, # minimum stop epoch\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers = 1,\n",
    "        use_gpu = use_gpu,\n",
    "        resources_per_worker = {\"CPU\":10, \"GPU\":2}\n",
    "    ),\n",
    "    # You can even grid search various datasets in Tune.\n",
    "    # \"datasets\": {\n",
    "    #     \"train\": tune.grid_search(\n",
    "    #         [ds1, ds2]\n",
    "    #     ),\n",
    "    # },\n",
    "    \"train_loop_config\": {\n",
    "                'epochs': tune.choice([350]),\n",
    "                'lr_max': tune.loguniform(1e-4,1e-2),\n",
    "                'lr_min': tune.loguniform(1e-5,1e-7),\n",
    "                'batch_size': tune.choice([4,8,16]),\n",
    "                'L2_norm'   : tune.choice([0]),\n",
    "                'verbose': False,\n",
    "                'DF'     : tune.choice([True,False]),\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': tune.choice([1,2,4]),\n",
    "                'num_repeat'  : tune.choice([1,2,4]),\n",
    "                'num_block'   : tune.choice([1,2,3]),\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set,\n",
    "            }\n",
    "\n",
    "}\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "\n",
    "train_loop_config = {\n",
    "                'epochs': 10,\n",
    "                'lr_max': 1e-4,\n",
    "                'lr_min': 2.5e-6,\n",
    "                'batch_size': 8,\n",
    "                'L2_norm'   : 0,\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 1,\n",
    "                'num_repeat'  : 4,\n",
    "                'num_block'   : 2,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'device'      : device,\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set\n",
    "                # You can even grid search various datasets in Tune.\n",
    "                # \"datasets\": tune.grid_search(\n",
    "                #         [ds1, ds2]\n",
    "                #     ),\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers = 2,\n",
    "    use_gpu = use_gpu,\n",
    "    resources_per_worker = {\"CPU\":4, \"GPU\":1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=1))\n",
    "\n",
    "# def train_loop_per_worker(params):\n",
    "#     train_GM(train_set=train_set, valid_set=valid_set,  device=device, config=params)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker = train_GM,\n",
    "    train_loop_config = train_loop_config,\n",
    "    scaling_config = scaling_config,\n",
    "    run_config = run_config,\n",
    "\n",
    ")\n",
    "# result = trainer.fit()\n",
    "tuner = tune.Tuner(\n",
    "    trainer,\n",
    "    param_space = param_space,\n",
    "    tune_config =tune.TuneConfig(\n",
    "        scheduler=tune_schedule,\n",
    "        num_samples=30, # number of samples of hyperparameter space\n",
    "    ),\n",
    "    # run_config = RunConfig(storage_path=\"./results\", name=\"test_experiment\")\n",
    ")\n",
    "    \n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric='rmse_val',mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ray_results\n",
    "plot_ray_results(best_result, metrics_names=['rmse_train','rmse_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ray_results(results, metrics_names=['rmse_train','rmse_val'],ylim=[20,50])"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'epochs': 10,\n",
    "    'lr_max': 1e-4,\n",
    "    'lr_min': 2.5e-6,\n",
    "    'batch_size': 8,\n",
    "    'L2_norm'   : 0,\n",
    "    'verbose': False,\n",
    "    'DF'     : True,\n",
    "    'schedule': [],\n",
    "    'grid_space': 16**3,\n",
    "    'learning_rate_decay': 0.5,\n",
    "    'skip_spacing': 1,\n",
    "    'num_repeat'  : 4,\n",
    "    'num_block'   : 2,\n",
    "    'device'      : device,\n",
    "}\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "\n",
    "    # split the dataset to train, validation, test\n",
    "    train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "    # normailzation\n",
    "    extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "    config['maxB'] = extremes[2]\n",
    "    config['minB'] = extremes[3]\n",
    "    config['train_set'] = train_set \n",
    "    config['valid_set'] = valid_set\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    print(\"----------------------------\")\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare = train_GM(\n",
    "        config=config)\n",
    "        \n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN_ETH.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,10])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([15,20])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop]*1e6,'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop]*1e6,'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(epoch_stop)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
