{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ETH data to CNN generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    use_gpu = True\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    use_gpu = False\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReadData import ReadETHFolder, ReadETHFile\n",
    "foldername=\"./ETH_Data/v/\"\n",
    "currentname = \"./ETH_Data/\"+\"currents_3787.h5\"\n",
    "file_num = 100\n",
    "data_shape = (16,16,16,3)\n",
    "Bfield = torch.tensor(ReadETHFolder(foldername,file_num, data_shape)).permute(0,4,1,2,3)\n",
    "current = torch.tensor(ReadETHFile(currentname))\n",
    "current = current[0:Bfield.shape[0],:]\n",
    "\n",
    "print(Bfield.shape)\n",
    "print(current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net,Generative_net_test ,ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 8\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,3) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net_test(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)\n",
    "\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from Training_loop import grad_loss_Jacobain\n",
    "x = torch.randn(2,8)\n",
    "y = Bfield[0:2]\n",
    "preds = Generative_network(x)\n",
    "print(preds.shape)\n",
    "loss =   F.l1_loss(preds,y)+grad_loss_Jacobain(preds,y)\n",
    "        # optimizer.zero_grad() #zero out all of gradient\n",
    "loss.backward()\n",
    "\n",
    "make_dot(loss, params=dict(Generative_network.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from functools import partial\n",
    "from ray.train import RunConfig, ScalingConfig\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "# split the dataset to train, validation, test\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "# normailzation\n",
    "extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "tune_schedule = ASHAScheduler(\n",
    "        metric=\"loss\", # metric to optimize. This metric should be reported with tune.report()\n",
    "        mode=\"min\",\n",
    "        max_t=10,\n",
    "        grace_period=1, # minimum stop epoch\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers = 1,\n",
    "        use_gpu = False,\n",
    "        #resource_per_worker = {\"CPU\":1, \"GPU\":1}\n",
    "    ),\n",
    "    # You can even grid search various datasets in Tune.\n",
    "    # \"datasets\": {\n",
    "    #     \"train\": tune.grid_search(\n",
    "    #         [ds1, ds2]\n",
    "    #     ),\n",
    "    # },\n",
    "    \"config\": {\n",
    "                'epochs': tune.choice([10]),\n",
    "                'lr_max': tune.loguniform(1e-4,1e-2),\n",
    "                'lr_min': tune.loguniform(1e-5,1e-7),\n",
    "                'batch_size': tune.choice([4,8,16]),\n",
    "                'L2_norm'   : tune.choice([0]),\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 1,\n",
    "                'num_repeat'  : 4,\n",
    "                'num_block'   : 2,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "}\n",
    "\n",
    "}\n",
    "param_space =   {\n",
    "                'epochs': tune.choice([10]),\n",
    "                'lr_max': tune.loguniform(1e-4,1e-2),\n",
    "                'lr_min': tune.loguniform(1e-5,1e-7),\n",
    "                'batch_size': tune.choice([4,8,16]),\n",
    "                'L2_norm'   : tune.choice([0]),\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 1,\n",
    "                'num_repeat'  : 4,\n",
    "                'num_block'   : 2,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "}\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "\n",
    "def trainer(config):\n",
    "    train_GM(train_set=train_set, valid_set=valid_set,  device=device, config=config)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainer,\n",
    "    param_space = param_space,\n",
    "    tune_config =tune.TuneConfig(\n",
    "        scheduler=tune_schedule,\n",
    "        num_samples=10, # number of samples of hyperparameter space\n",
    "    ),\n",
    "    # run_config = RunConfig(storage_path=\"./results\", name=\"test_experiment\")\n",
    ")\n",
    "    \n",
    "tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(config = {\n",
    "'epochs': 10,\n",
    "'lr_max': 1e-3,\n",
    "'lr_min': 2.5e-6,\n",
    "'batch_size': 8,\n",
    "'L2_norm'   : 0,\n",
    "'verbose': False,\n",
    "'DF'     : False,\n",
    "'schedule': [],\n",
    "'grid_space': 16**3,\n",
    "'learning_rate_decay': 0.5,\n",
    "'skip_spacing': 1,\n",
    "'num_repeat'  : 4,\n",
    "'num_block'   : 2,\n",
    "'maxB'        : extremes[2],\n",
    "'minB'        : extremes[3],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "\n",
    "Config = {\n",
    "'epochs': 10,\n",
    "'lr_max': 1e-3,\n",
    "'lr_min': 2.5e-6,\n",
    "'batch_size': 8,\n",
    "'L2_norm'   : 0,\n",
    "'verbose': False,\n",
    "'DF'     : False,\n",
    "'schedule': [],\n",
    "'grid_space': 16**3,\n",
    "'learning_rate_decay': 0.5,\n",
    "'skip_spacing': 1,\n",
    "'num_repeat'  : 4,\n",
    "'num_block'   : 2,\n",
    "}\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "\n",
    "    # split the dataset to train, validation, test\n",
    "    train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "    # normailzation\n",
    "    extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "    Config['maxB'] = extremes[2]\n",
    "    Config['minB'] = extremes[3]\n",
    "\n",
    "    #Using Dataloader for batch train\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=Config['batch_size'],shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_set,batch_size=Config['batch_size'],shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    print(\"----------------------------\")\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare = train_GM( \n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader, \n",
    "        Config=Config, \n",
    "        device=device)\n",
    "        \n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN_ETH.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,10])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([15,20])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop]*1e6,'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop]*1e6,'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(epoch_stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # In this example, we don't change the model architecture\n",
    "        # due to simplicity.\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    for i in range(10):\n",
    "        train_func(model, optimizer, train_loader)\n",
    "        acc = test_func(model, test_loader)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        train.report({\"mean_accuracy\": acc})\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "}\n",
    "\n",
    "# Uncomment this to enable distributed execution\n",
    "# `ray.init(address=\"auto\")`\n",
    "\n",
    "# Download the dataset first\n",
    "datasets.MNIST(\"~/data\", train=True, download=True)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_mnist,\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
