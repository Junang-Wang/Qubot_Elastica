{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook employs a fully connective neural network(FC) or its alias artificial neural network (ANN) to learn the mapping between input current configuration between output magnetic field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReadData import ReadCurrentAndField_CNN\n",
    "import glob\n",
    "import os \n",
    "\n",
    "# TODO zhoujing edit this Data loading \n",
    "# print(os.getcwd())\n",
    "foldername=\"./Data/\"\n",
    "filepattern = \"MagneticField[0-9]*.txt\"\n",
    "train_file_num= 1000\n",
    "#data = ReadFolder(foldername,filepattern)\n",
    "current,data = ReadCurrentAndField_CNN (foldername,filepattern,train_file_num)\n",
    "\n",
    "fileList = glob.glob(foldername+filepattern)\n",
    "position = data[:,0:3,2:18,2:18,2:18]\n",
    "Bfield = data[:,3:,2:18,2:18,2:18]\n",
    "\n",
    "# print(fileList)\n",
    "print(data.shape)\n",
    "print('current shape', current.shape)\n",
    "print('Bfield shape', Bfield.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "print(data[2,:,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bfield.shape)\n",
    "print(Bfield.mean()*1e3)\n",
    "print(Bfield.var())\n",
    "print(Bfield.std()*1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data normalization\n",
    "#find min and max value of input position and Bfield\n",
    "max_current, max_current_index = torch.max(current, dim=0, keepdim=True)\n",
    "# print(max_current)\n",
    "min_current, min_current_index = torch.min(current, dim=0, keepdim=True)\n",
    "# print(min_current)\n",
    "\n",
    "max_Bfield, max_Bfield_index = torch.max(Bfield.transpose(0,1).reshape(3,-1), dim=1, keepdim=True)\n",
    "print(max_Bfield)\n",
    "min_Bfield, min_Bfield_index = torch.min(Bfield.transpose(0,1).reshape(3,-1), dim=1, keepdim=True)\n",
    "print(min_Bfield)\n",
    "\n",
    "dimB = Bfield.shape\n",
    "dimc = current.shape\n",
    "print(min_current.shape)\n",
    "print(min_Bfield.shape)\n",
    "\n",
    "minB=min_Bfield.expand(3,int(Bfield.numel()/3)).reshape(3,dimB[0],dimB[2],dimB[3],dimB[4]).transpose(0,1)\n",
    "maxB=max_Bfield.expand(3,int(Bfield.numel()/3)).reshape(3,dimB[0],dimB[2],dimB[3],dimB[4]).transpose(0,1)\n",
    "\n",
    "ave_current=0.5*(max_current.expand(dimc[0],dimc[1])+min_current.expand(dimc[0],dimc[1]))\n",
    "diff_current=0.5*(max_current.expand(dimc[0],dimc[1])-min_current.expand(dimc[0],dimc[1]))\n",
    "\n",
    "current_norm = (current-ave_current)/diff_current\n",
    "Bfield_norm = (Bfield-(minB+maxB)*0.5)/(0.5*(maxB-minB))\n",
    "\n",
    "print(min_current.shape)\n",
    "print(max_current.shape)\n",
    "print(min_Bfield.shape)\n",
    "print(max_Bfield.shape)\n",
    "\n",
    "torch.save(min_current, \"./normalize_data/cnn_min_current.pt\")\n",
    "torch.save(max_current, \"./normalize_data/cnn_max_current.pt\")\n",
    "torch.save(min_Bfield, \"./normalize_data/cnn_min_Bfield.pt\")\n",
    "torch.save(max_Bfield, \"./normalize_data/cnn_max_Bfield.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "num_input = 12\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,4,1) # (Cin, Cout, num_block)\n",
    "BB_args = (2,2) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "\n",
    "Generative_network = Generative_net(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)\n",
    "\n",
    "print(maxB.shape)\n",
    "print(maxB[0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxB=maxB.cuda(0)\n",
    "MinB=minB.cuda(0)\n",
    "print(MaxB.device)\n",
    "print(MinB.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset,Generative_net_test\n",
    "from Training_loop import train_part_GM,get_mean_of_dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current_norm,\n",
    "    train_y=Bfield_norm\n",
    ")\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 12\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,2) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "epochs = 400\n",
    "learning_rate_decay = .5\n",
    "learning_rates = [1.0e-4]\n",
    "RMSE_lr = []\n",
    "schedule = []\n",
    "linear_lr = False\n",
    "weight_decays = [0]\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "    for learning_rate in tqdm(learning_rates):\n",
    "        for weight_decay in weight_decays:\n",
    "\n",
    "            # split the dataset to train, validation, test\n",
    "            train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "            #Using Dataloader for batch train\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,shuffle=True)\n",
    "            valid_loader = torch.utils.data.DataLoader(dataset=valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "            get_mean_of_dataloader(valid_loader,model=Generative_network,device=device)\n",
    "            print(\"----------------------------\")\n",
    "            \n",
    "            print(\"----------------------------\")\n",
    "            # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "            Generative_network.apply(weight_init)\n",
    "            optimizer = torch.optim.Adam([{'params':Generative_network.parameters()}], lr=learning_rate, weight_decay= weight_decay, betas=(0.5,0.99))\n",
    "            RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare,loss_train,loss_val= train_part_GM(\n",
    "                model=Generative_network, optimizer=optimizer, train_loader=train_loader, valid_loader=valid_loader, epochs=epochs, \n",
    "                learning_rate_decay=learning_rate_decay, weight_decay=weight_decay, schedule=schedule, grid_space= dimB[2]*dimB[3]*dimB[4], DF=DF,verbose=False, device=device, maxB=MaxB[0,:], minB=MinB[0,:],\n",
    "                lr_max=learning_rate, lr_min=2.5e-6,max_epoch=epochs, linear_lr=linear_lr)\n",
    "        \n",
    "        \n",
    "        RMSE_lr.append(RMSE_val_history[epoch_stop].item())\n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_val.shape)\n",
    "print(loss_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RMSE_lr)\n",
    "print(learning_rates)\n",
    "print(RMSE_lr[0],learning_rates[0])\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(learning_rates,RMSE_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.title('loss train and val')\n",
    "plt.plot(iter_history[0:epoch_stop],loss_train[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],loss_val[0:epoch_stop],'-*')\n",
    "plt.legend(['loss_train','loss_val'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([15,20])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "# plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop],'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position[0:1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# a = torch.range(1,48).reshape(3,4,4)\n",
    "a = torch.randn(3,4,4)\n",
    "b = a+1\n",
    "grad_a = torch.gradient(a)\n",
    "grad_a_x = torch.gradient(a[0])\n",
    "grad_a_y = torch.gradient(a[1])\n",
    "grad_a_z = torch.gradient(a[2])\n",
    "print(len(a))\n",
    "print(grad_a[2])\n",
    "print(grad_a_z)\n",
    "error = torch.sum(grad_a[1] + grad_a[2]) - torch.sum(sum(grad_a_x)+sum(grad_a_y) +sum(grad_a_z))\n",
    "print(error)\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Generative_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from Training_loop import grad_loss\n",
    "x = torch.randn(2,12)\n",
    "y = Bfield[0:2]\n",
    "preds = Generative_network(x)\n",
    "print(preds.shape)\n",
    "loss =   grad_loss(preds,y)\n",
    "        # optimizer.zero_grad() #zero out all of gradient\n",
    "loss.backward()\n",
    "\n",
    "make_dot(loss, params=dict(Generative_network.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
