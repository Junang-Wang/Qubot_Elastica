{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ETH data to CNN generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"ray[data,train,tune,serve]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "\n",
    "if torch.cuda.device_count():\n",
    "    device = 'cuda'\n",
    "    use_gpu = True\n",
    "    print('Good to go')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    use_gpu = False\n",
    "    print('Using cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 21, 21, 21])\n",
      "current shape torch.Size([10, 12])\n",
      "Bfield shape torch.Size([10, 3, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from ReadData import ReadCurrentAndField_CNN,add_gaussian_noise\n",
    "import glob\n",
    "import os \n",
    "\n",
    "# TODO zhoujing edit this Data loading \n",
    "# print(os.getcwd())\n",
    "foldername=\"./Data/\"\n",
    "filepattern = \"MagneticField[0-9]*.txt\"\n",
    "train_file_num= 10\n",
    "#data = ReadFolder(foldername,filepattern)\n",
    "current,data = ReadCurrentAndField_CNN (foldername,filepattern,train_file_num)\n",
    "\n",
    "fileList = glob.glob(foldername+filepattern)\n",
    "position = data[:,0:3,2:18,2:18,2:18]\n",
    "Bfield = data[:,3:,2:18,2:18,2:18]\n",
    "noise = 0.0\n",
    "# print(fileList)\n",
    "print(data.shape)\n",
    "print('current shape', current.shape)\n",
    "print('Bfield shape', Bfield.shape)\n",
    "# current = add_gaussian_noise(current,noise=noise)\n",
    "# Bfield = add_gaussian_noise(Bfield,noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 0, 2, 3, 8, 9, 6]\n",
      "(tensor(14.7264, dtype=torch.float64), tensor(-14.9384, dtype=torch.float64), tensor(0.0366), tensor(-0.0411))\n"
     ]
    }
   ],
   "source": [
    "from Neural_network import eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer,TorchConfig\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "import os\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    x=current,\n",
    "    y=Bfield\n",
    ")\n",
    "# split the dataset to train, validation, test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [0.8,0.1,0.1])\n",
    "print(train_set.indices)\n",
    "# normailzation\n",
    "extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "print(extremes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net,Generative_net_test ,ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "###############################################\n",
    "# Config the neural network\n",
    "###############################################\n",
    "num_input = 12\n",
    "output_shape = (3,16,16,16)\n",
    "SB_args = (64,64,1,4) # (Cin, Cout, num_repeat, num_block)\n",
    "BB_args = (2,3) # (scale_factor, num_block)\n",
    "SB_block = ResidualEMNSBlock_3d \n",
    "BB_block = BigBlock\n",
    "DF = False # whether using divergence free model\n",
    "\n",
    "Generative_network = Generative_net_test(SB_args, BB_args, SB_block, BB_block, num_input=num_input, output_shape= output_shape)\n",
    "print(Generative_network)\n",
    "\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from Training_loop import grad_loss_Jacobain\n",
    "x = torch.randn(2,8)\n",
    "y = Bfield[0:2]\n",
    "preds = Generative_network(x)\n",
    "print(preds.shape)\n",
    "loss =   F.l1_loss(preds,y)+grad_loss_Jacobain(preds,y)\n",
    "        # optimizer.zero_grad() #zero out all of gradient\n",
    "loss.backward()\n",
    "\n",
    "make_dot(loss, params=dict(Generative_network.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-23 11:46:22</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:08.05        </td></tr>\n",
       "<tr><td>Memory:      </td><td>24.4/31.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/28 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_e5a2d_00000</td><td style=\"text-align: right;\">           1</td><td>C:/Users/14032/ray_results/TorchTrainer_2024-03-23_11-46-14/TorchTrainer_e5a2d_00000_0_2024-03-23_11-46-14\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_e5a2d_00000</td><td>ERROR   </td><td>127.0.0.1:55104</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 11:46:22,424\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_e5a2d_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::_Inner.train()\u001b[39m (pid=55104, ip=127.0.0.1, actor_id=ac3d49637b988cf68a3dc01101000000, repr=TorchTrainer)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=32436, ip=127.0.0.1, actor_id=3a12522470063c592c0764a601000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x00000186828C1630>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"d:\\QubotGit\\QubotGitFile\\Qubot_Elastica\\Modeling eMNS\\Training_loop_v2.py\", line 104, in train_GM\n",
      "    model = construct_model_GM(config)\n",
      "  File \"d:\\QubotGit\\QubotGitFile\\Qubot_Elastica\\Modeling eMNS\\Training_loop_v2.py\", line 250, in construct_model_GM\n",
      "    gird_size = config['grid_size']\n",
      "KeyError: 'grid_size'\n",
      "2024-03-23 11:46:22,453\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_e5a2d_00000]\n",
      "2024-03-23 11:46:22,453\tINFO tune.py:1042 -- Total run time: 8.07 seconds (8.04 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"C:\\Users\\14032\\ray_results\\TorchTrainer_2024-03-23_11-46-14\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(KeyError)\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;31mRayTaskError(KeyError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=55104, ip=127.0.0.1, actor_id=ac3d49637b988cf68a3dc01101000000, repr=TorchTrainer)\n  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 342, in train\n    raise skipped from exception_cause(skipped)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 43, in check_for_failure\n    ray.get(object_ref)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2624, in get\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(KeyError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=32436, ip=127.0.0.1, actor_id=3a12522470063c592c0764a601000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x00000186828C1630>)\n  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"c:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 118, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"d:\\QubotGit\\QubotGitFile\\Qubot_Elastica\\Modeling eMNS\\Training_loop_v2.py\", line 104, in train_GM\n    model = construct_model_GM(config)\n  File \"d:\\QubotGit\\QubotGitFile\\Qubot_Elastica\\Modeling eMNS\\Training_loop_v2.py\", line 250, in construct_model_GM\n    gird_size = config['grid_size']\nKeyError: 'grid_size'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTrainingFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 124\u001b[0m\n\u001b[0;32m    115\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TorchTrainer(\n\u001b[0;32m    116\u001b[0m     train_loop_per_worker \u001b[38;5;241m=\u001b[39m train_GM,\n\u001b[0;32m    117\u001b[0m     train_loop_config \u001b[38;5;241m=\u001b[39m train_loop_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m#----------------------------------------------\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# tuner = tune.Tuner(\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#     trainer,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# # tune the model    \u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# results = tuner.fit()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14032\\.conda\\envs\\myenv\\lib\\site-packages\\ray\\train\\base_trainer.py:640\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    636\u001b[0m result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([restore_msg, TrainingFailedError\u001b[38;5;241m.\u001b[39m_FAILURE_CONFIG_MSG])\n\u001b[0;32m    642\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresult\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTrainingFailedError\u001b[0m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"C:\\Users\\14032\\ray_results\\TorchTrainer_2024-03-23_11-46-14\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "from Neural_network import eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer,TorchConfig\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "import os\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    x=current,\n",
    "    y=Bfield\n",
    ")\n",
    "# split the dataset to train, validation, test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [0.85,0.1,0.05])\n",
    "print(train_set)\n",
    "# normailzation\n",
    "extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "tune_schedule = ASHAScheduler(\n",
    "        metric=\"rmse_val\", # metric to optimize. This metric should be reported with tune.report()\n",
    "        mode=\"min\",\n",
    "        max_t=350,\n",
    "        grace_period=350, # minimum stop epoch\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers = 1,\n",
    "        use_gpu = use_gpu,\n",
    "        resources_per_worker = {\"CPU\":4, \"GPU\":0}\n",
    "    ),\n",
    "    # You can even grid search various datasets in Tune.\n",
    "    # \"datasets\": {\n",
    "    #     \"train\": tune.grid_search(\n",
    "    #         [ds1, ds2]\n",
    "    #     ),\n",
    "    # },\n",
    "    \"train_loop_config\": {\n",
    "                'epochs': 350,\n",
    "                'lr_max': tune.grid_search([1e-3,1e-4,5e-4]),\n",
    "                'lr_min': tune.grid_search([1e-5,2.5e-6,2.5e-7]),\n",
    "                'batch_size': 8,\n",
    "                'L2_norm'   : 0,\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 16**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 2,\n",
    "                'num_repeat'  : 1,\n",
    "                'num_block'   : 3,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set,\n",
    "                'num_input'   : 12,\n",
    "            }\n",
    "\n",
    "}\n",
    "\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "\n",
    "train_loop_config = {\n",
    "                'epochs': 1,\n",
    "                'lr_max': 5e-4,\n",
    "                'lr_min': 2.5e-6,\n",
    "                'batch_size': 8,\n",
    "                'L2_norm'   : 0,\n",
    "                'verbose': False,\n",
    "                'DF'     : False,\n",
    "                'schedule': [],\n",
    "                'grid_space': 8**3,\n",
    "                'learning_rate_decay': 0.5,\n",
    "                'skip_spacing': 2,\n",
    "                'num_repeat'  : 1,\n",
    "                'num_block'   : 3,\n",
    "                'maxB'        : extremes[2],\n",
    "                'minB'        : extremes[3],\n",
    "                'device'      : device,\n",
    "                'train_set'   : train_set,\n",
    "                'valid_set'   : valid_set,\n",
    "                'num_input'   : 12,\n",
    "                # You can even grid search various datasets in Tune.\n",
    "                # \"datasets\": tune.grid_search(\n",
    "                #         [ds1, ds2]\n",
    "                #     ),\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers = 1,\n",
    "    use_gpu = use_gpu,\n",
    "    # resources_per_worker = {\"CPU\":4, \"GPU\":1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=1))#,storage_path='D:\\Qubot\\Trained_model', \n",
    "                       #name='EMS_CNN_'+'s_'+str(train_loop_config['skip_spacing'])+'r_'+str(train_loop_config['num_repeat'])+'b_'+str(train_loop_config['num_block']) )\n",
    "#\n",
    "# def train_loop_per_worker(params):\n",
    "#     train_GM(train_set=train_set, valid_set=valid_set,  device=device, config=params)\n",
    "torch_config = TorchConfig(backend=\"gloo\")\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker = train_GM,\n",
    "    train_loop_config = train_loop_config,\n",
    "    torch_config=torch_config,\n",
    "    scaling_config = scaling_config,\n",
    "    run_config = run_config,\n",
    "\n",
    ")\n",
    "# train the model\n",
    "result = trainer.fit()\n",
    "#----------------------------------------------\n",
    "# tuner = tune.Tuner(\n",
    "#     trainer,\n",
    "#     param_space = param_space,\n",
    "#     tune_config =tune.TuneConfig(\n",
    "#         scheduler=tune_schedule,\n",
    "#         num_samples=1, # number of samples of hyperparameter space\n",
    "#     ),\n",
    "#     # run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=2),storage_path=\"/home/qubot/ray_results\", name=\"test_experiment\"),\n",
    "                            # checkpoint_score_attribute='rmse_val', checkpoint_score_order='min\n",
    "# )\n",
    "# # tune the model    \n",
    "# results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ray_results\n",
    "print(result)\n",
    "\n",
    "plot_ray_results(result, metrics_names=['rmse_train','rmse_val'],ylim=[0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric='rmse_val',mode='min')\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_ray_results\n",
    "plot_ray_results(best_result, metrics_names=['rmse_train','rmse_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import estimate_test_set \n",
    "test_estimator = estimate_test_set(result.checkpoint, test_set, train_loop_config)\n",
    "test_estimator.fit()\n",
    "test_estimator.peek_z(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_estimator.peek_3D(length=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old version of training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_network import Generative_net, Generative_net_test, ResidualEMNSBlock_3d, BigBlock, weight_init, eMNS_Dataset\n",
    "from Training_loop_v2 import train_GM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# construct dataset\n",
    "dataset = eMNS_Dataset(\n",
    "    train_x=current,\n",
    "    train_y=Bfield\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'epochs': 1,\n",
    "    'lr_max': 1e-4,\n",
    "    'lr_min': 2.5e-6,\n",
    "    'batch_size': 8,\n",
    "    'L2_norm'   : 0,\n",
    "    'verbose': False,\n",
    "    'DF'     : False,\n",
    "    'schedule': [],\n",
    "    'grid_space': 16**3,\n",
    "    'learning_rate_decay': 0.5,\n",
    "    'skip_spacing': 2,\n",
    "    'num_repeat'  : 2,\n",
    "    'num_block'   : 3,\n",
    "    'device'      : device,\n",
    "    'num_input'   : 12,\n",
    "}\n",
    "train_percents = np.arange(1.0,1.01,0.1)\n",
    "RMSE_history_end = np.zeros(len(train_percents))\n",
    "RMSE_val_history_end = np.zeros(len(train_percents))\n",
    "loss_history_end = np.zeros(len(train_percents))\n",
    "iter_history_end = np.zeros(len(train_percents))\n",
    "mse_history_end = np.zeros(len(train_percents))\n",
    "mse_val_history_end = np.zeros(len(train_percents))\n",
    "train_stop_epoch = np.zeros(len(train_percents))\n",
    "\n",
    "################################################\n",
    "# Train the neural network\n",
    "################################################\n",
    "index=0\n",
    "for train_percent in train_percents:\n",
    "    epoch_stop = 0\n",
    "    print('train_percent',train_percent)\n",
    "\n",
    "    # split the dataset to train, validation, test\n",
    "    train_set, valid_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "\n",
    "    # normailzation\n",
    "    extremes = dataset.train_norm(train_indices = train_set.indices)\n",
    "\n",
    "    config['maxB'] = extremes[2]\n",
    "    config['minB'] = extremes[3]\n",
    "    config['train_set'] = train_set \n",
    "    config['valid_set'] = valid_set\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    print(\"----------------------------\")\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "    RMSE_history, RMSE_val_history, loss_history, iter_history, mse_history, mse_val_history,epoch_stop,Rsquare = train_GM(\n",
    "        config=config)\n",
    "        \n",
    "    \n",
    "    #save RMSE and loss after early stopping\n",
    "    RMSE_history_end[index] = RMSE_history[epoch_stop]\n",
    "    RMSE_val_history_end[index]= RMSE_val_history[epoch_stop]\n",
    "    loss_history_end[index] = loss_history[epoch_stop]\n",
    "    iter_history_end[index] = iter_history[epoch_stop]\n",
    "    mse_history_end[index] = mse_history[epoch_stop]\n",
    "    mse_val_history_end[index] = mse_val_history[epoch_stop]\n",
    "    index=index+1\n",
    "    print('training stop at epoch:',epoch_stop)\n",
    "    print('training stop at epoch:',Rsquare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Generative_network, 'EMS_CNN_ETH.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ave_site = 5\n",
    "ave_kernel = 1/ave_site*np.ones(ave_site)\n",
    "loss_history_conv = np.convolve(loss_history.numpy(),ave_kernel,'same')\n",
    "\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(iter_history,loss_history,'-o')\n",
    "plt.plot(iter_history,loss_history_conv,'-*')\n",
    "plt.legend(['loss','loss_conv'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,10])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val RMSE(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_history[0:epoch_stop],'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],RMSE_val_history[0:epoch_stop],'-*')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_history[0:epoch_stop]*1000,'-o')\n",
    "# plt.plot(2e-5*np.arange(epoch_stop),RMSE_val_history[0:epoch_stop]*1000,'-*')\n",
    "# plt.ylim([15,20])\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('RMSE(mT)')\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Train and Val loss(sample_num=1000)')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_history[0:epoch_stop]*1e6,'-o')\n",
    "plt.plot(iter_history[0:epoch_stop],mse_val_history[0:epoch_stop]*1e6,'-*')\n",
    "plt.legend(['train CNN','val CNN'])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('mse(mT^2)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(epoch_stop)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
